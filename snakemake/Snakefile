#
# Snakemake workflow for Asgard OG Diversity Analysis
#
# This workflow automates the pipeline described in the README.md.
# It handles dependency management and parallel execution of jobs.
#
# To run this workflow:
# 1. Install Snakemake in the conda environment: mamba install -c bioconda snakemake
# 2. Configure the paths in `config.yaml`.
# 3. Execute snakemake: snakemake --cores <num_cores> --use-conda
#

import pandas as pd

# --- Load Configuration ---
# The config file holds all user-configurable paths and parameters.
configfile: "config.yaml"

# --- Helper Function ---
# This function reads the list of OG IDs generated by the `prepare_og_list` checkpoint.
# This is necessary because the exact OGs to be processed are not known before the workflow starts.
def get_og_ids(wildcards):
    """Reads the OG IDs from the checkpoint output file."""
    return pd.read_csv(checkpoints.prepare_og_list.get().output.csv)['OG_ID'].tolist()

# --- Checkpoint: Prepare OG List ---
# This is a "checkpoint" because its output is needed to determine the full
# structure of the Directed Acyclic Graph (DAG) of jobs.
checkpoint prepare_og_list:
    """
    Filters the main proteome database to get a list of OGs with more than
    a specified number of members.
    """
    input:
        db=config["proteome_db"],
        script=Path(config["scripts_dir"]) / "prepare_og_list.py"
    output:
        csv=Path(config["results_dir"]) / "og_list.csv"
    params:
        min_members=config["min_members"]
    log:
        Path(config["log_dir"]) / "prepare_og_list.log"
    shell:
        """
        python {input.script} \
            --proteome-db {input.db} \
            --output-csv {output.csv} \
            --min-members {params.min_members} > {log} 2>&1
        """

# --- Rule: All ---
# This is the final target rule. Snakemake will work backwards from here
# to determine which jobs need to be run.
rule all:
    """
    Specifies the final output files for the entire workflow.
    """
    input:
        # The main results files from the sequence and structural analyses.
        Path(config["results_dir"]) / "og_sequence_diversity.csv",
        Path(config["results_dir"]) / "all_vs_all_structural_metrics.csv"

# --- Sequence Processing Branch ---

rule fetch_og_sequences:
    """
    For each OG, extract all member sequences into a separate FASTA file.
    """
    input:
        og_list=checkpoints.prepare_og_list.get().output.csv,
        db=config["proteome_db"],
        script=Path(config["scripts_dir"]) / "fetch_og_sequences.py"
    output:
        # This is a directory that will contain all the raw FASTA files.
        # Snakemake's `directory()` function marks this output as a directory.
        directory(Path(config["results_dir"]) / "raw_fastas")
    log:
        Path(config["log_dir"]) / "fetch_og_sequences.log"
    shell:
        """
        python {input.script} \
            --og-list-csv {input.og_list} \
            --proteome-db {input.db} \
            --output-dir {output} > {log} 2>&1
        """

rule initial_mafft:
    """
    Run an initial MAFFT alignment on the raw sequences for each OG.
    """
    input:
        fastas=Path(config["results_dir"]) / "raw_fastas",
        script=Path(config["scripts_dir"]) / "run_initial_mafft_parallel.py"
    output:
        directory(Path(config["results_dir"]) / "initial_mafft")
    params:
        num_cores=config["mafft_cores"]
    log:
        Path(config["log_dir"]) / "initial_mafft.log"
    shell:
        """
        python {input.script} \
            --input_dir {input.fastas} \
            --output_dir {output} \
            --log_dir {config[log_dir]}/mafft_logs \
            --num_cores {params.num_cores} > {log} 2>&1
        """

rule filter_alignments:
    """
    Filter the initial alignments to remove fragmentary sequences.
    """
    input:
        alignments=Path(config["results_dir"]) / "initial_mafft",
        script=Path(config["scripts_dir"]) / "filter_mafft_alignments_by_length.py"
    output:
        directory(Path(config["results_dir"]) / "filtered_mafft")
    log:
        Path(config["log_dir"]) / "filter_alignments.log"
    shell:
        """
        python {input.script} \
            --input-dir {input.alignments} \
            --output-dir {output} \
            --input-suffix ".aln" > {log} 2>&1
        """

rule refine_alignments:
    """
    Refine the filtered alignments using MAFFT and TrimAl.
    """
    input:
        alignments=Path(config["results_dir"]) / "filtered_mafft",
        script=Path(config["scripts_dir"]) / "refine_alignments.py"
    output:
        directory(Path(config["results_dir"]) / "refined_mafft")
    params:
        max_workers=config["refine_cores"]
    log:
        Path(config["log_dir"]) / "refine_alignments.log"
    shell:
        """
        python {input.script} \
            --input-dir {input.alignments} \
            --output-dir {output} \
            --input-suffix "_len_filtered.aln" \
            --output-suffix "_final_trimmed.fasta" \
            --max-workers {params.max_workers} > {log} 2>&1
        """

rule infer_trees:
    """
    Infer a phylogenetic tree for each refined alignment using FastTree.
    """
    input:
        alignments=Path(config["results_dir"]) / "refined_mafft",
        script=Path(config["scripts_dir"]) / "run_fasttree_parallel.py"
    output:
        directory(Path(config["results_dir"]) / "trees")
    params:
        cores=config["fasttree_cores"]
    log:
        Path(config["log_dir"]) / "infer_trees.log"
    shell:
        """
        python {input.script} \
            -i {input.alignments} \
            -o {output} \
            --input_suffix "_final_trimmed.fasta" \
            --output_suffix "_tree.nwk" \
            --cores {params.cores} > {log} 2>&1
        """

rule calculate_sequence_diversity:
    """
    Calculate sequence diversity metrics for each OG using the refined
    alignments and inferred trees.
    """
    input:
        msa_dir=Path(config["results_dir"]) / "refined_mafft",
        tree_dir=Path(config["results_dir"]) / "trees",
        script=Path(config["scripts_dir"]) / "calculate_sequence_diversity.py"
    output:
        csv=Path(config["results_dir"]) / "og_sequence_diversity.csv"
    log:
        Path(config["log_dir"]) / "calculate_sequence_diversity.log"
    shell:
        """
        python {input.script} \
            --msa-dir {input.msa_dir} \
            --tree-dir {input.tree_dir} \
            --output-csv {output.csv} \
            --msa-suffix "_final_trimmed.fasta" \
            --tree-suffix "_tree.nwk" > {log} 2>&1
        """

# --- Structural Analysis Branch ---

rule calculate_structural_metrics:
    """
    Calculate all-vs-all structural similarity metrics (TM-scores, RMSD)
    for all high-quality structures within each OG.
    """
    input:
        og_list=checkpoints.prepare_og_list.get().output.csv,
        db=config["proteome_db"],
        pdb_dir=config["pdb_dir"],
        script=Path(config["scripts_dir"]) / "calculate_all_vs_all_metrics.py"
    output:
        csv=Path(config["results_dir"]) / "all_vs_all_structural_metrics.csv"
    params:
        max_workers=config["tmalign_cores"]
    log:
        Path(config["log_dir"]) / "calculate_structural_metrics.log"
    shell:
        """
        python {input.script} \
            --og-list-csv {input.og_list} \
            --proteome-db {input.db} \
            --pdb-dir {input.pdb_dir} \
            --output-csv {output.csv} \
            --max-workers {params.max_workers} > {log} 2>&1
        """
