# Snakemake Workflow for Asgard OG Diversity Analysis

This directory contains a [Snakemake](https://snakemake.readthedocs.io/en/stable/) workflow to automate the data processing pipeline for the Asgard OG diversity project. It is a refactoring of the original shell script-based pipeline described in the main `README.md`.

Using Snakemake provides several advantages:
- **Reproducibility**: The entire workflow is defined in a single `Snakefile`, ensuring that the analysis is run the same way every time.
- **Scalability**: Snakemake can automatically parallelize jobs and is compatible with cluster and cloud computing environments.
- **Resumability**: If the workflow is interrupted, it can be resumed from where it left off without re-running completed steps.

## 1. Installation & Setup

This workflow assumes you have already set up the main conda environment as described in the parent `README.md`.

### Install Snakemake
If you don't have Snakemake installed in your environment, you can add it with `mamba`:
```bash
mamba install -c bioconda -c conda-forge snakemake-minimal
```

### External Dependencies
This workflow relies on the same external dependencies as the original pipeline. Please ensure that `mafft` is installed and available in your system's `PATH`.

## 2. Configuration

Before running the workflow, you must configure the input paths in the `config.yaml` file.

1.  **Open `snakemake/config.yaml`** in a text editor.
2.  **Update the paths** for `proteome_db` and `pdb_dir` to point to the location where you have downloaded the large data files from Zenodo.
3.  (Optional) Adjust the number of cores allocated for parallel tasks (`mafft_cores`, `refine_cores`, etc.) to match your system's specifications.

## 3. Running the Pipeline

All Snakemake commands should be run from the root directory of this repository.

### Dry Run
It is always a good practice to perform a dry run first. This will show you which jobs Snakemake plans to execute without actually running them.
```bash
snakemake --snakefile snakemake/Snakefile -n
```

### Executing the Workflow
To run the full pipeline, use the following command. Snakemake will read the `Snakefile` and `config.yaml` to execute all necessary steps.
```bash
snakemake --snakefile snakemake/Snakefile --cores 8 --use-conda
```
- `--cores 8`: Specifies the total number of cores Snakemake can use. Adjust this based on your machine.
- `--use-conda`: Ensures that if any rule had a specific conda environment, it would be used. It's good practice to include this.

### Visualizing the Workflow
You can generate a Directed Acyclic Graph (DAG) to visualize the dependencies in the workflow. This requires `graphviz` to be installed (`mamba install -c conda-forge graphviz`).
```bash
snakemake --snakefile snakemake/Snakefile --dag | dot -Tpng > snakemake/workflow_dag.png
```
This will create a PNG image of the workflow graph in the `snakemake` directory.

## 4. Workflow Outputs

The workflow will create two main directories inside the `snakemake/` folder:
- `snakemake/results/`: This directory contains all the intermediate and final data files generated by the pipeline, organized into subdirectories for each step (e.g., `raw_fastas`, `initial_mafft`, `trees`, etc.).
- `snakemake/logs/`: Contains log files for each rule, which are useful for debugging any issues.
