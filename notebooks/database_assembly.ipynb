{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f9e62b-ca57-4d27-a8d7-b50c55f38a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This notebook was used to assemble the proteome_database_v3.5.csv, which is the main, complete dataset described in the pub. It draws primarily from the integrated_asgard_gv_ortho_interpro.parquet, which represents the integrated Orthofinder and Interproscan results. As we conducted additional analyses to incorporate into the dataset, those were includes as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32eea9a-bba4-43f7-9e09-630bf89a7c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This first cell loads the integrated parquet results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3017e92f-96e2-43c1-bb4f-304e35e2e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "parquet_metadata_file = \"integrated_asgard_gv_ortho_interpro.parquet\"\n",
    "try:\n",
    "    df_meta = pd.read_parquet(\n",
    "        parquet_metadata_file, engine=\"auto\", columns=[\"ProteinID\"]\n",
    "    )\n",
    "    print(\"First 5 ProteinIDs from parquet file:\")\n",
    "    print(df_meta[\"ProteinID\"].head().tolist())\n",
    "except Exception as e:\n",
    "    print(f\"Error reading ProteinID column: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59791b2-be75-4368-ac14-ebe70de1119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assembles the initial core data CSV by combining metadata from a parquet file with sequence search hit information and sequences from a FASTA file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75fe4fe-f327-4fc5-a50b-9fe4739d88e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "def assemble_core_csv_with_fasta(\n",
    "    parquet_file: str,\n",
    "    pdb_results_file: str,\n",
    "    filtered_fasta_file: str,  # Path to the FASTA with sequences\n",
    "    output_csv_file: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Assembles the initial core data CSV by combining metadata from a parquet file\n",
    "    with sequence search hit information and sequences from a FASTA file.\n",
    "    Handles FASTA headers containing '|' delimiters.\n",
    "\n",
    "    Args:\n",
    "        parquet_file: Path to the input parquet file containing integrated metadata.\n",
    "        pdb_results_file: Path to the MMseqs2 results file from the PDB search.\n",
    "        filtered_fasta_file: Path to the FASTA file containing sequences for filtering.\n",
    "        output_csv_file: Path where the assembled CSV file will be written.\n",
    "    \"\"\"\n",
    "    print(\"Starting CSV assembly...\")\n",
    "    print(f\"  Input Parquet: {parquet_file}\")\n",
    "    print(f\"  PDB Results: {pdb_results_file}\")\n",
    "    print(f\"  Input FASTA: {filtered_fasta_file}\")\n",
    "    print(f\"  Output CSV: {output_csv_file}\")\n",
    "\n",
    "    # --- 1. Load main metadata from Parquet ---\n",
    "    try:\n",
    "        print(f\"\\nReading metadata from {parquet_file}...\")\n",
    "        df = pd.read_parquet(parquet_file, engine=\"auto\")\n",
    "        print(f\"  Loaded {len(df):,} records from parquet file.\")\n",
    "        print(f\"  Parquet Columns Found: {df.columns.tolist()}\")\n",
    "\n",
    "        # --- Column mapping based on user-provided list ---\n",
    "        required_cols = {\n",
    "            \"ProteinID\": \"ProteinID\",\n",
    "            \"GenomeID\": \"Source_Genome_Assembly_Accession\",\n",
    "            \"OriginalName\": \"Source_Protein_Annotation\",\n",
    "            \"Dataset\": \"Source_Dataset\",\n",
    "            \"Phylum\": \"Taxonomy_Phylum\",\n",
    "            \"Taxonomy\": \"Taxonomy_Species\",\n",
    "            \"OG_ID\": \"Orthogroup\",\n",
    "            \"All_IPR_Hits\": \"IPR_Signatures\",\n",
    "            \"All_GO_Terms\": \"IPR_GO_Terms\",\n",
    "            \"Num_Domains\": \"Num_Domains\",\n",
    "            \"Domain_Architecture\": \"Domain_Architecture\",\n",
    "            \"Type\": \"Type\",\n",
    "            \"Is_Hypothetical\": \"Is_Hypothetical\",\n",
    "            \"Has_Known_Structure\": \"Has_Known_Structure\",\n",
    "            \"ncbi_taxid_placeholder\": \"NCBI_TaxID\",\n",
    "            \"taxonomy_supergroup_placeholder\": \"Taxonomy_Supergroup\",\n",
    "            \"taxonomy_class_placeholder\": \"Taxonomy_Class\",\n",
    "            \"uniprot_ac_placeholder\": \"UniProtKB_AC\",\n",
    "            \"afdb_status_placeholder\": \"AFDB_Status\",\n",
    "        }\n",
    "        protein_id_col_name = \"ProteinID\"  # Confirmed column name\n",
    "        if protein_id_col_name not in df.columns:\n",
    "            print(\n",
    "                f\"Error: Crucial column for ProteinID ('{protein_id_col_name}') not found in {parquet_file}.\"\n",
    "            )\n",
    "            sys.exit(1)\n",
    "        print(f\"  Using '{protein_id_col_name}' as the ProteinID column from parquet.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Parquet file not found at {parquet_file}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading or processing parquet file {parquet_file}: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 2. Load Sequences from FASTA ---\n",
    "    sequences = {}\n",
    "    try:\n",
    "        print(f\"\\nReading sequences from {filtered_fasta_file}...\")\n",
    "        count_fasta = 0\n",
    "        count_parse_errors = 0\n",
    "        with open(filtered_fasta_file, \"r\") as fastafile:\n",
    "            for record in SeqIO.parse(fastafile, \"fasta\"):\n",
    "                # <<< CHANGE START >>>\n",
    "                # Parse the ID: Take the part before the first '|' if present,\n",
    "                # otherwise use the whole record.id (which handles headers without '|')\n",
    "                fasta_id = (\n",
    "                    record.description.split(\"|\")[0]\n",
    "                    if \"|\" in record.description\n",
    "                    else record.id\n",
    "                )\n",
    "                # Basic check if the extracted ID looks reasonable (optional)\n",
    "                if not fasta_id:\n",
    "                    print(\n",
    "                        f\"  Warning: Could not extract valid ID from FASTA header: {record.description[:100]}...\"\n",
    "                    )\n",
    "                    count_parse_errors += 1\n",
    "                    continue\n",
    "                # <<< CHANGE END >>>\n",
    "\n",
    "                sequences[fasta_id] = str(record.seq)\n",
    "                count_fasta += 1\n",
    "\n",
    "        print(f\"  Loaded {len(sequences):,} sequences from FASTA file.\")\n",
    "        if count_parse_errors > 0:\n",
    "            print(\n",
    "                f\"  Encountered {count_parse_errors} headers where ID parsing failed.\"\n",
    "            )\n",
    "        if count_fasta == 0:\n",
    "            print(f\"  Warning: No sequences found in {filtered_fasta_file}.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Filtered FASTA file not found at {filtered_fasta_file}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading FASTA file {filtered_fasta_file}: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 3. Load PDB hit IDs ---\n",
    "    pdb_hit_ids = set()\n",
    "    try:\n",
    "        print(f\"\\nReading PDB hit IDs from {pdb_results_file}...\")\n",
    "        if os.path.exists(pdb_results_file) and os.path.getsize(pdb_results_file) > 0:\n",
    "            pdb_hits_df = pd.read_csv(\n",
    "                pdb_results_file,\n",
    "                sep=\"\\t\",\n",
    "                header=None,\n",
    "                usecols=[0],\n",
    "                names=[\"query\"],\n",
    "                comment=\"#\",\n",
    "                low_memory=False,\n",
    "            )\n",
    "            # Also parse the query IDs from the PDB results file just in case they have '|'\n",
    "            pdb_hit_ids = set(\n",
    "                pdb_hits_df[\"query\"]\n",
    "                .dropna()\n",
    "                .astype(str)\n",
    "                .apply(lambda x: x.split(\"|\")[0])\n",
    "            )\n",
    "            print(\n",
    "                f\"  Found {len(pdb_hit_ids):,} unique ProteinIDs (parsed) with PDB hits.\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"  PDB results file '{pdb_results_file}' not found or is empty. Assuming no PDB hits.\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDB results file {pdb_results_file}: {e}\")\n",
    "        pdb_hit_ids = set()\n",
    "\n",
    "    # --- 4. Prepare DataFrame for Output ---\n",
    "    print(\"\\nPreparing final DataFrame...\")\n",
    "    unique_protein_ids_in_parquet = df[protein_id_col_name].unique()\n",
    "    output_df = pd.DataFrame(index=unique_protein_ids_in_parquet)\n",
    "    output_df.index.name = \"ProteinID_Index\"  # Temporary index name\n",
    "\n",
    "    # --- Map and add columns from the parquet file ---\n",
    "    print(\"  Mapping columns from parquet to output DataFrame...\")\n",
    "    for parquet_col_name, output_col_name in required_cols.items():\n",
    "        if parquet_col_name == protein_id_col_name:\n",
    "            print(\n",
    "                f\"  Skipping mapping for '{parquet_col_name}' as it's the primary ID.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        if parquet_col_name in df.columns:\n",
    "            try:\n",
    "                mapping_series = df.drop_duplicates(\n",
    "                    subset=[protein_id_col_name]\n",
    "                ).set_index(protein_id_col_name)[parquet_col_name]\n",
    "                output_df[output_col_name] = output_df.index.map(mapping_series)\n",
    "            except KeyError as e:\n",
    "                print(\n",
    "                    f\"!!! Internal KeyError during mapping for column '{parquet_col_name}': {e}\"\n",
    "                )\n",
    "                output_df[output_col_name] = pd.NA\n",
    "            except Exception as e:\n",
    "                print(f\"!!! Error during mapping for column '{parquet_col_name}': {e}\")\n",
    "                output_df[output_col_name] = pd.NA\n",
    "        else:\n",
    "            print(\n",
    "                f\"  Info: Column '{parquet_col_name}' not found in input parquet. Adding empty column '{output_col_name}'.\"\n",
    "            )\n",
    "            output_df[output_col_name] = pd.NA\n",
    "\n",
    "    # Add Sequence column from the dictionary\n",
    "    output_df[\"Sequence\"] = output_df.index.map(sequences)\n",
    "    missing_seq_count = output_df[\"Sequence\"].isna().sum()\n",
    "    if missing_seq_count > 0:\n",
    "        print(\n",
    "            f\"  Info: {missing_seq_count:,} proteins from parquet file did not have a sequence in the filtered FASTA file (expected).\"\n",
    "        )\n",
    "    print(\"  Added 'Sequence' column from FASTA.\")\n",
    "\n",
    "    # Calculate Length robustly, handling potential non-string values\n",
    "    print(\"  Calculating 'Length' column...\")\n",
    "    output_df[\"Length\"] = (\n",
    "        output_df[\"Sequence\"]\n",
    "        .apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "        .astype(int)\n",
    "    )\n",
    "    print(\"  Calculated 'Length' column.\")\n",
    "\n",
    "    # Add Sequence Search Hit Flags\n",
    "    output_df[\"SeqSearch_PDB_Hit\"] = output_df.index.isin(pdb_hit_ids)\n",
    "    output_df[\"SeqSearch_AFDB_Hit\"] = False  # Based on previous results\n",
    "    output_df[\"SeqSearch_MGnify_Hit\"] = False  # Search was skipped\n",
    "    print(\"  Added 'SeqSearch_*_Hit' flag columns.\")\n",
    "\n",
    "    # Reset index to make ProteinID a regular column again\n",
    "    output_df.reset_index(inplace=True)\n",
    "    output_df.rename(\n",
    "        columns={\"ProteinID_Index\": \"ProteinID\"}, inplace=True\n",
    "    )  # Rename index col\n",
    "\n",
    "    # --- Filter rows: Keep only proteins present in the filtered FASTA ---\n",
    "    initial_rows = len(output_df)\n",
    "    # Ensure comparison is robust by checking against the keys from the sequences dict\n",
    "    output_df = output_df[output_df[\"ProteinID\"].isin(sequences.keys())].copy()\n",
    "    rows_after_fasta_filter = len(output_df)\n",
    "    print(\n",
    "        f\"  Filtered DataFrame to keep only proteins present in '{filtered_fasta_file}'.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Rows before FASTA filter: {initial_rows:,}. Rows after: {rows_after_fasta_filter:,}\"\n",
    "    )\n",
    "\n",
    "    # --- Reorder columns ---\n",
    "    desired_order = [\n",
    "        \"ProteinID\",\n",
    "        \"Sequence\",\n",
    "        \"Length\",\n",
    "        \"Source_Dataset\",\n",
    "        \"Dataset\",\n",
    "        \"Source_Genome_Assembly_Accession\",\n",
    "        \"GenomeID\",\n",
    "        \"Source_Protein_Annotation\",\n",
    "        \"OriginalName\",\n",
    "        \"NCBI_TaxID\",\n",
    "        \"Taxonomy_Supergroup\",\n",
    "        \"Taxonomy_Phylum\",\n",
    "        \"Phylum\",\n",
    "        \"Taxonomy_Class\",\n",
    "        \"Taxonomy_Species\",\n",
    "        \"Taxonomy\",\n",
    "        \"Orthogroup\",\n",
    "        \"OG_ID\",\n",
    "        \"IPR_Signatures\",\n",
    "        \"All_IPR_Hits\",\n",
    "        \"IPR_GO_Terms\",\n",
    "        \"All_GO_Terms\",\n",
    "        \"UniProtKB_AC\",\n",
    "        \"AFDB_Status\",\n",
    "        \"SeqSearch_PDB_Hit\",\n",
    "        \"SeqSearch_AFDB_Hit\",\n",
    "        \"SeqSearch_MGnify_Hit\",\n",
    "        \"Num_Domains\",\n",
    "        \"Domain_Architecture\",\n",
    "        \"Type\",\n",
    "        \"Is_Hypothetical\",\n",
    "        \"Has_Known_Structure\",\n",
    "    ]\n",
    "    present_columns = [col for col in desired_order if col in output_df.columns]\n",
    "    present_columns.extend(\n",
    "        [col for col in output_df.columns if col not in present_columns]\n",
    "    )\n",
    "    final_columns = (\n",
    "        pd.Series(present_columns).drop_duplicates().tolist()\n",
    "    )  # Ensure unique cols\n",
    "    output_df = output_df[final_columns]\n",
    "    print(\n",
    "        f\"  Final DataFrame has {len(output_df.columns)} columns and {len(output_df):,} rows.\"\n",
    "    )\n",
    "\n",
    "    # --- 5. Write to CSV ---\n",
    "    try:\n",
    "        print(f\"\\nWriting final CSV to {output_csv_file}...\")\n",
    "        output_dir = os.path.dirname(output_csv_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"  Created output directory: {output_dir}\")\n",
    "\n",
    "        # Check if DataFrame is empty before writing\n",
    "        if output_df.empty:\n",
    "            print(\"  Warning: Final DataFrame is empty. Writing only headers to CSV.\")\n",
    "            # Write only headers if empty\n",
    "            with open(output_csv_file, \"w\") as f:\n",
    "                f.write(\",\".join(output_df.columns) + \"\\n\")\n",
    "        else:\n",
    "            output_df.to_csv(\n",
    "                output_csv_file, index=False, na_rep=\"NA\"\n",
    "            )  # Use NA for missing values\n",
    "            print(f\"  Successfully wrote CSV file to {output_csv_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing CSV file {output_csv_file}: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"\\nCSV assembly script finished.\")\n",
    "\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Define your input and output file paths here ---\n",
    "    parquet_metadata_file = \"integrated_asgard_gv_ortho_interpro.parquet\"\n",
    "    pdb_search_results_file = \"results_vs_pdb_v2.txt\"\n",
    "    filtered_fasta_path = \"folding_candidates_final_filtered.fasta\"\n",
    "    output_database_csv = \"proteome_database_v0.1.csv\"\n",
    "\n",
    "    # --- Basic check if input files exist ---\n",
    "    if not os.path.exists(parquet_metadata_file):\n",
    "        print(f\"Error: Parquet metadata file '{parquet_metadata_file}' not found.\")\n",
    "        sys.exit(1)\n",
    "    if not os.path.exists(filtered_fasta_path):\n",
    "        print(f\"Error: Filtered FASTA file '{filtered_fasta_path}' not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Run the assembly function ---\n",
    "    assemble_core_csv_with_fasta(\n",
    "        parquet_file=parquet_metadata_file,\n",
    "        pdb_results_file=pdb_search_results_file,\n",
    "        filtered_fasta_file=filtered_fasta_path,\n",
    "        output_csv_file=output_database_csv,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35c3fc-e353-41f1-a26d-86e0f5141a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The next cell analyzes the IPR code assignments, in an effort to clarify uncertainties/missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4358d5af-d781-4ee4-a9c4-d50b0cc394fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- Configuration: PLEASE UPDATE THESE PATHS ---\n",
    "unknowns_csv_path = (\n",
    "    \"your_unknowns_only.csv\"  # Path to the CSV file generated by --output-unknowns\n",
    ")\n",
    "interpro_list_path = \"interpro_entry.list\"  # Path to your InterPro entry list TSV file\n",
    "output_dir = \".\"  # Directory to save any output files/plots (optional)\n",
    "\n",
    "# --- Parameters ---\n",
    "top_n_ipr = 30  # How many top IPR IDs to display\n",
    "top_n_keywords = 50  # How many top annotation keywords to display\n",
    "\n",
    "# --- Ensure output directory exists ---\n",
    "try:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Output directory: {os.path.abspath(output_dir)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not create output directory '{output_dir}': {e}\")\n",
    "    output_dir = \".\"  # Default to current directory if creation fails\n",
    "\n",
    "print(f\"Unknowns CSV: {unknowns_csv_path}\")\n",
    "print(f\"InterPro List: {interpro_list_path}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 1: Load Data\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Section 1: Loading Data ---\")\n",
    "\n",
    "# --- Load Unknowns CSV ---\n",
    "try:\n",
    "    df_unknowns = pd.read_csv(unknowns_csv_path, low_memory=False)\n",
    "    print(f\"Successfully loaded {len(df_unknowns)} rows from {unknowns_csv_path}\")\n",
    "    # Fill NaN values in key columns to avoid errors later\n",
    "    df_unknowns[\"IPR_Signatures\"] = df_unknowns[\"IPR_Signatures\"].fillna(\"\")\n",
    "    df_unknowns[\"Source_Protein_Annotation\"] = df_unknowns[\n",
    "        \"Source_Protein_Annotation\"\n",
    "    ].fillna(\"\")\n",
    "    # Display first few rows and info (optional, uncomment if needed)\n",
    "    # print(\"\\nUnknowns DataFrame Head:\")\n",
    "    # print(df_unknowns.head())\n",
    "    # print(\"\\nUnknowns DataFrame Info:\")\n",
    "    # df_unknowns.info()\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"ERROR: Unknowns CSV file not found at {unknowns_csv_path}. Please check the path.\"\n",
    "    )\n",
    "    sys.exit(1)  # Stop execution if file not found\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load unknowns CSV: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# --- Load InterPro List TSV ---\n",
    "def load_interpro_list_script(filepath):\n",
    "    \"\"\"Loads InterPro entry list into a dictionary for script use.\"\"\"\n",
    "    print(f\"Attempting to load InterPro list from: {filepath}\")\n",
    "    ipr_map = {}\n",
    "    abs_filepath = os.path.abspath(filepath)\n",
    "\n",
    "    if not os.path.exists(abs_filepath) or not os.path.isfile(abs_filepath):\n",
    "        print(f\"ERROR: InterPro list file not found or is not a file: {abs_filepath}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Use pandas for robust TSV reading\n",
    "        df_ipr = pd.read_csv(\n",
    "            abs_filepath,\n",
    "            sep=\"\\t\",\n",
    "            header=0,\n",
    "            names=[\n",
    "                \"ENTRY_AC\",\n",
    "                \"ENTRY_TYPE\",\n",
    "                \"ENTRY_NAME\",\n",
    "            ],  # Assign names in case header is missing\n",
    "            usecols=[0, 1, 2],  # Only read first 3 columns\n",
    "            on_bad_lines=\"warn\",  # Report problematic lines\n",
    "            encoding=\"utf-8\",\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "\n",
    "        # Check if first row was mistakenly read as data instead of header\n",
    "        if not df_ipr.empty and df_ipr.iloc[0][\"ENTRY_AC\"] == \"ENTRY_AC\":\n",
    "            df_ipr = df_ipr.iloc[1:]\n",
    "\n",
    "        # Filter for valid IPR IDs and convert to dictionary\n",
    "        df_ipr = df_ipr[df_ipr[\"ENTRY_AC\"].astype(str).str.startswith(\"IPR\")]\n",
    "        ipr_map = df_ipr.set_index(\"ENTRY_AC\").to_dict(\"index\")\n",
    "\n",
    "        print(\n",
    "            f\"Successfully processed {len(ipr_map)} InterPro entries from {abs_filepath}.\"\n",
    "        )\n",
    "        if len(ipr_map) < 10:  # Check if map seems very small\n",
    "            print(\n",
    "                \"WARN: Very few InterPro entries loaded. Check file format and content.\"\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load or parse InterPro list file {abs_filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "    return ipr_map\n",
    "\n",
    "\n",
    "ipr_details_map = load_interpro_list_script(interpro_list_path)\n",
    "\n",
    "# Example lookup (if loaded successfully)\n",
    "if ipr_details_map and len(ipr_details_map) > 0:\n",
    "    example_id = list(ipr_details_map.keys())[0]\n",
    "    print(f\"Example InterPro entry: {example_id} -> {ipr_details_map.get(example_id)}\")\n",
    "elif not ipr_details_map:\n",
    "    print(\n",
    "        \"WARN: InterPro map could not be loaded. Analysis requiring IPR details will be skipped.\"\n",
    "    )\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 2: Count Proteins Lacking IPR Signatures\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Section 2: Counting Proteins Lacking IPR Signatures ---\")\n",
    "\n",
    "total_unknowns = len(df_unknowns)\n",
    "# Count rows where IPR_Signatures is null, empty string, or whitespace only\n",
    "unknowns_no_ipr = df_unknowns[\n",
    "    df_unknowns[\"IPR_Signatures\"].astype(str).str.strip() == \"\"\n",
    "].copy()\n",
    "count_no_ipr = len(unknowns_no_ipr)\n",
    "\n",
    "print(f\"Total 'Unknown/Unclassified' proteins: {total_unknowns}\")\n",
    "if total_unknowns > 0:\n",
    "    percentage_no_ipr = (count_no_ipr / total_unknowns) * 100\n",
    "    print(\n",
    "        f\"Number of unknowns with NO IPR signatures: {count_no_ipr} ({percentage_no_ipr:.2f}%)\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Number of unknowns with NO IPR signatures: 0\")\n",
    "\n",
    "# Display some examples of proteins without IPR signatures (optional)\n",
    "# if count_no_ipr > 0:\n",
    "#     print(\"\\nExample rows with no IPR Signatures (Annotation only):\")\n",
    "#     print(unknowns_no_ipr[['Source_Protein_Annotation']].head())\n",
    "\n",
    "print(\n",
    "    \"\\nInsight: If this percentage is high, improving classification might heavily depend on\"\n",
    ")\n",
    "print(\n",
    "    \"analyzing and expanding the 'Annotation_Keywords' in the main script's CUSTOM_RULES.\"\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 3: Find Most Frequent IPR IDs Among Unknowns\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Section 3: Finding Most Frequent IPR IDs Among Unknowns ---\")\n",
    "\n",
    "# --- Parse IPR Signatures and Count Frequencies ---\n",
    "all_ipr_ids_unknowns = []\n",
    "\n",
    "\n",
    "def extract_iprs(ipr_string):\n",
    "    # Split by common delimiters, strip whitespace, filter out empty strings\n",
    "    ids = [\n",
    "        ipr_id.strip()\n",
    "        for ipr_id in re.split(r\"[,;|]\", str(ipr_string))\n",
    "        if ipr_id.strip()\n",
    "    ]\n",
    "    return ids\n",
    "\n",
    "\n",
    "# Apply the function to the 'IPR_Signatures' column and flatten the list\n",
    "# Ensure the column exists before applying\n",
    "if \"IPR_Signatures\" in df_unknowns.columns:\n",
    "    all_ipr_ids_unknowns = df_unknowns[\"IPR_Signatures\"].apply(extract_iprs).sum()\n",
    "    print(\n",
    "        f\"Total IPR signature occurrences found in unknowns: {len(all_ipr_ids_unknowns)}\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"WARN: 'IPR_Signatures' column not found in unknowns CSV. Skipping IPR frequency analysis.\"\n",
    "    )\n",
    "    all_ipr_ids_unknowns = []  # Ensure list exists but is empty\n",
    "\n",
    "# Count the frequency of each IPR ID\n",
    "ipr_counts = Counter(all_ipr_ids_unknowns)\n",
    "\n",
    "# Get the most common IPR IDs\n",
    "most_common_ipr = ipr_counts.most_common(top_n_ipr)\n",
    "\n",
    "print(f\"\\nTop {top_n_ipr} most frequent IPR IDs among unclassified proteins:\")\n",
    "if not most_common_ipr:\n",
    "    print(\"No IPR IDs found or processed in the unknowns file.\")\n",
    "else:\n",
    "    # Create a DataFrame for better display\n",
    "    df_top_ipr = pd.DataFrame(most_common_ipr, columns=[\"IPR_ID\", \"Frequency\"])\n",
    "    print(df_top_ipr.to_string())  # Print full dataframe\n",
    "\n",
    "    # --- Basic Plot ---\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(df_top_ipr[\"IPR_ID\"], df_top_ipr[\"Frequency\"])\n",
    "        plt.xlabel(\"Frequency\")\n",
    "        plt.ylabel(\"IPR ID\")\n",
    "        plt.title(f\"Top {top_n_ipr} Most Frequent IPR IDs in Unknowns\")\n",
    "        plt.gca().invert_yaxis()  # Display top ID at the top\n",
    "        plt.tight_layout()\n",
    "        # Save the plot (optional)\n",
    "        plot_path = os.path.join(output_dir, \"top_ipr_ids_unknowns.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        print(f\"\\nPlot saved to {plot_path}\")\n",
    "        # plt.show() # Uncomment to display plot if running interactively\n",
    "        plt.close()  # Close plot to free memory\n",
    "    except Exception as e:\n",
    "        print(f\"\\nWARN: Could not generate IPR frequency plot: {e}\")\n",
    "\n",
    "\n",
    "print(\n",
    "    \"\\nInsight: These IPR IDs are the most common signatures NOT being caught by your current rules.\"\n",
    ")\n",
    "print(\"They are the primary candidates to investigate further.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 4: Analyze Details of Frequent Unknown IPR IDs\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Section 4: Analyzing Details of Frequent Unknown IPR IDs ---\")\n",
    "\n",
    "if not ipr_details_map:\n",
    "    print(\"Skipping IPR detail analysis because the InterPro map was not loaded.\")\n",
    "elif not most_common_ipr:\n",
    "    print(\"Skipping IPR detail analysis because no frequent IPR IDs were found.\")\n",
    "else:\n",
    "    print(f\"Looking up details for the top {top_n_ipr} IPR IDs...\")\n",
    "    top_ipr_data = []\n",
    "    for ipr_id, frequency in most_common_ipr:\n",
    "        details = ipr_details_map.get(\n",
    "            ipr_id, {\"type\": \"Not Found\", \"name\": \"Not Found\"}\n",
    "        )\n",
    "        top_ipr_data.append(\n",
    "            {\n",
    "                \"IPR_ID\": ipr_id,\n",
    "                \"Frequency\": frequency,\n",
    "                \"IPR_Type\": details.get(\"type\", \"N/A\"),\n",
    "                \"IPR_Name\": details.get(\"name\", \"N/A\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df_top_ipr_details = pd.DataFrame(top_ipr_data)\n",
    "\n",
    "    print(\"\\nDetails of Top Frequent IPR IDs in Unknowns:\")\n",
    "    # Display the full table\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_colwidth\", None):\n",
    "        print(df_top_ipr_details.to_string())\n",
    "\n",
    "print(\"\\nInsight:\")\n",
    "print(\n",
    "    \"- Look at the IPR_Name: Does it suggest a function? Does it contain keywords you could add\"\n",
    ")\n",
    "print(\"  to an existing category's 'IPR_Keywords' list (remember to use lowercase)?\")\n",
    "print(\n",
    "    \"- Does the function represent a new category you need to create in CUSTOM_RULES?\"\n",
    ")\n",
    "print(\n",
    "    \"- Look at the IPR_Type: Is it 'Domain', 'Family', etc.? This helps understand the nature\"\n",
    ")\n",
    "print(\"  of the unclassified signatures.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 5: Analyze Annotation Keywords\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Section 5: Analyzing Annotation Keywords ---\")\n",
    "\n",
    "if \"Source_Protein_Annotation\" not in df_unknowns.columns:\n",
    "    print(\n",
    "        \"WARN: 'Source_Protein_Annotation' column not found. Skipping annotation keyword analysis.\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Analyzing keywords in 'Source_Protein_Annotation'...\")\n",
    "\n",
    "    # Define common words to ignore (customize this list as needed!)\n",
    "    stop_words = {\n",
    "        \"protein\",\n",
    "        \"hypothetical\",\n",
    "        \"uncharacterized\",\n",
    "        \"predicted\",\n",
    "        \"putative\",\n",
    "        \"domain\",\n",
    "        \"family\",\n",
    "        \"containing\",\n",
    "        \"like\",\n",
    "        \"of\",\n",
    "        \"the\",\n",
    "        \"a\",\n",
    "        \"an\",\n",
    "        \"in\",\n",
    "        \"to\",\n",
    "        \"and\",\n",
    "        \"is\",\n",
    "        \"it\",\n",
    "        \"with\",\n",
    "        \"by\",\n",
    "        \"on\",\n",
    "        \"at\",\n",
    "        \"from\",\n",
    "        \"as\",\n",
    "        \"for\",\n",
    "        \"or\",\n",
    "        \"et\",\n",
    "        \"al\",\n",
    "        \"type\",\n",
    "        \"subunit\",\n",
    "        \"chain\",\n",
    "        \"region\",\n",
    "        \"motif\",\n",
    "        \"repeat\",\n",
    "        \"protein,\",\n",
    "        \"unknown\",\n",
    "        \"function\",\n",
    "        # Add more domain-specific or common words if they obscure results\n",
    "    }\n",
    "\n",
    "    all_words = []\n",
    "    # Ensure the column is treated as string and lowercase\n",
    "    annotation_series = df_unknowns[\"Source_Protein_Annotation\"].astype(str).str.lower()\n",
    "\n",
    "    # Simple word tokenization and filtering\n",
    "    for annotation in annotation_series:\n",
    "        # Remove punctuation (basic), split into words\n",
    "        words = re.findall(r\"\\b\\w+\\b\", annotation)\n",
    "        # Filter out stop words and very short words (e.g., <= 2 letters)\n",
    "        filtered_words = [\n",
    "            word for word in words if word not in stop_words and len(word) > 2\n",
    "        ]\n",
    "        all_words.extend(filtered_words)\n",
    "\n",
    "    print(f\"Total potentially relevant words found in annotations: {len(all_words)}\")\n",
    "\n",
    "    # Count word frequencies\n",
    "    keyword_counts = Counter(all_words)\n",
    "    most_common_keywords = keyword_counts.most_common(top_n_keywords)\n",
    "\n",
    "    print(\n",
    "        f\"\\nTop {top_n_keywords} most frequent keywords in annotations (excluding common words):\"\n",
    "    )\n",
    "    if not most_common_keywords:\n",
    "        print(\"No significant keywords found after filtering.\")\n",
    "    else:\n",
    "        df_top_keywords = pd.DataFrame(\n",
    "            most_common_keywords, columns=[\"Keyword\", \"Frequency\"]\n",
    "        )\n",
    "        print(df_top_keywords.to_string())\n",
    "\n",
    "        # --- Basic Plot ---\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.barh(df_top_keywords[\"Keyword\"], df_top_keywords[\"Frequency\"])\n",
    "            plt.xlabel(\"Frequency\")\n",
    "            plt.ylabel(\"Keyword\")\n",
    "            plt.title(f\"Top {top_n_keywords} Annotation Keywords in Unknowns\")\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            # Save the plot (optional)\n",
    "            plot_path = os.path.join(output_dir, \"top_annotation_keywords_unknowns.png\")\n",
    "            plt.savefig(plot_path)\n",
    "            print(f\"\\nPlot saved to {plot_path}\")\n",
    "            # plt.show() # Uncomment to display plot if running interactively\n",
    "            plt.close()  # Close plot to free memory\n",
    "        except Exception as e:\n",
    "            print(f\"\\nWARN: Could not generate annotation keyword plot: {e}\")\n",
    "\n",
    "    print(\"\\nInsight:\")\n",
    "    print(\"- Are there recurring functional terms here that are missing from your\")\n",
    "    print(\"  'Annotation_Keywords' lists in CUSTOM_RULES (remember lowercase)?\")\n",
    "    print(\n",
    "        \"- Do these keywords suggest functions that belong to existing categories or point\"\n",
    "    )\n",
    "    print(\"  towards new categories needed?\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 6: Conclusion\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Section 6: Conclusion & Next Steps ---\")\n",
    "print(\n",
    "    \"\\nThis analysis provides several starting points for refining the `CUSTOM_RULES`\"\n",
    ")\n",
    "print(\"in your `add_specific_category_IPR_v10.py` script:\")\n",
    "print(\"\\n1. Proteins without IPR: If many unknowns lack IPR IDs, focus on improving\")\n",
    "print(\"   the `Annotation_Keywords` rules.\")\n",
    "print(\"2. Frequent Unknown IPR IDs: Investigate the functions of the top IPR IDs\")\n",
    "print(\"   identified. Add relevant IDs or derived keywords (`IPR_Keywords`) to your\")\n",
    "print(\"   existing or new categories in `CUSTOM_RULES`.\")\n",
    "print(\"3. Frequent Annotation Keywords: Add relevant keywords found in the annotation\")\n",
    "print(\"   analysis to the `Annotation_Keywords` lists in `CUSTOM_RULES`.\")\n",
    "print(\"\\nRecommendation:\")\n",
    "print(\n",
    "    \"* Iteratively update the `CUSTOM_RULES` dictionary in `add_specific_category_IPR_v10.py`\"\n",
    ")\n",
    "print(\"  based on these findings.\")\n",
    "print(\"* Re-run the main script.\")\n",
    "print(\"* Re-run this analysis script on the *new* unknowns file to see if the\")\n",
    "print(\"  classification has improved and identify the next set of common unknowns.\")\n",
    "print(\"* Repeat this process until the number of 'Unknown/Unclassified' proteins\")\n",
    "print(\"  is acceptably low.\")\n",
    "\n",
    "print(\"\\nAnalysis complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37319c16-795b-4eb8-bec0-bc4bb77d6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell collects the sequences lacking IPR codes and concatenates them into a single fasta, from which we ran interproscan on them. The missing data was a result of previously clustering the sequences, but then I went back and decided to run interproscan on everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9333b4-ac3a-4b17-9ede-1fcb3cd35ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "unknowns_csv_path = \"your_unknowns_only.csv\"  # Input CSV with unknown protein IDs\n",
    "original_fasta_files = [\n",
    "    \"Fastas_filtered/Asgard_all_globular_proteins.fasta\",  # Path to first original FASTA\n",
    "    \"Fastas_filtered/GV_all_globular_proteins.fasta\",  # Path to second original FASTA\n",
    "    # Add more original FASTA files if needed\n",
    "]\n",
    "output_fasta_path = (\n",
    "    \"unknown_protein_sequences.fasta\"  # Output FASTA for InterProScan input\n",
    ")\n",
    "\n",
    "# Column in the CSV containing the protein IDs (adjust if different)\n",
    "protein_id_column = \"ProteinID\"\n",
    "\n",
    "# --- Script Logic ---\n",
    "\n",
    "print(f\"Loading unknown protein IDs from: {unknowns_csv_path}\")\n",
    "\n",
    "try:\n",
    "    df_unknowns = pd.read_csv(unknowns_csv_path)\n",
    "    if protein_id_column not in df_unknowns.columns:\n",
    "        print(\n",
    "            f\"ERROR: Protein ID column '{protein_id_column}' not found in {unknowns_csv_path}\"\n",
    "        )\n",
    "        print(f\"Available columns: {df_unknowns.columns.tolist()}\")\n",
    "        sys.exit(1)\n",
    "    # Ensure IDs are strings and handle potential NaN values\n",
    "    unknown_ids = set(df_unknowns[protein_id_column].dropna().astype(str))\n",
    "    print(f\"Found {len(unknown_ids)} unique unknown protein IDs.\")\n",
    "    if not unknown_ids:\n",
    "        print(\"ERROR: No protein IDs found in the unknowns file. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Unknowns CSV file not found at {unknowns_csv_path}\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to read or process {unknowns_csv_path}: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "print(\"\\nExtracting sequences from:\")\n",
    "for f in original_fasta_files:\n",
    "    print(f\"- {f}\")\n",
    "\n",
    "sequences_found = 0\n",
    "sequences_written = 0\n",
    "ids_found = set()\n",
    "\n",
    "try:\n",
    "    with open(output_fasta_path, \"w\") as outfile:\n",
    "        for fasta_file in original_fasta_files:\n",
    "            if not os.path.exists(fasta_file):\n",
    "                print(f\"WARNING: Original FASTA file not found, skipping: {fasta_file}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing {fasta_file}...\")\n",
    "            try:\n",
    "                for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "                    # --- MODIFIED LINE ---\n",
    "                    # Extract the ID part before the first pipe '|'\n",
    "                    # This assumes your CSV ID matches this part, e.g., 'RLI68853.1'\n",
    "                    current_id = record.id.split(\"|\")[0]\n",
    "                    # --- END MODIFIED LINE ---\n",
    "\n",
    "                    if current_id in unknown_ids:\n",
    "                        sequences_found += 1\n",
    "                        ids_found.add(current_id)\n",
    "                        SeqIO.write(record, outfile, \"fasta\")\n",
    "                        sequences_written += 1\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: Failed to parse {fasta_file}: {e}\")\n",
    "                # Decide if you want to continue or exit on parse error\n",
    "                # continue\n",
    "\n",
    "except IOError as e:\n",
    "    print(f\"ERROR: Could not write to output file {output_fasta_path}: {e}\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during sequence extraction: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "print(\"\\nExtraction complete.\")\n",
    "print(f\"Total sequences processed where ID matched an unknown ID: {sequences_found}\")\n",
    "print(f\"Total unique unknown IDs found in FASTA files: {len(ids_found)}\")\n",
    "print(f\"Total sequences written to {output_fasta_path}: {sequences_written}\")\n",
    "\n",
    "# --- Report missing IDs ---\n",
    "missing_ids = unknown_ids - ids_found\n",
    "if missing_ids:\n",
    "    print(\n",
    "        f\"\\nWARNING: {len(missing_ids)} unknown IDs were not found in the provided FASTA files.\"\n",
    "    )\n",
    "    # Optionally print the first few missing IDs\n",
    "    # print(\"Example missing IDs:\", list(missing_ids)[:10])\n",
    "    # Consider writing missing IDs to a file\n",
    "    missing_ids_file = \"missing_unknown_ids.txt\"\n",
    "    try:\n",
    "        with open(missing_ids_file, \"w\") as f_missing:\n",
    "            for mid in sorted(list(missing_ids)):\n",
    "                f_missing.write(mid + \"\\n\")\n",
    "        print(f\"Full list of missing IDs written to {missing_ids_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not write missing IDs file: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nAll unknown IDs were found in the provided FASTA files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b0e0f-4c72-4237-8dbd-487d25d46234",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The next cell follows running interproscan on the sequences that were left out of the original run. It integrates those IPR results into the main database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a3ce2-b615-429b-91b8-0ce1cc70e18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import numpy as np  # For checking NaN\n",
    "\n",
    "# --- Configuration: PLEASE UPDATE THESE PATHS ---\n",
    "\n",
    "# Path to the main database CSV file (output from the last classification run)\n",
    "main_database_csv = \"proteome_database_v0.3.csv\"\n",
    "\n",
    "# Path to the NEW InterProScan TSV output file (from the run on unknowns)\n",
    "# Adjust the filename if InterProScan generated a different one.\n",
    "new_ipr_tsv = \"InterProScan_Results/unknown_protein_sequences.fasta.tsv\"\n",
    "\n",
    "# Path for the updated output CSV file\n",
    "updated_database_csv = \"proteome_database_v0.4.csv\"\n",
    "\n",
    "# --- Column Names ---\n",
    "# Adjust these if your main CSV uses different names\n",
    "protein_id_col_main = \"ProteinID\"\n",
    "ipr_col_main = \"IPR_Signatures\"\n",
    "\n",
    "# --- Script Logic ---\n",
    "\n",
    "print(\"--- Starting Integration of New InterProScan Results ---\")\n",
    "\n",
    "# --- Step 1: Parse the new InterProScan TSV output ---\n",
    "print(f\"Reading new InterProScan results from: {new_ipr_tsv}\")\n",
    "\n",
    "if not os.path.exists(new_ipr_tsv):\n",
    "    print(f\"ERROR: New InterProScan TSV file not found: {new_ipr_tsv}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    # Define column names based on standard InterProScan TSV format\n",
    "    # We only need Protein ID (col 0) and IPR ID (col 11)\n",
    "    col_names = [\n",
    "        \"Protein_ID_raw\",\n",
    "        \"MD5\",\n",
    "        \"Length\",\n",
    "        \"Analysis\",\n",
    "        \"Sig_Acc\",\n",
    "        \"Sig_Desc\",\n",
    "        \"Start\",\n",
    "        \"Stop\",\n",
    "        \"Score\",\n",
    "        \"Status\",\n",
    "        \"Date\",\n",
    "        \"IPR_ID\",\n",
    "        \"IPR_Desc\",\n",
    "        \"GO\",\n",
    "        \"Pathway\",\n",
    "    ]\n",
    "    # Read only necessary columns, specify separator and no header\n",
    "    df_new_ipr = pd.read_csv(\n",
    "        new_ipr_tsv,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        names=col_names,\n",
    "        usecols=[\"Protein_ID_raw\", \"IPR_ID\"],  # Read raw ID first\n",
    "        dtype={\"Protein_ID_raw\": str, \"IPR_ID\": str},  # Read as string initially\n",
    "    )\n",
    "    print(f\"Read {len(df_new_ipr)} lines from TSV.\")\n",
    "\n",
    "    # --- *** ID PARSING FIX *** ---\n",
    "    # Extract the part before the first pipe '|' from the raw protein ID column\n",
    "    print(\"Parsing Protein IDs from TSV (removing extra info)...\")\n",
    "    df_new_ipr[\"Protein_ID\"] = df_new_ipr[\"Protein_ID_raw\"].str.split(\"|\").str[0]\n",
    "    # --- *** END ID PARSING FIX *** ---\n",
    "\n",
    "    # Filter out rows without an IPR ID (often represented as '-')\n",
    "    df_new_ipr.dropna(subset=[\"IPR_ID\"], inplace=True)\n",
    "    df_new_ipr = df_new_ipr[df_new_ipr[\"IPR_ID\"] != \"-\"]\n",
    "    print(f\"Found {len(df_new_ipr)} annotations with IPR IDs.\")\n",
    "\n",
    "    # --- Step 2: Aggregate IPR IDs per Protein ---\n",
    "    # Group by the *parsed* Protein ID and join unique IPR IDs with a semicolon\n",
    "    print(\"Aggregating new IPR IDs per protein...\")\n",
    "    # Ensure IDs are unique before joining\n",
    "    new_ipr_map_series = df_new_ipr.groupby(\"Protein_ID\")[\"IPR_ID\"].apply(\n",
    "        lambda x: \";\".join(sorted(x.unique()))\n",
    "    )\n",
    "\n",
    "    if new_ipr_map_series.empty:\n",
    "        print(\n",
    "            \"WARNING: No valid IPR signatures found in the new TSV file after processing.\"\n",
    "        )\n",
    "        # Decide whether to exit or continue (continuing will just rewrite the main CSV)\n",
    "        # sys.exit(1) # Uncomment to exit if no new IPRs found\n",
    "    else:\n",
    "        print(\n",
    "            f\"Aggregated new IPR signatures for {len(new_ipr_map_series)} unique proteins.\"\n",
    "        )\n",
    "        # Display some examples (optional)\n",
    "        # print(\"\\nExample aggregated signatures:\")\n",
    "        # print(new_ipr_map_series.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: New InterProScan TSV file not found at {new_ipr_tsv}\")\n",
    "    sys.exit(1)\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(f\"ERROR: New InterProScan TSV file is empty: {new_ipr_tsv}\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to read or process {new_ipr_tsv}: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# --- Step 3: Load the main database CSV ---\n",
    "print(f\"\\nReading main database CSV: {main_database_csv}\")\n",
    "\n",
    "if not os.path.exists(main_database_csv):\n",
    "    print(f\"ERROR: Main database CSV file not found: {main_database_csv}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    df_main = pd.read_csv(main_database_csv, low_memory=False)\n",
    "    print(f\"Read {len(df_main)} rows from main database.\")\n",
    "\n",
    "    # --- Validate required columns exist ---\n",
    "    if protein_id_col_main not in df_main.columns:\n",
    "        print(\n",
    "            f\"ERROR: Protein ID column '{protein_id_col_main}' not found in {main_database_csv}\"\n",
    "        )\n",
    "        sys.exit(1)\n",
    "    if ipr_col_main not in df_main.columns:\n",
    "        print(\n",
    "            f\"ERROR: IPR Signatures column '{ipr_col_main}' not found in {main_database_csv}\"\n",
    "        )\n",
    "        # If it doesn't exist, create it\n",
    "        print(f\"Creating column '{ipr_col_main}'...\")\n",
    "        df_main[ipr_col_main] = np.nan  # Initialize with NaN\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Main database CSV file not found at {main_database_csv}\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to read main database CSV {main_database_csv}: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# --- Step 4: Merge/Update IPR Signatures ---\n",
    "print(\"\\nUpdating IPR signatures in the main database...\")\n",
    "\n",
    "# Ensure the IPR column is treated as string for checking emptiness\n",
    "# Use .fillna('') before checking to handle actual NaN values correctly\n",
    "df_main[ipr_col_main] = df_main[ipr_col_main].fillna(\"\").astype(str)\n",
    "\n",
    "# Create a boolean mask for rows where IPR_Signatures is currently empty or effectively 'nan'\n",
    "is_empty_mask = (df_main[ipr_col_main].str.strip() == \"\") | (\n",
    "    df_main[ipr_col_main].str.strip().str.lower() == \"nan\"\n",
    ")\n",
    "\n",
    "# Get the Protein IDs for the rows that need updating (using the mask)\n",
    "ids_to_update = df_main.loc[is_empty_mask, protein_id_col_main]\n",
    "\n",
    "# Map the new signatures onto these IDs\n",
    "# Use .get() on the series map to handle IDs that might be in the main DB but not have new IPRs\n",
    "# signatures_to_apply = ids_to_update.map(lambda pid: new_ipr_map_series.get(pid)) # Not directly needed for update\n",
    "\n",
    "# Update the main DataFrame only where the original was empty AND new data exists\n",
    "# Create a combined mask: original was empty AND new signature was found\n",
    "update_mask = is_empty_mask & df_main[protein_id_col_main].isin(\n",
    "    new_ipr_map_series.index\n",
    ")\n",
    "\n",
    "# Get the number of rows that will actually be updated\n",
    "rows_to_be_updated_count = update_mask.sum()\n",
    "print(\n",
    "    f\"Found {rows_to_be_updated_count} rows with previously empty IPR signatures that have new results.\"\n",
    ")\n",
    "\n",
    "# Apply the update using .loc\n",
    "# We map directly from the series using the index alignment provided by Protein_ID\n",
    "# Ensure that only rows matching the update_mask are targeted\n",
    "if rows_to_be_updated_count > 0:\n",
    "    df_main.loc[update_mask, ipr_col_main] = df_main.loc[\n",
    "        update_mask, protein_id_col_main\n",
    "    ].map(new_ipr_map_series)\n",
    "    print(\"IPR signature update applied.\")\n",
    "else:\n",
    "    print(\"No rows needed updating.\")\n",
    "\n",
    "\n",
    "# --- Step 5: Save the updated database ---\n",
    "print(f\"\\nSaving updated database to: {updated_database_csv}\")\n",
    "\n",
    "try:\n",
    "    df_main.to_csv(updated_database_csv, index=False)\n",
    "    print(f\"Successfully wrote {len(df_main)} rows to {updated_database_csv}.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to write updated database CSV: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n--- Integration Finished ---\")\n",
    "print(\n",
    "    f\"\\nNext Step: Re-run the classification script (add_specific_category_IPR_v10.py) using '{updated_database_csv}' as the input CSV.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe04f3-34c0-4809-941a-ff26729178d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This cell adds the USPnet Signal Peptide Predictions into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b096b81-51e6-452f-8134-b2962af3ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Integrate USPNet Signal Peptide Predictions\n",
    "#\n",
    "# This cell reads the USPNet prediction results, aligns them with the main protein database using the original input FASTA order, and merges the predictions into the main dataframe.\n",
    "\n",
    "# %% [code]\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration: File Paths ---\n",
    "# Path to the main database CSV file (output from the last step, e.g., taxonomy refinement)\n",
    "main_csv_path = \"proteome_database_v0.6.csv\"\n",
    "\n",
    "# Path to the USPNet output CSV file (generated by predict_fast.py)\n",
    "uspnet_results_path = (\n",
    "    \"USPNet_Intermediate/results.csv\"  # Default location based on run_uspnet.sh\n",
    ")\n",
    "\n",
    "# Path to the FASTA file originally used as input for USPNet\n",
    "input_fasta_path = \"all_proteins_for_dtm.fasta\"\n",
    "\n",
    "# Path for the updated output CSV file\n",
    "output_csv_path = \"proteome_database_v0.7.csv\"\n",
    "\n",
    "# --- Configuration: Column Names ---\n",
    "# In main input CSV\n",
    "PROTEIN_ID_COL_MAIN = \"ProteinID\"\n",
    "# In USPNet output CSV\n",
    "USP_SEQUENCE_COL = \"sequence\"  # We won't use this directly for merging\n",
    "USP_PRED_TYPE_COL = \"predicted_type\"\n",
    "USP_PRED_CLEAVAGE_COL = \"predicted_cleavage\"\n",
    "# New columns to create in the output CSV\n",
    "NEW_SP_TYPE_COL = \"Signal_Peptide_USPNet\"\n",
    "NEW_SP_CLEAVAGE_COL = \"SP_Cleavage_Site_USPNet\"\n",
    "\n",
    "# --- Script Logic ---\n",
    "\n",
    "print(\"--- Starting Integration of USPNet Results ---\")\n",
    "print(f\"Main database CSV: {main_csv_path}\")\n",
    "print(f\"USPNet results CSV: {uspnet_results_path}\")\n",
    "print(f\"Original FASTA input for USPNet: {input_fasta_path}\")\n",
    "print(f\"Output database CSV: {output_csv_path}\")\n",
    "\n",
    "# --- Validate input files ---\n",
    "if not os.path.exists(main_csv_path):\n",
    "    print(f\"ERROR: Main database CSV not found: {main_csv_path}\")\n",
    "    # sys.exit(1) # Use raise instead in a notebook or handle differently\n",
    "    raise FileNotFoundError(f\"Main database CSV not found: {main_csv_path}\")\n",
    "if not os.path.exists(uspnet_results_path):\n",
    "    print(f\"ERROR: USPNet results CSV not found: {uspnet_results_path}\")\n",
    "    raise FileNotFoundError(f\"USPNet results CSV not found: {uspnet_results_path}\")\n",
    "if not os.path.exists(input_fasta_path):\n",
    "    print(f\"ERROR: Input FASTA file not found: {input_fasta_path}\")\n",
    "    raise FileNotFoundError(f\"Input FASTA file not found: {input_fasta_path}\")\n",
    "\n",
    "# --- Step 1: Read Protein IDs from FASTA in order ---\n",
    "print(f\"\\nReading Protein IDs from FASTA: {input_fasta_path}...\")\n",
    "protein_ids_in_order = []\n",
    "try:\n",
    "    for record in SeqIO.parse(input_fasta_path, \"fasta\"):\n",
    "        # Assuming the ID used by USPNet corresponds to record.id\n",
    "        protein_ids_in_order.append(record.id)\n",
    "    print(f\"Read {len(protein_ids_in_order)} IDs from FASTA.\")\n",
    "    if not protein_ids_in_order:\n",
    "        print(\"ERROR: No sequences found in the FASTA file.\")\n",
    "        raise ValueError(\"No sequences found in FASTA file.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to read or parse FASTA file {input_fasta_path}: {e}\")\n",
    "    raise  # Re-raise the exception\n",
    "\n",
    "# --- Step 2: Read USPNet results ---\n",
    "print(f\"Reading USPNet results: {uspnet_results_path}...\")\n",
    "try:\n",
    "    df_uspnet = pd.read_csv(uspnet_results_path)\n",
    "    print(f\"Read {len(df_uspnet)} rows from USPNet results.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to read USPNet results CSV {uspnet_results_path}: {e}\")\n",
    "    raise  # Re-raise the exception\n",
    "\n",
    "# --- Step 3: Check length consistency and add ProteinID ---\n",
    "if len(protein_ids_in_order) != len(df_uspnet):\n",
    "    print(\"ERROR: Mismatch in number of sequences!\")\n",
    "    print(\n",
    "        f\"  FASTA file '{input_fasta_path}' has {len(protein_ids_in_order)} sequences.\"\n",
    "    )\n",
    "    print(f\"  USPNet results '{uspnet_results_path}' has {len(df_uspnet)} rows.\")\n",
    "    print(\"Cannot reliably merge results. Please check the inputs.\")\n",
    "    raise ValueError(\"Mismatch between FASTA sequence count and USPNet result count.\")\n",
    "\n",
    "print(\"Adding ProteinID column to USPNet results based on FASTA order...\")\n",
    "df_uspnet[PROTEIN_ID_COL_MAIN] = protein_ids_in_order\n",
    "# Select and rename columns for merging\n",
    "df_uspnet_to_merge = df_uspnet[\n",
    "    [PROTEIN_ID_COL_MAIN, USP_PRED_TYPE_COL, USP_PRED_CLEAVAGE_COL]\n",
    "].copy()\n",
    "df_uspnet_to_merge.rename(\n",
    "    columns={\n",
    "        USP_PRED_TYPE_COL: NEW_SP_TYPE_COL,\n",
    "        USP_PRED_CLEAVAGE_COL: NEW_SP_CLEAVAGE_COL,\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# --- Step 4: Load main database ---\n",
    "print(f\"\\nLoading main database: {main_csv_path}...\")\n",
    "try:\n",
    "    df_main = pd.read_csv(main_csv_path, low_memory=False)\n",
    "    print(f\"Loaded {len(df_main)} rows.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load main database CSV {main_csv_path}: {e}\")\n",
    "    raise  # Re-raise the exception\n",
    "\n",
    "# --- Step 5: Merge USPNet results ---\n",
    "print(f\"Merging USPNet results into main database using '{PROTEIN_ID_COL_MAIN}'...\")\n",
    "\n",
    "# Check if merge columns already exist and remove them first if necessary\n",
    "if NEW_SP_TYPE_COL in df_main.columns:\n",
    "    print(\n",
    "        f\"Warning: Column '{NEW_SP_TYPE_COL}' already exists. It will be overwritten.\"\n",
    "    )\n",
    "    df_main.drop(columns=[NEW_SP_TYPE_COL], inplace=True)\n",
    "if NEW_SP_CLEAVAGE_COL in df_main.columns:\n",
    "    print(\n",
    "        f\"Warning: Column '{NEW_SP_CLEAVAGE_COL}' already exists. It will be overwritten.\"\n",
    "    )\n",
    "    df_main.drop(columns=[NEW_SP_CLEAVAGE_COL], inplace=True)\n",
    "\n",
    "# Perform left merge to keep all rows from the main database\n",
    "df_merged = pd.merge(\n",
    "    df_main,\n",
    "    df_uspnet_to_merge,\n",
    "    on=PROTEIN_ID_COL_MAIN,\n",
    "    how=\"left\",  # Keep all rows from df_main\n",
    ")\n",
    "\n",
    "# Check if merge resulted in the same number of rows\n",
    "if len(df_merged) != len(df_main):\n",
    "    print(\"ERROR: Merge resulted in an unexpected number of rows!\")\n",
    "    print(f\" Original rows: {len(df_main)}, Merged rows: {len(df_merged)}\")\n",
    "    raise ValueError(\"Merge changed the number of rows in the dataframe.\")\n",
    "\n",
    "# Report how many rows got USPNet data\n",
    "populated_uspnet = df_merged[NEW_SP_TYPE_COL].notna().sum()\n",
    "print(f\"Successfully merged USPNet data for {populated_uspnet} proteins.\")\n",
    "if populated_uspnet < len(df_uspnet_to_merge):\n",
    "    print(\n",
    "        f\"Warning: Some proteins with USPNet results ({len(df_uspnet_to_merge) - populated_uspnet}) were not found in the main database '{main_csv_path}'.\"\n",
    "    )\n",
    "\n",
    "# --- Step 6: Save Updated CSV ---\n",
    "print(f\"\\nSaving updated database to: {output_csv_path}\")\n",
    "try:\n",
    "    # Ensure new columns are strings, fill potential merge NaNs with empty string\n",
    "    df_merged[NEW_SP_TYPE_COL] = df_merged[NEW_SP_TYPE_COL].fillna(\"\").astype(str)\n",
    "    df_merged[NEW_SP_CLEAVAGE_COL] = (\n",
    "        df_merged[NEW_SP_CLEAVAGE_COL].fillna(\"\").astype(str)\n",
    "    )\n",
    "\n",
    "    df_merged.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Successfully wrote {len(df_merged)} rows to {output_csv_path}.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to write updated CSV: {e}\")\n",
    "    raise  # Re-raise the exception\n",
    "\n",
    "print(\"\\n--- USPNet Integration Finished ---\")\n",
    "print(f\"\\nNext Step: Integrate DeepTMHMM results into '{output_csv_path}'.\")\n",
    "\n",
    "# Display first few rows of the updated dataframe (optional, common in notebooks)\n",
    "print(\"\\nPreview of updated data:\")\n",
    "display(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4803ed6-7e19-403e-888e-f7a90ee90f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell creates a new column for predicted subcellular localization, based on the USPNet result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1335bd8-5a9b-465b-a24e-ce1dc1bb6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set the path to your CSV file\n",
    "csv_file_path = \"proteome_database_v0.7.csv\"\n",
    "\n",
    "# Define columns used\n",
    "# Adjust these if your column names are different\n",
    "protein_id_col = \"ProteinID\"  # Assuming this is your protein identifier column\n",
    "sequence_col = \"Sequence\"\n",
    "virus_family_col = \"Virus_Name\"\n",
    "archaea_phylum_col = \"Asgard_Phylum\"  # Or use 'Source_Dataset' if more reliable\n",
    "uspnet_col = \"Signal_Peptide_USPNet\"\n",
    "cleavage_site_col = (\n",
    "    \"SP_Cleavage_Site_USPNet\"  # Column with the sequence UP TO the cleavage site\n",
    ")\n",
    "\n",
    "# Define output columns\n",
    "localization_output_col = \"Predicted_Subcellular_Localization\"\n",
    "mature_seq_output_col = \"Mature_Protein_Sequence\"\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    print(f\"Successfully loaded '{csv_file_path}'. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"Error: File not found at '{csv_file_path}'. Please ensure the path is correct.\"\n",
    "    )\n",
    "    # Stop execution if file not found\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Data Validation ---\n",
    "# Check for essential columns for the entire process\n",
    "required_cols = [\n",
    "    protein_id_col,\n",
    "    sequence_col,\n",
    "    virus_family_col,\n",
    "    archaea_phylum_col,\n",
    "    uspnet_col,\n",
    "    cleavage_site_col,\n",
    "]\n",
    "missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(\n",
    "        f\"Error: The following required columns are missing from the CSV: {missing_cols}\"\n",
    "    )\n",
    "    raise KeyError(f\"Missing columns: {missing_cols}\")\n",
    "else:\n",
    "    print(\"All required columns found.\")\n",
    "\n",
    "# --- Define Organism Type ---\n",
    "is_virus = df[virus_family_col].notna()\n",
    "is_archaea = df[archaea_phylum_col].notna() & ~is_virus\n",
    "is_other = ~is_virus & ~is_archaea\n",
    "if is_other.sum() > 0:\n",
    "    warnings.warn(\n",
    "        f\"{is_other.sum()} proteins did not clearly map to 'Virus' or 'Archaea'. Localization/Mature sequence might be less specific.\"\n",
    "    )\n",
    "\n",
    "# --- Define Localization Logic ---\n",
    "conditions = [\n",
    "    is_archaea & (df[uspnet_col] == \"NO_SP\"),\n",
    "    is_archaea & (df[uspnet_col].isin([\"SP\", \"TAT\"])),\n",
    "    is_archaea & (df[uspnet_col].isin([\"LIPO\", \"TATLIPO\", \"PILIN\"])),\n",
    "    is_virus & (df[uspnet_col] == \"NO_SP\"),\n",
    "    is_virus & (df[uspnet_col].isin([\"SP\", \"TAT\"])),\n",
    "    is_virus & (df[uspnet_col].isin([\"LIPO\", \"TATLIPO\", \"PILIN\"])),\n",
    "]\n",
    "outputs = [\n",
    "    \"Archaea: Cytoplasmic/Membrane (non-SP)\",\n",
    "    \"Archaea: Secreted/Membrane (Sec/Tat pathway)\",\n",
    "    \"Archaea: Membrane-associated (Lipoprotein/Pilin)\",\n",
    "    \"Host: Cytoplasm/Nucleus/Virus Factory\",\n",
    "    \"Host: Secretory Pathway (Secreted/Membrane/Organelle)\",\n",
    "    \"Host: Membrane-associated (Lipoprotein/Pilin-like)\",\n",
    "]\n",
    "default_output = \"Unknown/Other\"\n",
    "\n",
    "# --- Apply Localization Logic ---\n",
    "df[localization_output_col] = np.select(conditions, outputs, default=default_output)\n",
    "print(f\"\\nAdded '{localization_output_col}' column.\")\n",
    "\n",
    "\n",
    "# --- Define Mature Sequence Logic ---\n",
    "def get_mature_sequence(row):\n",
    "    \"\"\"\n",
    "    Calculates the mature protein sequence based on USPNet prediction.\n",
    "    Removes the signal peptide sequence if predicted.\n",
    "    \"\"\"\n",
    "    full_sequence = row[sequence_col]\n",
    "    sp_type = row[uspnet_col]\n",
    "    cleavage_seq = row[cleavage_site_col]  # Sequence up to and including cleavage site\n",
    "\n",
    "    # Ensure sequence is a string (handle potential NaNs)\n",
    "    if not isinstance(full_sequence, str):\n",
    "        return None  # Or return '', or np.nan depending on desired handling\n",
    "\n",
    "    # Default to the full sequence\n",
    "    mature_sequence = full_sequence\n",
    "\n",
    "    # If a signal peptide is predicted (not NO_SP)\n",
    "    if sp_type != \"NO_SP\":\n",
    "        # Check if cleavage site sequence is valid (string, not empty)\n",
    "        if isinstance(cleavage_seq, str) and cleavage_seq:\n",
    "            # Check if the full sequence starts with the cleavage site sequence\n",
    "            if full_sequence.startswith(cleavage_seq):\n",
    "                # Calculate the length of the cleavage site sequence\n",
    "                cleavage_len = len(cleavage_seq)\n",
    "                # Extract the sequence *after* the cleavage site\n",
    "                mature_sequence = full_sequence[cleavage_len:]\n",
    "            else:\n",
    "                # Warning: Predicted SP but cleavage site doesn't match start of sequence\n",
    "                warnings.warn(\n",
    "                    f\"ProteinID {row[protein_id_col]}: SP predicted ({sp_type}), but cleavage site '{cleavage_seq[:20]}...' not found at start of sequence. Using full sequence.\"\n",
    "                )\n",
    "                mature_sequence = full_sequence  # Fallback to full sequence\n",
    "        else:\n",
    "            # Warning: Predicted SP but cleavage site data is missing/invalid\n",
    "            warnings.warn(\n",
    "                f\"ProteinID {row[protein_id_col]}: SP predicted ({sp_type}), but cleavage site data is missing or invalid. Using full sequence.\"\n",
    "            )\n",
    "            mature_sequence = full_sequence  # Fallback to full sequence\n",
    "\n",
    "    return mature_sequence\n",
    "\n",
    "\n",
    "# --- Apply Mature Sequence Logic ---\n",
    "print(\"\\nCalculating Mature Protein Sequences...\")\n",
    "df[mature_seq_output_col] = df.apply(get_mature_sequence, axis=1)\n",
    "print(f\"Added '{mature_seq_output_col}' column.\")\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\nValue Counts for Predicted Subcellular Localization:\")\n",
    "print(df[localization_output_col].value_counts())\n",
    "\n",
    "# Add sequence length columns for comparison\n",
    "df[\"Original_Seq_Length\"] = df[sequence_col].str.len()\n",
    "df[\"Mature_Seq_Length\"] = df[mature_seq_output_col].str.len()\n",
    "\n",
    "# Handle potential None values in Mature_Seq_Length if sequence was None\n",
    "df[\"Mature_Seq_Length\"] = df[\"Mature_Seq_Length\"].fillna(0).astype(int)\n",
    "\n",
    "print(\"\\nFirst 5 rows showing sequence length changes:\")\n",
    "display_cols = [\n",
    "    protein_id_col,\n",
    "    uspnet_col,\n",
    "    localization_output_col,\n",
    "    \"Original_Seq_Length\",\n",
    "    \"Mature_Seq_Length\",  # Show lengths instead of full sequences for brevity\n",
    "]\n",
    "# Ensure columns exist before trying to display them\n",
    "display_cols = [col for col in display_cols if col in df.columns]\n",
    "print(df[display_cols].head())\n",
    "\n",
    "# --- Optional: Save the updated DataFrame ---\n",
    "output_csv_path = \"proteome_database_v0.8.csv\"  # Consider incrementing version\n",
    "try:\n",
    "    # Select columns to save (optional, can save all)\n",
    "    # cols_to_save = [...]\n",
    "    # df.to_csv(output_csv_path, columns=cols_to_save, index=False)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nSuccessfully saved updated data to '{output_csv_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving updated data: {e}\")\n",
    "\n",
    "# Clean up temporary length columns if you don't want them saved\n",
    "# df = df.drop(columns=['Original_Seq_Length', 'Mature_Seq_Length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8a099-c1f2-4ffa-9cad-26f6d20c89d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44372534-2606-45cc-88e7-0c71aba8946c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to your main data file (output from the previous step)\n",
    "main_csv_path = \"proteome_database_v0.8.csv\"\n",
    "# Path to the mapping file provided\n",
    "mapping_file_path = \"mapping_parquet_proteinid_to_uniprotkb_or_upi.tsv\"\n",
    "# Column names used for merging and updating\n",
    "protein_id_col = \"ProteinID\"\n",
    "uniprot_col = \"UniProtKB_AC\"\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    # Load the main dataframe which should include the empty UniProtKB_AC column\n",
    "    df_main = pd.read_csv(main_csv_path)\n",
    "    print(f\"Successfully loaded main data '{main_csv_path}'. Shape: {df_main.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"Error: Main data file not found at '{main_csv_path}'. Make sure it was saved correctly in the previous step.\"\n",
    "    )\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the main CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Load the mapping file - assuming it's tab-separated (TSV)\n",
    "    df_mapping = pd.read_csv(mapping_file_path, sep=\"\\t\")\n",
    "    print(\n",
    "        f\"Successfully loaded mapping file '{mapping_file_path}'. Shape: {df_mapping.shape}\"\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"Error: Mapping file not found at '{mapping_file_path}'. Please ensure the path is correct.\"\n",
    "    )\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the mapping TSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Data Validation ---\n",
    "# Ensure necessary columns exist in both dataframes\n",
    "if protein_id_col not in df_main.columns:\n",
    "    print(\n",
    "        f\"Error: Column '{protein_id_col}' not found in main dataframe '{main_csv_path}'.\"\n",
    "    )\n",
    "    raise KeyError(f\"Missing column: {protein_id_col} in main dataframe\")\n",
    "if uniprot_col not in df_main.columns:\n",
    "    print(\n",
    "        f\"Warning: Column '{uniprot_col}' not found in main dataframe '{main_csv_path}'. It will be created.\"\n",
    "    )\n",
    "    # Add the column if it's missing (though it should exist based on your header)\n",
    "    df_main[uniprot_col] = np.nan\n",
    "\n",
    "if protein_id_col not in df_mapping.columns:\n",
    "    print(\n",
    "        f\"Error: Column '{protein_id_col}' not found in mapping dataframe '{mapping_file_path}'.\"\n",
    "    )\n",
    "    raise KeyError(f\"Missing column: {protein_id_col} in mapping dataframe\")\n",
    "if uniprot_col not in df_mapping.columns:\n",
    "    print(\n",
    "        f\"Error: Column '{uniprot_col}' not found in mapping dataframe '{mapping_file_path}'.\"\n",
    "    )\n",
    "    raise KeyError(f\"Missing column: {uniprot_col} in mapping dataframe\")\n",
    "\n",
    "# --- Prepare for Merge ---\n",
    "# Check how many rows currently have a UniProt AC in the main dataframe\n",
    "# Convert potential empty strings or placeholders to NaN for accurate counting\n",
    "df_main[uniprot_col] = df_main[uniprot_col].replace(\"\", np.nan)\n",
    "initial_filled_count = df_main[uniprot_col].notna().sum()\n",
    "print(\n",
    "    f\"\\nInitial count of non-empty '{uniprot_col}' entries in main dataframe: {initial_filled_count}\"\n",
    ")\n",
    "\n",
    "# Select only the necessary columns from the mapping file for efficiency\n",
    "df_mapping_subset = df_mapping[[protein_id_col, uniprot_col]].copy()\n",
    "\n",
    "# Handle potential duplicate ProteinIDs in the mapping file.\n",
    "# If a ProteinID maps to multiple UniProt ACs, this keeps the first one found.\n",
    "duplicates_in_mapping = df_mapping_subset[protein_id_col].duplicated().sum()\n",
    "if duplicates_in_mapping > 0:\n",
    "    print(\n",
    "        f\"Warning: Found {duplicates_in_mapping} duplicate '{protein_id_col}' entries in the mapping file. Keeping the first occurrence for each.\"\n",
    "    )\n",
    "    df_mapping_subset = df_mapping_subset.drop_duplicates(\n",
    "        subset=[protein_id_col], keep=\"first\"\n",
    "    )\n",
    "\n",
    "# --- Perform Merge ---\n",
    "# Use a left merge: keep all rows from df_main, add UniProt ACs from df_mapping_subset.\n",
    "# 'suffixes' handles the case where the uniprot_col already exists in df_main.\n",
    "# The original column remains unchanged, the merged data goes into 'UniProtKB_AC_mapped'.\n",
    "df_merged = pd.merge(\n",
    "    df_main,\n",
    "    df_mapping_subset,\n",
    "    on=protein_id_col,\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_mapped\"),  # Suffix for the column coming from the mapping file\n",
    ")\n",
    "\n",
    "# --- Update the UniProtKB_AC Column ---\n",
    "# Check if the merge created the new column with the suffix\n",
    "if uniprot_col + \"_mapped\" in df_merged.columns:\n",
    "    print(f\"Updating '{uniprot_col}' column with mapped values...\")\n",
    "    # Fill NaN values in the original UniProtKB_AC column using the mapped values.\n",
    "    # This preserves any existing values and only fills where it was originally NaN.\n",
    "    df_merged[uniprot_col] = df_merged[uniprot_col].fillna(\n",
    "        df_merged[uniprot_col + \"_mapped\"]\n",
    "    )\n",
    "\n",
    "    # Clean up: Drop the temporary mapped column\n",
    "    df_merged = df_merged.drop(columns=[uniprot_col + \"_mapped\"])\n",
    "else:\n",
    "    # This case is unlikely with the suffixes parameter used correctly, but included as a fallback.\n",
    "    print(\n",
    "        f\"Warning: Merge did not create a separate '{uniprot_col}_mapped' column. Check merge logic.\"\n",
    "    )\n",
    "\n",
    "# Ensure the updated column is treated as string (or object) to handle NaNs gracefully\n",
    "df_merged[uniprot_col] = df_merged[uniprot_col].astype(object)\n",
    "\n",
    "# --- Display Results ---\n",
    "final_filled_count = df_merged[uniprot_col].notna().sum()\n",
    "print(\"\\nMerge complete.\")\n",
    "print(f\"Final count of non-empty '{uniprot_col}' entries: {final_filled_count}\")\n",
    "print(f\"Number of entries newly filled: {final_filled_count - initial_filled_count}\")\n",
    "\n",
    "# Show some rows where the UniProt AC was potentially filled\n",
    "print(f\"\\nExample rows with '{uniprot_col}' populated (showing first 5):\")\n",
    "print(\n",
    "    df_merged[df_merged[uniprot_col].notna()][[protein_id_col, uniprot_col]]\n",
    "    .head()\n",
    "    .to_markdown(index=False, numalign=\"left\", stralign=\"left\")\n",
    ")\n",
    "\n",
    "# Show some rows where the UniProt AC might still be empty (no match in mapping file)\n",
    "print(f\"\\nExample rows where '{uniprot_col}' might still be empty (showing first 5):\")\n",
    "print(\n",
    "    df_merged[df_merged[uniprot_col].isna()][[protein_id_col, uniprot_col]]\n",
    "    .head()\n",
    "    .to_markdown(index=False, numalign=\"left\", stralign=\"left\")\n",
    ")\n",
    "\n",
    "# --- Save the updated DataFrame ---\n",
    "# Overwrite the original file with the updated data\n",
    "output_csv_path = \"proteome_database_v0.9.csv\"\n",
    "try:\n",
    "    df_merged.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nSuccessfully saved updated data back to '{output_csv_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving updated data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ae8471-9718-4fb6-ae2e-6bc916811ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell is part of the workflow to integrate uniprot accessions into the dataset, to ease structural filtering conducted later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f138a7-b9f2-42c0-b3b8-efb4aa29f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to your main data file (output from the previous step)\n",
    "main_csv_path = \"proteome_database_v0.8.csv\"  # Start with v0.8 again\n",
    "# Path to the mapping file provided (ensure this filename is correct)\n",
    "mapping_file_path = \"mapping_parquet_proteinid_to_uniprotkb_or_upi.tsv\"\n",
    "# Column names used for merging and updating\n",
    "protein_id_col = \"ProteinID\"\n",
    "uniprot_col = \"UniProtKB_AC\"\n",
    "# Temporary column for merging based on base ID (without version suffix)\n",
    "base_id_col = protein_id_col + \"_base\"\n",
    "# Output file path\n",
    "output_csv_path = \"proteome_database_v0.9.csv\"\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    # Load the main dataframe\n",
    "    df_main = pd.read_csv(main_csv_path)\n",
    "    print(f\"Successfully loaded main data '{main_csv_path}'. Shape: {df_main.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Main data file not found at '{main_csv_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the main CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Load the mapping file - assuming it's tab-separated (TSV)\n",
    "    df_mapping = pd.read_csv(mapping_file_path, sep=\"\\t\")\n",
    "    print(\n",
    "        f\"Successfully loaded mapping file '{mapping_file_path}'. Shape: {df_mapping.shape}\"\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Mapping file not found at '{mapping_file_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the mapping TSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Data Validation ---\n",
    "# Ensure necessary columns exist in both dataframes\n",
    "required_cols_main = [protein_id_col, uniprot_col]\n",
    "required_cols_map = [protein_id_col, uniprot_col]\n",
    "\n",
    "if not all(col in df_main.columns for col in required_cols_main):\n",
    "    missing = [col for col in required_cols_main if col not in df_main.columns]\n",
    "    print(f\"Error: Columns {missing} not found in main dataframe '{main_csv_path}'.\")\n",
    "    raise KeyError(\"Missing columns in main dataframe\")\n",
    "\n",
    "if not all(col in df_mapping.columns for col in required_cols_map):\n",
    "    missing = [col for col in required_cols_map if col not in df_mapping.columns]\n",
    "    print(\n",
    "        f\"Error: Columns {missing} not found in mapping dataframe '{mapping_file_path}'.\"\n",
    "    )\n",
    "    raise KeyError(\"Missing columns in mapping dataframe\")\n",
    "\n",
    "# --- Create Base ID for Merging ---\n",
    "print(\"\\nCreating base ID columns (without version suffix) for merging...\")\n",
    "# Main data: Split ProteinID at '.' and take the first part\n",
    "df_main[base_id_col] = df_main[protein_id_col].astype(str).str.split(\".\", n=1).str[0]\n",
    "# Mapping data: Assume it already lacks suffix, just copy\n",
    "df_mapping[base_id_col] = (\n",
    "    df_mapping[protein_id_col].astype(str).str.strip()\n",
    ")  # Ensure clean\n",
    "\n",
    "# --- Debugging: Inspect IDs ---\n",
    "print(\"\\n--- Debugging Info ---\")\n",
    "print(f\"First 5 '{protein_id_col}' values from main data:\")\n",
    "print(df_main[protein_id_col].head().to_list())\n",
    "print(f\"First 5 '{base_id_col}' values from main data:\")\n",
    "print(df_main[base_id_col].head().to_list())\n",
    "\n",
    "print(f\"\\nFirst 5 '{protein_id_col}' values from mapping data:\")\n",
    "print(df_mapping[protein_id_col].head().to_list())\n",
    "print(f\"First 5 '{base_id_col}' values from mapping data:\")\n",
    "print(df_mapping[base_id_col].head().to_list())\n",
    "\n",
    "\n",
    "# --- Debugging: Check Overlap using Base ID ---\n",
    "main_base_ids = set(df_main[base_id_col])\n",
    "mapping_base_ids = set(df_mapping[base_id_col])\n",
    "overlapping_base_ids = main_base_ids.intersection(mapping_base_ids)\n",
    "print(f\"\\nNumber of unique Base IDs in main data: {len(main_base_ids)}\")\n",
    "print(f\"Number of unique Base IDs in mapping data: {len(mapping_base_ids)}\")\n",
    "print(\n",
    "    f\"Number of Base IDs overlapping between the two files: {len(overlapping_base_ids)}\"\n",
    ")\n",
    "\n",
    "if len(overlapping_base_ids) == 0:\n",
    "    print(\n",
    "        \">>> Critical Issue: No Base IDs match between the two files! Merge will result in no changes.\"\n",
    "    )\n",
    "else:\n",
    "    print(\">>> Some overlapping Base IDs found. Proceeding with merge.\")\n",
    "print(\"--- End Debugging Info ---\")\n",
    "\n",
    "\n",
    "# --- Prepare for Merge ---\n",
    "# Check how many rows currently have a UniProt AC in the main dataframe\n",
    "df_main[uniprot_col] = df_main[uniprot_col].replace(\n",
    "    \"\", np.nan\n",
    ")  # Ensure empty strings are NaN\n",
    "initial_filled_count = df_main[uniprot_col].notna().sum()\n",
    "print(\n",
    "    f\"\\nInitial count of non-empty '{uniprot_col}' entries in main dataframe: {initial_filled_count}\"\n",
    ")\n",
    "\n",
    "# Select only the necessary columns from the mapping file for the merge\n",
    "# Include the base_id_col and the uniprot_col\n",
    "df_mapping_subset = df_mapping[[base_id_col, uniprot_col]].copy()\n",
    "\n",
    "# Handle potential duplicate Base IDs in the mapping file.\n",
    "# If a Base ID maps to multiple UniProt ACs, this keeps the first one found.\n",
    "duplicates_in_mapping = df_mapping_subset[base_id_col].duplicated().sum()\n",
    "if duplicates_in_mapping > 0:\n",
    "    print(\n",
    "        f\"Warning: Found {duplicates_in_mapping} duplicate '{base_id_col}' entries in the mapping file. Keeping the first occurrence for each.\"\n",
    "    )\n",
    "    df_mapping_subset = df_mapping_subset.drop_duplicates(\n",
    "        subset=[base_id_col], keep=\"first\"\n",
    "    )\n",
    "\n",
    "# --- Perform Merge on Base ID ---\n",
    "# Use a left merge: keep all rows from df_main, add UniProt ACs based on base_id_col match.\n",
    "print(f\"\\nPerforming merge on '{base_id_col}'...\")\n",
    "df_merged = pd.merge(\n",
    "    df_main,\n",
    "    df_mapping_subset,\n",
    "    on=base_id_col,  # Merge using the base ID column\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_mapped\"),  # Suffix for the UniProtKB_AC column coming from mapping\n",
    ")\n",
    "\n",
    "# --- Update the UniProtKB_AC Column ---\n",
    "if uniprot_col + \"_mapped\" in df_merged.columns:\n",
    "    print(f\"Updating '{uniprot_col}' column with mapped values...\")\n",
    "    # Use .fillna() to update only where the original column is NaN\n",
    "    # This prevents overwriting any existing values if df_main already had some\n",
    "    df_merged[uniprot_col] = df_merged[uniprot_col].fillna(\n",
    "        df_merged[uniprot_col + \"_mapped\"]\n",
    "    )\n",
    "    # Clean up: Drop the temporary mapped column\n",
    "    df_merged = df_merged.drop(columns=[uniprot_col + \"_mapped\"])\n",
    "else:\n",
    "    print(f\"Warning: Merge did not create a separate '{uniprot_col}_mapped' column.\")\n",
    "\n",
    "# Clean up the temporary base ID column from the final dataframe\n",
    "if base_id_col in df_merged.columns:\n",
    "    df_merged = df_merged.drop(columns=[base_id_col])\n",
    "    print(f\"Removed temporary column '{base_id_col}'.\")\n",
    "\n",
    "\n",
    "# Ensure the updated column is treated as string (or object)\n",
    "df_merged[uniprot_col] = df_merged[uniprot_col].astype(object)\n",
    "\n",
    "# --- Display Results ---\n",
    "final_filled_count = df_merged[uniprot_col].notna().sum()\n",
    "print(\"\\nMerge complete.\")\n",
    "print(f\"Final count of non-empty '{uniprot_col}' entries: {final_filled_count}\")\n",
    "print(f\"Number of entries newly filled: {final_filled_count - initial_filled_count}\")\n",
    "\n",
    "# Show some rows where the UniProt AC was potentially filled\n",
    "print(f\"\\nExample rows with '{uniprot_col}' populated (showing first 5):\")\n",
    "print(\n",
    "    df_merged[df_merged[uniprot_col].notna()][[protein_id_col, uniprot_col]]\n",
    "    .head()\n",
    "    .to_markdown(index=False, numalign=\"left\", stralign=\"left\")\n",
    ")\n",
    "\n",
    "# Show some rows where the UniProt AC might still be empty\n",
    "print(f\"\\nExample rows where '{uniprot_col}' might still be empty (showing first 5):\")\n",
    "print(\n",
    "    df_merged[df_merged[uniprot_col].isna()][[protein_id_col, uniprot_col]]\n",
    "    .head()\n",
    "    .to_markdown(index=False, numalign=\"left\", stralign=\"left\")\n",
    ")\n",
    "\n",
    "# --- Save the updated DataFrame ---\n",
    "try:\n",
    "    df_merged.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nSuccessfully saved updated data back to '{output_csv_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving updated data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4bc7ec-a2ab-4265-acdf-349dee12ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell adds a column indicating whether a protein has an entry in the AlphaFold database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63131f2-96a6-4ccd-b748-2beed0fa6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to your updated main data file\n",
    "main_csv_path = \"proteome_database_v0.9.csv\"\n",
    "# Path to the file containing UniProt ACs found in AFDB\n",
    "afdb_uniprot_list_path = (\n",
    "    \"PDB_AFDB_screening/afdb_processing_output/afdb_found_uniprot_acs_or_upi.csv\"\n",
    ")\n",
    "# Column names\n",
    "uniprot_col = \"UniProtKB_AC\"\n",
    "protein_id_col = \"ProteinID\"  # Used for displaying examples\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    # Load the main dataframe (which now has UniProtKB_AC populated)\n",
    "    df_main = pd.read_csv(main_csv_path)\n",
    "    print(f\"Successfully loaded main data '{main_csv_path}'. Shape: {df_main.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Main data file not found at '{main_csv_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the main CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Load the AFDB UniProt list file\n",
    "    # Assuming it's a CSV and the UniProt IDs are in the first column (index 0)\n",
    "    # Adjust 'header=None' and 'usecols=[0]' if the file has headers or a different structure\n",
    "    df_afdb_uniprot = pd.read_csv(\n",
    "        afdb_uniprot_list_path, header=None, usecols=[0], names=[uniprot_col]\n",
    "    )\n",
    "    print(\n",
    "        f\"Successfully loaded AFDB UniProt list '{afdb_uniprot_list_path}'. Shape: {df_afdb_uniprot.shape}\"\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: AFDB UniProt list file not found at '{afdb_uniprot_list_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the AFDB UniProt list CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Process and Count ---\n",
    "# Get the set of unique UniProt ACs from the AFDB list\n",
    "# Drop any potential NaN values and ensure they are strings\n",
    "afdb_uniprot_set = set(df_afdb_uniprot[uniprot_col].dropna().astype(str))\n",
    "print(f\"\\nFound {len(afdb_uniprot_set)} unique UniProt ACs in the AFDB list file.\")\n",
    "\n",
    "# Filter the main dataframe:\n",
    "# 1. Keep rows where UniProtKB_AC is not NaN/null\n",
    "# 2. Keep rows where the UniProtKB_AC is present in the afdb_uniprot_set\n",
    "df_main_filtered = df_main[\n",
    "    df_main[uniprot_col].notna()\n",
    "    & df_main[uniprot_col].astype(str).isin(afdb_uniprot_set)\n",
    "].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Count the number of rows in the filtered dataframe\n",
    "overlap_count = len(df_main_filtered)\n",
    "total_with_uniprot = df_main[uniprot_col].notna().sum()\n",
    "\n",
    "print(f\"\\nTotal entries in main database with a UniProtKB_AC: {total_with_uniprot}\")\n",
    "print(\n",
    "    f\"Number of entries whose UniProtKB_AC was found in the AFDB list: {overlap_count}\"\n",
    ")\n",
    "\n",
    "if total_with_uniprot > 0:\n",
    "    percentage = (overlap_count / total_with_uniprot) * 100\n",
    "    print(f\"Percentage of mapped entries found in AFDB list: {percentage:.2f}%\")\n",
    "\n",
    "# Display a few examples of overlapping proteins\n",
    "print(\"\\nExample ProteinIDs whose UniProtKB_AC matched the AFDB list (first 5):\")\n",
    "print(\n",
    "    df_main_filtered[[protein_id_col, uniprot_col]]\n",
    "    .head()\n",
    "    .to_markdown(index=False, numalign=\"left\", stralign=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91af9fc-19dd-4493-a680-96faba7589cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell parses the results files from \"Metagenomic-scale analysis of the predicted protein structure universe\" (https://doi.org/10.1101/2025.04.23.650224). It generates a list of the protein IDs found only in the ESM Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8755ffa4-4fea-43bb-876e-abe3e8a8dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re  # Import regular expressions for UniProt ID check\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to the AFESM cluster metadata file (unzipped)\n",
    "cluster_meta_path = \"2-repID_isOnlyESM_nMem_nAllMem_repPlddt_avgPlddt_avgAllPlddt_repLen_avgLen_avgAllLen_LCAtaxID_nBiome_LCBID.tsv\"\n",
    "# Path to the AFESM cluster membership file (unzipped)\n",
    "cluster_members_path = \"1-AFESMClusters-repId_memId_cluFlag_taxId_biomeId.tsv\"  # File 1 from description, assuming entryId is memId based on docs\n",
    "# Output file for the filtered UniProt IDs\n",
    "output_ids_path = \"afesm_esm_only_uniprot_ids.txt\"\n",
    "\n",
    "# Define expected column names for AFESM files based on documentation\n",
    "# File 2: Cluster Metadata\n",
    "meta_cols = [\n",
    "    \"repID\",\n",
    "    \"isOnlyESM\",\n",
    "    \"nMem\",\n",
    "    \"nAllMem\",\n",
    "    \"repPlddt\",\n",
    "    \"avgPlddt\",\n",
    "    \"avgAllPlddt\",\n",
    "    \"repLen\",\n",
    "    \"avgLen\",\n",
    "    \"avgAllLen\",\n",
    "    \"LCAtaxID\",\n",
    "    \"nBiome\",\n",
    "    \"LCBID\",\n",
    "]\n",
    "# File 1: Cluster Membership - Assuming entryId is the member ID based on file name\n",
    "members_cols = [\n",
    "    \"repId\",\n",
    "    \"memId\",\n",
    "    \"cluFlag\",\n",
    "    \"taxId\",\n",
    "    \"biomeID\",\n",
    "]  # Using File 1 description, mapping entryId to memId\n",
    "\n",
    "# Variables to hold specific column names used in the script\n",
    "afesm_rep_id_col = \"repID\"  # Use 'repID' from meta_cols for consistency\n",
    "afesm_member_id_col = \"memId\"  # From members_cols\n",
    "afesm_is_esm_only_col = \"isOnlyESM\"  # From meta_cols\n",
    "\n",
    "# --- Load Cluster Metadata ---\n",
    "try:\n",
    "    # Load cluster metadata, specifying NO header and assigning names.\n",
    "    df_meta = pd.read_csv(cluster_meta_path, sep=\"\\t\", header=None, names=meta_cols)\n",
    "    print(\n",
    "        f\"Successfully loaded cluster metadata '{cluster_meta_path}' (no header, unzipped). Shape: {df_meta.shape}\"\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Cluster metadata file not found at '{cluster_meta_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading cluster metadata: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Identify ESM-only Cluster Representatives ---\n",
    "# Ensure the isOnlyESM column is numeric\n",
    "df_meta[afesm_is_esm_only_col] = pd.to_numeric(\n",
    "    df_meta[afesm_is_esm_only_col], errors=\"coerce\"\n",
    ")\n",
    "esm_only_reps = set(\n",
    "    df_meta[df_meta[afesm_is_esm_only_col] == 1][afesm_rep_id_col].unique()\n",
    ")\n",
    "print(f\"\\nIdentified {len(esm_only_reps)} representative IDs for ESM-only clusters.\")\n",
    "\n",
    "# Free up memory from df_meta as it's no longer needed\n",
    "del df_meta\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "print(\"Freed memory from metadata dataframe.\")\n",
    "\n",
    "if not esm_only_reps:\n",
    "    print(\"Warning: No ESM-only cluster representatives found. Stopping.\")\n",
    "    # Exit if no reps found, otherwise the next step processes everything\n",
    "    exit()\n",
    "\n",
    "# --- Process Cluster Members in Chunks (to save memory) ---\n",
    "print(f\"\\nProcessing cluster members file '{cluster_members_path}' in chunks...\")\n",
    "chunk_size = 10_000_000  # Process 10 million rows at a time, adjust if needed\n",
    "esm_only_uniprot_ids_set = set()\n",
    "processed_rows = 0\n",
    "\n",
    "try:\n",
    "    # Use iterator=True and get_chunk to read the large file piece by piece\n",
    "    chunk_iterator = pd.read_csv(\n",
    "        cluster_members_path,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        names=members_cols,\n",
    "        low_memory=False,\n",
    "        iterator=True,\n",
    "        chunksize=chunk_size,\n",
    "    )\n",
    "\n",
    "    for i, chunk in enumerate(chunk_iterator):\n",
    "        processed_rows += len(chunk)\n",
    "        print(\n",
    "            f\"  Processing chunk {i + 1} (Rows {processed_rows - chunk_size + 1} - {processed_rows})...\"\n",
    "        )\n",
    "\n",
    "        # Ensure correct data types for filtering\n",
    "        chunk[\"repId\"] = chunk[\"repId\"].astype(str)\n",
    "        chunk[afesm_member_id_col] = chunk[afesm_member_id_col].astype(str)\n",
    "\n",
    "        # Filter the chunk for members belonging to ESM-only clusters\n",
    "        esm_only_members_chunk = chunk[chunk[\"repId\"].isin(esm_only_reps)]\n",
    "\n",
    "        # Extract member IDs from this chunk\n",
    "        member_ids_chunk = set(esm_only_members_chunk[afesm_member_id_col].dropna())\n",
    "\n",
    "        # Filter for potential UniProt IDs and add to the main set\n",
    "        for mem_id in member_ids_chunk:\n",
    "            if not mem_id.startswith(\"MGYP\") and not mem_id.startswith(\"pdb|\"):\n",
    "                esm_only_uniprot_ids_set.add(mem_id)\n",
    "\n",
    "        print(\n",
    "            f\"    Found {len(member_ids_chunk)} members in ESM-only clusters in this chunk.\"\n",
    "        )\n",
    "        print(\n",
    "            f\"    Current total unique potential UniProt IDs found: {len(esm_only_uniprot_ids_set)}\"\n",
    "        )\n",
    "\n",
    "    print(f\"\\nFinished processing {processed_rows} rows from cluster members file.\")\n",
    "    print(\n",
    "        f\"Total unique potential UniProt IDs found in ESM-only clusters: {len(esm_only_uniprot_ids_set)}\"\n",
    "    )\n",
    "\n",
    "    # --- Save the Filtered IDs to a File ---\n",
    "    print(f\"\\nSaving filtered UniProt IDs to '{output_ids_path}'...\")\n",
    "    with open(output_ids_path, \"w\") as f_out:\n",
    "        for uniprot_id in sorted(\n",
    "            list(esm_only_uniprot_ids_set)\n",
    "        ):  # Sort for consistency\n",
    "            f_out.write(f\"{uniprot_id}\\n\")\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Cluster members file not found at '{cluster_members_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while processing cluster members: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572db890-075d-40c0-808d-0d47e15d8a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell queries the proteome_database entries without AFDB entries against the ESM Atlas-only list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e3dcde-9460-4737-b18a-b95394019a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to your main data file\n",
    "main_csv_path = \"proteome_database_v0.9.csv\"\n",
    "# Path to the filtered list of ESM-only UniProt IDs created in the previous step\n",
    "afesm_ids_path = \"afesm_esm_only_uniprot_ids.txt\"\n",
    "# Column names in your main file\n",
    "protein_id_col = \"ProteinID\"\n",
    "uniprot_col = \"UniProtKB_AC\"\n",
    "# Optional: Output file for your proteins that matched\n",
    "output_match_path = \"proteins_hitting_afesm_esm_only_uniprot.csv\"\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    df_main = pd.read_csv(main_csv_path)\n",
    "    print(f\"Successfully loaded main data '{main_csv_path}'. Shape: {df_main.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Main data file not found at '{main_csv_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the main CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Load the filtered AFESM UniProt IDs into a set for efficient lookup\n",
    "    print(f\"Loading filtered AFESM UniProt IDs from '{afesm_ids_path}'...\")\n",
    "    afesm_esm_only_uniprot_ids_list = []  # Load into list first for inspection\n",
    "    with open(afesm_ids_path, \"r\") as f_in:\n",
    "        for line in f_in:\n",
    "            stripped_line = line.strip()\n",
    "            if stripped_line:  # Ensure non-empty lines\n",
    "                afesm_esm_only_uniprot_ids_list.append(stripped_line)\n",
    "\n",
    "    # Convert to uppercase when creating the set\n",
    "    afesm_esm_only_uniprot_ids_set = {\n",
    "        item.upper() for item in afesm_esm_only_uniprot_ids_list\n",
    "    }\n",
    "    print(\n",
    "        f\"Successfully loaded {len(afesm_esm_only_uniprot_ids_set)} unique UPPERCASE IDs into a set.\"\n",
    "    )\n",
    "\n",
    "    # --- Debugging: Print first few loaded IDs ---\n",
    "    print(\"\\n--- Debugging Info ---\")\n",
    "    print(\"First 10 UniProt IDs loaded from AFESM list file (as read):\")\n",
    "    print(afesm_esm_only_uniprot_ids_list[:10])\n",
    "    print(\"First 10 UniProt IDs from AFESM list file (UPPERCASE set sample):\")\n",
    "    # Convert set back to list temporarily for slicing sample\n",
    "    print(list(afesm_esm_only_uniprot_ids_set)[:10])\n",
    "    print(\"--- End Debugging Info ---\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"Error: AFESM ID list file not found at '{afesm_ids_path}'. Make sure the previous step ran correctly.\"\n",
    "    )\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the AFESM ID list: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Compare Your Database UniProt IDs to the AFESM Set ---\n",
    "# Get the set of non-null UniProt ACs from your main database, convert to uppercase\n",
    "main_uniprot_list = (\n",
    "    df_main[uniprot_col].dropna().astype(str).tolist()\n",
    ")  # Get as list first\n",
    "main_uniprot_ids = {\n",
    "    item.upper() for item in main_uniprot_list\n",
    "}  # Convert to uppercase set\n",
    "print(\n",
    "    f\"\\nFound {len(main_uniprot_ids)} unique UPPERCASE UniProt ACs in your main database.\"\n",
    ")\n",
    "\n",
    "# --- Debugging: Print first few loaded IDs from main DB ---\n",
    "print(\"\\n--- Debugging Info ---\")\n",
    "print(\"First 10 non-null UniProt IDs loaded from main database (as read):\")\n",
    "print(\n",
    "    main_uniprot_list[:10] if main_uniprot_list else \"No UniProt IDs found in main DB\"\n",
    ")\n",
    "print(\"First 10 non-null UniProt IDs from main database (UPPERCASE set sample):\")\n",
    "print(\n",
    "    list(main_uniprot_ids)[:10]\n",
    "    if main_uniprot_ids\n",
    "    else \"No UniProt IDs found in main DB\"\n",
    ")\n",
    "print(\"--- End Debugging Info ---\")\n",
    "\n",
    "\n",
    "# Find the intersection\n",
    "print(\"\\nPerforming intersection...\")\n",
    "overlapping_uniprot_ids = main_uniprot_ids.intersection(afesm_esm_only_uniprot_ids_set)\n",
    "overlap_count = len(overlapping_uniprot_ids)\n",
    "\n",
    "print(\n",
    "    f\"\\nNumber of your UniProtKB_ACs found within the AFESM ESM-only UniProt ID set: {overlap_count}\"\n",
    ")\n",
    "\n",
    "if len(main_uniprot_ids) > 0:\n",
    "    percentage = (overlap_count / len(main_uniprot_ids)) * 100\n",
    "    print(\n",
    "        f\"Percentage of your mapped UniProt IDs found in the AFESM ESM-only set: {percentage:.2f}%\"\n",
    "    )\n",
    "else:\n",
    "    print(\"No UniProt IDs found in your main database to compare.\")\n",
    "\n",
    "# --- Display Results & Save ---\n",
    "# Filter the main dataframe to show the proteins that matched\n",
    "# Need to compare the uppercase version of the column to the overlapping set\n",
    "df_main_matched = df_main[\n",
    "    df_main[uniprot_col].astype(str).str.upper().isin(overlapping_uniprot_ids)\n",
    "].copy()\n",
    "\n",
    "\n",
    "print(\n",
    "    \"\\nExample ProteinIDs whose UniProtKB_AC matched the AFESM ESM-only set (first 10):\"\n",
    ")\n",
    "print(\n",
    "    df_main_matched[[protein_id_col, uniprot_col]]\n",
    "    .head(10)\n",
    "    .to_markdown(index=False, numalign=\"left\", stralign=\"left\")\n",
    ")\n",
    "\n",
    "# Save the list of your ProteinIDs that hit the ESM-only set\n",
    "if not df_main_matched.empty:\n",
    "    try:\n",
    "        df_main_matched[[protein_id_col, uniprot_col]].to_csv(\n",
    "            output_match_path, index=False\n",
    "        )\n",
    "        print(\n",
    "            f\"\\nSaved list of matching ProteinIDs ({overlap_count} entries) to '{output_match_path}'\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving matching ProteinIDs: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo matching proteins found to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c739868d-df5f-48a2-9cf4-76305c24bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes the virus names and classifies them into families, creating a new Virus_Family column in the database. This allows for more robust classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ae3f7-a250-40eb-9afd-7af96d3e62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input file path (output from the previous step)\n",
    "input_csv_path = \"proteome_database_v10.csv\"\n",
    "# Output file path\n",
    "output_csv_path = \"proteome_database_v11.csv\"\n",
    "# Column containing the virus names\n",
    "virus_name_col = \"Virus_Name\"\n",
    "# Column to create/update with the family assignment\n",
    "virus_family_col = \"Virus_Family\"\n",
    "\n",
    "# --- Keyword to Family Mapping ---\n",
    "# Dictionary mapping lowercase keywords to family names.\n",
    "# Order matters: More specific keywords should come before less specific ones.\n",
    "# (e.g., 'moumouvirus' before 'virus')\n",
    "# Based on unique_virus_names.txt and known NCLDV families/groups.\n",
    "FAMILY_MAP = {\n",
    "    # Mimiviridae and related groups (Imitervirales)\n",
    "    \"mimivirus\": \"Mimiviridae\",\n",
    "    \"mamavirus\": \"Mimiviridae\",\n",
    "    \"moumouvirus\": \"Mimiviridae\",\n",
    "    \"megavirus\": \"Mimiviridae\",  # Proposed Megaviridae, often grouped\n",
    "    \"tupanvirus\": \"Mimiviridae\",  # Proposed Kheliviricetes\n",
    "    \"cedratvirus\": \"Mimiviridae\",  # Proposed Pithoviricetes\n",
    "    \"faustovirus\": \"Mimiviridae\",  # Proposed Duplodnaviria\n",
    "    \"pacmanvirus\": \"Mimiviridae\",  # Proposed Duplodnaviria\n",
    "    \"orpheovirus\": \"Mimiviridae\",  # Proposed Pithoviricetes\n",
    "    \"klosneuvirus\": \"Mimiviridae\",  # Proposed Klosneuvirinae\n",
    "    \"catovirus\": \"Mimiviridae\",  # Proposed Klosneuvirinae\n",
    "    \"hokovirus\": \"Mimiviridae\",  # Proposed Klosneuvirinae\n",
    "    \"indivirus\": \"Mimiviridae\",  # Proposed Klosneuvirinae\n",
    "    \"samba_virus\": \"Mimiviridae\",\n",
    "    \"bandra_megavirus\": \"Mimiviridae\",  # Likely related\n",
    "    \"niemeyer_virus\": \"Mimiviridae\",\n",
    "    \"terrestrivirus\": \"Mimiviridae\",\n",
    "    \"harvfovirus\": \"Mimiviridae\",\n",
    "    \"barrevirus\": \"Mimiviridae\",\n",
    "    \"dasosvirus\": \"Mimiviridae\",\n",
    "    \"gaeavirus\": \"Mimiviridae\",\n",
    "    \"satyrvirus\": \"Mimiviridae\",\n",
    "    \"hirudovirus\": \"Mimiviridae\",\n",
    "    \"edafosvirus\": \"Mimiviridae\",\n",
    "    \"homavirus\": \"Mimiviridae\",\n",
    "    \"acanthamoeba_polyphaga_lentillevirus\": \"Mimiviridae\",  # Added back\n",
    "    \"cotonvirus\": \"Mimiviridae\",\n",
    "    \"hyperionvirus\": \"Mimiviridae\",\n",
    "    \"powai_lake_megavirus\": \"Mimiviridae\",  # Likely related\n",
    "    # Marseilleviridae\n",
    "    \"marseillevirus\": \"Marseilleviridae\",\n",
    "    \"lausannevirus\": \"Marseilleviridae\",\n",
    "    \"tokyovirus\": \"Marseilleviridae\",\n",
    "    \"noumeavirus\": \"Marseilleviridae\",\n",
    "    \"kurlavirus\": \"Marseilleviridae\",\n",
    "    \"port-miou_virus\": \"Marseilleviridae\",\n",
    "    \"golden marseillevirus\": \"Marseilleviridae\",  # Handle space\n",
    "    # Phycodnaviridae\n",
    "    \"phycodnavirus\": \"Phycodnaviridae\",\n",
    "    \"chlorella_virus\": \"Phycodnaviridae\",\n",
    "    \"ostreococcus_virus\": \"Phycodnaviridae\",\n",
    "    \"micromonas_pusilla_virus\": \"Phycodnaviridae\",\n",
    "    \"bathycoccus_virus\": \"Phycodnaviridae\",\n",
    "    \"phaeocystis_globosa_virus\": \"Phycodnaviridae\",\n",
    "    \"emiliania_huxleyi_virus\": \"Phycodnaviridae\",\n",
    "    \"chrysochromulina_virus\": \"Phycodnaviridae\",\n",
    "    \"feldmannia_virus\": \"Phycodnaviridae\",\n",
    "    \"ectocarpus_siliculosus_virus\": \"Phycodnaviridae\",\n",
    "    \"prasinovirus\": \"Phycodnaviridae\",  # Often grouped here\n",
    "    # Iridoviridae\n",
    "    \"ranavirus\": \"Iridoviridae\",  # Specific genus first\n",
    "    \"lymphocystis_disease_virus\": \"Iridoviridae\",  # Specific genus first\n",
    "    \"chloriridovirus\": \"Iridoviridae\",  # Specific genus first\n",
    "    \"iridovirus\": \"Iridoviridae\",  # General term last for this family\n",
    "    # Ascoviridae\n",
    "    \"ascovirus\": \"Ascoviridae\",\n",
    "    # Pithoviridae\n",
    "    \"pithovirus\": \"Pithoviridae\",\n",
    "    # Pandoraviridae\n",
    "    \"pandoravirus\": \"Pandoraviridae\",\n",
    "    # Yaraviridae\n",
    "    \"yaravirus\": \"Yaraviridae\",\n",
    "    # Specific Archaeal Virus Families\n",
    "    \"bicaudavirus\": \"Bicaudaviridae\",\n",
    "    \"acidianus_two-tailed_virus\": \"Bicaudaviridae\",\n",
    "    \"sulfolobus_virus_stsv\": \"Rudiviridae\",\n",
    "    \"acidianus_tailed_spindle_virus\": \"Fuselloviridae\",\n",
    "    \"sulfolobus_monocaudavirus\": \"Fuselloviridae\",  # Or proposed Thaspiviridae\n",
    "    # Grouping for other proposed NCLDV families/genera\n",
    "    \"faunusvirus\": \"Chaseviridae\",\n",
    "    \"solivirus\": \"Pithoviridae-like\",\n",
    "    \"solumvirus\": \"Pithoviridae-like\",\n",
    "    # Catch-all for 'virus' if no other keyword matched\n",
    "    \"virus\": \"Unclassified Virus\",  # General catch-all if name contains 'virus'\n",
    "}\n",
    "\n",
    "\n",
    "# --- Helper Function ---\n",
    "def assign_family(name):\n",
    "    \"\"\"Assigns a virus family based on keywords in the name.\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return np.nan  # Return NaN if input is NaN\n",
    "\n",
    "    name_lower = str(name).lower()  # Convert to string and lower case\n",
    "\n",
    "    for keyword, family in FAMILY_MAP.items():\n",
    "        # Use regex to find keyword as a whole word or part of compound name\n",
    "        # \\b matches word boundaries, allowing matches like 'ranavirus' but not 'miranavirus'\n",
    "        # We also allow matches if the keyword is followed by '_' or '-' or ends the string\n",
    "        # Or if it starts the string and is followed by '_' or '-'\n",
    "        # This handles cases like 'Megavirus_lba' or 'Frog_virus_3'\n",
    "        pattern = r\"(?:^|\\b|_|-)\" + re.escape(keyword) + r\"(?:$|\\b|_|-)\"\n",
    "        if re.search(pattern, name_lower):\n",
    "            return family\n",
    "\n",
    "    # If no keyword matches, return Unknown\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    print(f\"Successfully loaded '{input_csv_path}'. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"Error: Input file not found at '{input_csv_path}'. Make sure the previous step ran correctly.\"\n",
    "    )\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Apply Mapping ---\n",
    "print(f\"\\nAssigning families based on '{virus_name_col}'...\")\n",
    "# Apply the function to the Virus_Name column\n",
    "# Ensure the target column exists, create if not\n",
    "if virus_family_col not in df.columns:\n",
    "    df[virus_family_col] = np.nan\n",
    "\n",
    "# Apply the function only where Virus_Name is not null\n",
    "df[virus_family_col] = df[virus_name_col].apply(assign_family)\n",
    "print(\"Family assignment complete.\")\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\nValue counts for assigned Virus Families:\")\n",
    "print(df[virus_family_col].value_counts(dropna=False))  # Include NaNs in count\n",
    "\n",
    "print(\n",
    "    f\"\\nExample rows with assigned '{virus_family_col}' (showing first 15 where Virus_Name is not NaN):\"\n",
    ")\n",
    "print(\n",
    "    df[df[virus_name_col].notna()][[virus_name_col, virus_family_col]]\n",
    "    .head(15)\n",
    "    .to_markdown(index=False, numalign=\"left\", stralign=\"left\")\n",
    ")\n",
    "\n",
    "# --- Save Updated Data ---\n",
    "try:\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(\n",
    "        f\"\\nSuccessfully saved updated data with '{virus_family_col}' to '{output_csv_path}'.\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving updated data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64066a5d-4254-478d-9241-49d8cce0638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell added USPNet results to the dataset for some proteins where it had been ommitted previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2758cd62-75ef-4af3-8c0f-74b141bf7862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input file path (the reordered combined file from Cell 7)\n",
    "input_csv_path = \"proteome_database_combined_final_v1.3.csv\"\n",
    "# Input path for MAIN DB *after* USPNet integration (contains correct derived cols)\n",
    "# !! CHECK THIS FILENAME !! Example: v0.8 if USPNet was added to v0.7\n",
    "main_db_with_uspnet_path = \"proteome_database_v1.1.csv\"\n",
    "# Input path for FILTERED-OUT DB *after* USPNet integration (contains correct derived cols)\n",
    "# !! CHECK THIS FILENAME !! Example: v0.7 if USPNet was added to v0.6\n",
    "filtered_db_with_uspnet_path = \"all_filtered_out_proteins_v0.8.csv\"\n",
    "# Output file path\n",
    "output_csv_path = (\n",
    "    \"proteome_database_combined_final_v1.4_complete.csv\"  # Final analysis-ready version\n",
    ")\n",
    "\n",
    "# Columns to add/populate\n",
    "protein_id_col = \"ProteinID\"\n",
    "cols_to_add = [\n",
    "    \"Predicted_Subcellular_Localization\",\n",
    "    \"Mature_Protein_Sequence\",\n",
    "    \"Mature_Seq_Length\",\n",
    "]\n",
    "# Also include ProteinID for merging\n",
    "cols_to_extract = [protein_id_col] + cols_to_add\n",
    "\n",
    "# --- Load Target Combined Data ---\n",
    "print(\"--- Loading Reordered Combined Database ---\")\n",
    "try:\n",
    "    df_combined = pd.read_csv(input_csv_path, low_memory=False)\n",
    "    print(f\"Loaded '{input_csv_path}'. Shape: {df_combined.shape}\")\n",
    "    # Drop existing (likely empty or incomplete) versions of the columns if they exist\n",
    "    existing_cols_to_drop = [col for col in cols_to_add if col in df_combined.columns]\n",
    "    if existing_cols_to_drop:\n",
    "        print(f\"Dropping existing columns before merge: {existing_cols_to_drop}\")\n",
    "        df_combined = df_combined.drop(columns=existing_cols_to_drop)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file not found at '{input_csv_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the combined CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Load Source Dataframes (with correct derived columns) ---\n",
    "print(\"\\n--- Loading Source Databases with USPNet Data ---\")\n",
    "df_source_data_list = []\n",
    "loaded_source = False\n",
    "\n",
    "# Load Main DB with USPNet\n",
    "try:\n",
    "    df_main_source = pd.read_csv(\n",
    "        main_db_with_uspnet_path, usecols=cols_to_extract, low_memory=False\n",
    "    )\n",
    "    print(\n",
    "        f\"Loaded main source DB '{main_db_with_uspnet_path}'. Shape: {df_main_source.shape}\"\n",
    "    )\n",
    "    # Check if all needed columns were loaded\n",
    "    if all(col in df_main_source.columns for col in cols_to_extract):\n",
    "        df_source_data_list.append(df_main_source)\n",
    "        loaded_source = True\n",
    "    else:\n",
    "        print(\n",
    "            f\"Warning: Main source DB missing one or more required columns: {cols_to_extract}\"\n",
    "        )\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Main source DB file not found at '{main_db_with_uspnet_path}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error loading main source DB: {e}\")\n",
    "\n",
    "# Load Filtered-Out DB with USPNet\n",
    "try:\n",
    "    df_filtered_source = pd.read_csv(\n",
    "        filtered_db_with_uspnet_path, usecols=cols_to_extract, low_memory=False\n",
    "    )\n",
    "    print(\n",
    "        f\"Loaded filtered-out source DB '{filtered_db_with_uspnet_path}'. Shape: {df_filtered_source.shape}\"\n",
    "    )\n",
    "    # Check if all needed columns were loaded\n",
    "    if all(col in df_filtered_source.columns for col in cols_to_extract):\n",
    "        df_source_data_list.append(df_filtered_source)\n",
    "        loaded_source = True\n",
    "    else:\n",
    "        print(\n",
    "            f\"Warning: Filtered-out source DB missing one or more required columns: {cols_to_extract}\"\n",
    "        )\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"Warning: Filtered-out source DB file not found at '{filtered_db_with_uspnet_path}'.\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error loading filtered-out source DB: {e}\")\n",
    "\n",
    "\n",
    "# --- Combine and Merge Source Data ---\n",
    "if not loaded_source:\n",
    "    print(\n",
    "        \"\\nError: Could not load necessary source data with USPNet columns. Cannot proceed.\"\n",
    "    )\n",
    "    # Handle error - maybe raise exception\n",
    "    raise RuntimeError(\"Failed to load source data with USPNet columns.\")\n",
    "else:\n",
    "    print(\"\\nCombining source data for merge...\")\n",
    "    df_source_combined = pd.concat(df_source_data_list, ignore_index=True)\n",
    "    # Drop duplicates based on ProteinID, keeping the first instance found\n",
    "    df_source_combined = df_source_combined.drop_duplicates(\n",
    "        subset=[protein_id_col], keep=\"first\"\n",
    "    )\n",
    "    print(f\"Combined source data shape (unique ProteinIDs): {df_source_combined.shape}\")\n",
    "\n",
    "    print(\"\\nMerging missing columns into the main combined dataframe...\")\n",
    "    # Ensure ProteinID types match for merge\n",
    "    df_combined[protein_id_col] = df_combined[protein_id_col].astype(str)\n",
    "    df_source_combined[protein_id_col] = df_source_combined[protein_id_col].astype(str)\n",
    "\n",
    "    # Perform left merge\n",
    "    df_final = pd.merge(\n",
    "        df_combined,\n",
    "        df_source_combined,\n",
    "        on=protein_id_col,\n",
    "        how=\"left\",  # Keep all rows from df_combined\n",
    "    )\n",
    "    print(\"Merge complete.\")\n",
    "\n",
    "    # --- Final Checks and Type Conversions ---\n",
    "    print(\"\\nPerforming final checks and type conversions...\")\n",
    "    # Check if merge changed row count (shouldn't happen with left merge on unique IDs)\n",
    "    if len(df_final) != len(df_combined):\n",
    "        print(\n",
    "            f\"Warning: Merge changed row count from {len(df_combined)} to {len(df_final)}!\"\n",
    "        )\n",
    "\n",
    "    # Fill NaNs and set types for the newly merged columns\n",
    "    df_final[\"Predicted_Subcellular_Localization\"] = (\n",
    "        df_final[\"Predicted_Subcellular_Localization\"].fillna(\"Unknown\").astype(str)\n",
    "    )\n",
    "    df_final[\"Mature_Protein_Sequence\"] = (\n",
    "        df_final[\"Mature_Protein_Sequence\"].fillna(\"\").astype(str)\n",
    "    )  # Fill with empty string? Or keep NaN?\n",
    "    df_final[\"Mature_Seq_Length\"] = (\n",
    "        pd.to_numeric(df_final[\"Mature_Seq_Length\"], errors=\"coerce\")\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    print(\"Final checks complete.\")\n",
    "    print(f\"Columns in final dataframe: {df_final.columns.tolist()}\")\n",
    "    print(\"Non-null counts for added columns:\")\n",
    "    print(\n",
    "        f\"  Predicted_Subcellular_Localization: {df_final['Predicted_Subcellular_Localization'].notna().sum()}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Mature_Protein_Sequence: {df_final['Mature_Protein_Sequence'].notna().sum()}\"\n",
    "    )\n",
    "    print(f\"  Mature_Seq_Length: {df_final['Mature_Seq_Length'].notna().sum()}\")\n",
    "\n",
    "    # --- Save Final Data ---\n",
    "    try:\n",
    "        # Optional: Reorder columns one last time if needed\n",
    "        # desired_final_order = [...]\n",
    "        # df_final = df_final[desired_final_order]\n",
    "        df_final.to_csv(output_csv_path, index=False)\n",
    "        print(\n",
    "            f\"\\nSuccessfully saved final combined and corrected data to '{output_csv_path}'. Shape: {df_final.shape}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving final data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca16fc5-94ca-417b-808c-cb4bd0ffa942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell adds results of the search of dataset proteins against the mgnify clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc945911-c082-4366-bb06-553debb9b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input file paths (adjust if necessary)\n",
    "main_db_path = \"proteome_database_combined_v1.4.csv\"\n",
    "mgnify_results_path = \"results_vs_mgnify.m8\"  # Your MMseqs2 output file\n",
    "\n",
    "# Output file path\n",
    "output_db_path = \"proteome_database_combined_v1.5_mgnify.csv\"\n",
    "\n",
    "# Column names\n",
    "protein_id_col = \"ProteinID\"  # Column in your main database\n",
    "mgnify_query_col = \"query\"  # Column in the MMseqs2 .m8 file (first column)\n",
    "new_hit_col = \"SeqSearch_MGnify_Hit\"  # Name of the new column to add\n",
    "\n",
    "# --- Load Main Database ---\n",
    "print(\"--- Loading Main Database ---\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    # Use low_memory=False for large files, consistent with your analysis notebook\n",
    "    df_main = pd.read_csv(main_db_path, low_memory=False)\n",
    "    print(f\"Loaded '{main_db_path}'. Shape: {df_main.shape}\")\n",
    "    # Ensure ProteinID column is string type for reliable matching\n",
    "    if protein_id_col in df_main.columns:\n",
    "        df_main[protein_id_col] = df_main[protein_id_col].astype(str)\n",
    "    else:\n",
    "        raise KeyError(\n",
    "            f\"Required column '{protein_id_col}' not found in main database.\"\n",
    "        )\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Main database file not found at '{main_db_path}'.\")\n",
    "    raise\n",
    "except KeyError as e:\n",
    "    print(f\"Error loading main database: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the main database CSV: {e}\")\n",
    "    raise\n",
    "print(f\"Main database loaded in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- Load MGnify MMseqs2 Results ---\n",
    "print(\"\\n--- Loading MGnify Search Results ---\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    # Define column names for the .m8 file based on the format used in the search plan\n",
    "    # We only really need the 'query' column\n",
    "    m8_cols = [\"query\", \"target\", \"pident\", \"qcov\", \"tcov\", \"evalue\"]\n",
    "    # Load only the query column to save memory, use tab separator\n",
    "    df_mgnify_hits = pd.read_csv(\n",
    "        mgnify_results_path,\n",
    "        sep=\"\\t\",\n",
    "        header=None,  # .m8 files typically don't have headers\n",
    "        names=m8_cols,\n",
    "        usecols=[mgnify_query_col],  # Only load the first column ('query')\n",
    "    )\n",
    "    print(f\"Loaded '{mgnify_results_path}'. Found {len(df_mgnify_hits)} total hits.\")\n",
    "    # Ensure query column is string type\n",
    "    df_mgnify_hits[mgnify_query_col] = df_mgnify_hits[mgnify_query_col].astype(str)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: MGnify results file not found at '{mgnify_results_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the MGnify results CSV: {e}\")\n",
    "    raise\n",
    "print(f\"MGnify results loaded in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- Identify Proteins with Hits ---\n",
    "print(\"\\n--- Identifying Proteins with MGnify Hits ---\")\n",
    "# Get the set of unique ProteinIDs that had at least one hit in the MGnify search\n",
    "proteins_with_mgnify_hits = set(df_mgnify_hits[mgnify_query_col].unique())\n",
    "print(f\"Found {len(proteins_with_mgnify_hits)} unique proteins with hits in MGnify.\")\n",
    "\n",
    "# --- Add New Column to Main Database ---\n",
    "print(f\"\\n--- Adding '{new_hit_col}' column ---\")\n",
    "# Initialize the new column to False\n",
    "df_main[new_hit_col] = False\n",
    "\n",
    "# Find the rows in the main dataframe where the ProteinID is in our set of hits\n",
    "hit_mask = df_main[protein_id_col].isin(proteins_with_mgnify_hits)\n",
    "\n",
    "# Set the new column to True for those rows\n",
    "df_main.loc[hit_mask, new_hit_col] = True\n",
    "\n",
    "# --- Verify ---\n",
    "print(f\"\\nValue counts for the new '{new_hit_col}' column:\")\n",
    "print(df_main[new_hit_col].value_counts())\n",
    "num_true = df_main[new_hit_col].sum()\n",
    "if num_true == len(proteins_with_mgnify_hits):\n",
    "    print(\n",
    "        \"Verification successful: Number of True values matches number of unique hitting proteins.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"Warning: Mismatch detected! Number of True values ({num_true}) does not match unique hitting proteins ({len(proteins_with_mgnify_hits)}). Check ProteinID matching.\"\n",
    "    )\n",
    "\n",
    "# --- Save Updated Database ---\n",
    "print(\"\\n--- Saving Updated Database ---\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    df_main.to_csv(output_db_path, index=False)\n",
    "    print(\n",
    "        f\"Successfully saved updated database with MGnify hits to '{output_db_path}'.\"\n",
    "    )\n",
    "    print(f\"Final shape: {df_main.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving updated database: {e}\")\n",
    "    raise\n",
    "print(f\"Database saved in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "print(\"\\n--- Integration Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86484266-7993-4083-893d-be146375fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell adds results from the search against ESM Atlas entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c588d498-2680-4385-9dfe-f17722caaa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input file paths (adjust if necessary)\n",
    "# Use the output from the MGnify integration step or v1.4 if starting fresh\n",
    "main_db_path = \"proteome_database_combined_v1.5_mgnify.csv\"  # Or v1.4\n",
    "mgnify_results_path = \"results_vs_mgnify.m8\"  # Your MMseqs2 output file\n",
    "\n",
    "# *** USER ACTION NEEDED: Path to your downloaded AFESM metadata file ***\n",
    "afesm_metadata_path = \"2-repID_isOnlyESM_nMem_nAllMem_repPlddt_avgPlddt_avgAllPlddt_repLen_avgLen_avgAllLen_LCAtaxID_nBiome_LCBID.tsv\"  # <-- UPDATE THIS PATH\n",
    "\n",
    "# Output file path\n",
    "output_db_path = \"proteome_database_combined_v1.6_esm_dark.csv\"\n",
    "\n",
    "# Column names\n",
    "protein_id_col = \"ProteinID\"  # Column in your main database\n",
    "mgnify_query_col = \"query\"  # Column in the MMseqs2 .m8 file (first column)\n",
    "mgnify_target_col = \"target\"  # Column in the MMseqs2 .m8 file (second column)\n",
    "afesm_repid_col = \"repID\"  # Column name in the AFESM metadata file\n",
    "\n",
    "# Hit columns (ensure these exist in your input main_db_path)\n",
    "pdb_hit_col = \"SeqSearch_PDB_Hit\"\n",
    "afdb_hit_col = \"SeqSearch_AFDB_Hit\"\n",
    "mgnify_hit_col = \"SeqSearch_MGnify_Hit\"  # Added previously\n",
    "esma_hit_col = \"SeqSearch_ESMA_Hit\"  # New column for ESMA hits via MGnify/AFESM\n",
    "dark_col = \"Is_Structurally_Dark\"  # New triple-negative flag\n",
    "\n",
    "# --- Load Main Database ---\n",
    "print(\"--- Loading Main Database ---\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    df_main = pd.read_csv(main_db_path, low_memory=False)\n",
    "    print(f\"Loaded '{main_db_path}'. Shape: {df_main.shape}\")\n",
    "    # Ensure ProteinID column is string type for reliable matching\n",
    "    if protein_id_col in df_main.columns:\n",
    "        df_main[protein_id_col] = df_main[protein_id_col].astype(str)\n",
    "    else:\n",
    "        raise KeyError(\n",
    "            f\"Required column '{protein_id_col}' not found in main database.\"\n",
    "        )\n",
    "    # Ensure previous hit columns exist and are boolean\n",
    "    for col in [pdb_hit_col, afdb_hit_col, mgnify_hit_col]:\n",
    "        if col not in df_main.columns:\n",
    "            print(f\"Warning: Column '{col}' not found. Adding as False.\")\n",
    "            df_main[col] = False\n",
    "        else:\n",
    "            # Fill potential NAs from previous steps and ensure boolean\n",
    "            df_main[col] = df_main[col].fillna(False).astype(bool)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Main database file not found at '{main_db_path}'.\")\n",
    "    raise\n",
    "except KeyError as e:\n",
    "    print(f\"Error loading main database: {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the main database CSV: {e}\")\n",
    "    raise\n",
    "print(f\"Main database loaded in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- Load AFESM Metadata (Representative IDs with Structures) ---\n",
    "print(\"\\n--- Loading AFESM Metadata ---\")\n",
    "start_time = time.time()\n",
    "afesm_repids_with_structure = set()\n",
    "try:\n",
    "    # Read the gzipped TSV file, using only the first column ('repID')\n",
    "    # Assuming the first column is indeed 'repID' based on the file name and description\n",
    "    df_afesm_meta = pd.read_csv(\n",
    "        afesm_metadata_path,\n",
    "        sep=\"\\t\",\n",
    "        usecols=[0],  # Load only the first column\n",
    "        header=0,  # Use the first row as header to get the column name\n",
    "    )\n",
    "    # Check the actual column name loaded (should be 'repID')\n",
    "    loaded_col_name = df_afesm_meta.columns[0]\n",
    "    if loaded_col_name != afesm_repid_col:\n",
    "        print(\n",
    "            f\"Warning: Expected column name '{afesm_repid_col}' but found '{loaded_col_name}'. Using '{loaded_col_name}'.\"\n",
    "        )\n",
    "        afesm_repid_col = loaded_col_name  # Use the actual loaded name\n",
    "\n",
    "    # Extract unique IDs into the set, ensuring they are strings\n",
    "    afesm_repids_with_structure = set(\n",
    "        df_afesm_meta[afesm_repid_col].dropna().astype(str)\n",
    "    )\n",
    "\n",
    "    if not afesm_repids_with_structure:\n",
    "        print(\n",
    "            f\"Warning: No IDs loaded from '{afesm_metadata_path}'. Check file content and path.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Loaded {len(afesm_repids_with_structure)} AFESM representative IDs with structures from '{afesm_metadata_path}'.\"\n",
    "        )\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"Error: AFESM metadata file not found at '{afesm_metadata_path}'. Cannot determine ESMA hits.\"\n",
    "    )\n",
    "    print(\n",
    "        \"Proceeding without ESMA hit information. The 'SeqSearch_ESMA_Hit' and 'Is_Structurally_Dark' columns will reflect this.\"\n",
    "    )\n",
    "    afesm_repids_with_structure = set()  # Ensure empty set\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the AFESM metadata: {e}\")\n",
    "    afesm_repids_with_structure = set()  # Ensure empty set\n",
    "print(f\"AFESM metadata loaded in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "# --- Load MGnify MMseqs2 Results (Query and Target) ---\n",
    "# Only proceed if we have AFESM IDs to check against\n",
    "proteins_hitting_esma_target = set()\n",
    "if afesm_repids_with_structure:\n",
    "    print(\"\\n--- Loading MGnify Search Results (Query & Target) ---\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # Define column names for the .m8 file\n",
    "        m8_cols = [\"query\", \"target\", \"pident\", \"qcov\", \"tcov\", \"evalue\"]\n",
    "        # Load only the query and target columns\n",
    "        df_mgnify_hits = pd.read_csv(\n",
    "            mgnify_results_path,\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            names=m8_cols,\n",
    "            usecols=[mgnify_query_col, mgnify_target_col],  # Load query and target\n",
    "        )\n",
    "        print(\n",
    "            f\"Loaded '{mgnify_results_path}'. Found {len(df_mgnify_hits)} total hits.\"\n",
    "        )\n",
    "        # Ensure columns are string type\n",
    "        df_mgnify_hits[mgnify_query_col] = df_mgnify_hits[mgnify_query_col].astype(str)\n",
    "        df_mgnify_hits[mgnify_target_col] = df_mgnify_hits[mgnify_target_col].astype(\n",
    "            str\n",
    "        )\n",
    "\n",
    "        # --- Identify Your Proteins Hitting MGnify Targets that ARE AFESM Representatives ---\n",
    "        print(\"\\n--- Filtering MGnify hits for targets present in AFESM metadata ---\")\n",
    "        # Filter the hits dataframe where the target is in our set of AFESM repIDs\n",
    "        df_esma_relevant_hits = df_mgnify_hits[\n",
    "            df_mgnify_hits[mgnify_target_col].isin(afesm_repids_with_structure)\n",
    "        ]\n",
    "        print(\n",
    "            f\"Found {len(df_esma_relevant_hits)} hits where the MGnify target is an AFESM representative with a structure.\"\n",
    "        )\n",
    "\n",
    "        # Get the set of unique ProteinIDs (query) from these relevant hits\n",
    "        proteins_hitting_esma_target = set(\n",
    "            df_esma_relevant_hits[mgnify_query_col].unique()\n",
    "        )\n",
    "        print(\n",
    "            f\"Found {len(proteins_hitting_esma_target)} unique query proteins hitting an AFESM representative target.\"\n",
    "        )\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\n",
    "            f\"Error: MGnify results file not found at '{mgnify_results_path}'. Cannot determine ESMA hits.\"\n",
    "        )\n",
    "        proteins_hitting_esma_target = set()  # Ensure empty set\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"An error occurred while loading or processing the MGnify results CSV: {e}\"\n",
    "        )\n",
    "        proteins_hitting_esma_target = set()  # Ensure empty set\n",
    "    print(\n",
    "        f\"MGnify results loaded and processed in {time.time() - start_time:.2f} seconds.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"\\nSkipping MGnify results loading/processing as no AFESM metadata was loaded.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Add New ESMA Hit Column to Main Database ---\n",
    "print(f\"\\n--- Adding '{esma_hit_col}' column ---\")\n",
    "# Initialize the new column to False\n",
    "df_main[esma_hit_col] = False\n",
    "# Find rows where the ProteinID is in our set of proteins hitting an AFESM target\n",
    "if proteins_hitting_esma_target:  # Only update if we have hits\n",
    "    esma_hit_mask = df_main[protein_id_col].isin(proteins_hitting_esma_target)\n",
    "    # Set the new column to True for those rows\n",
    "    df_main.loc[esma_hit_mask, esma_hit_col] = True\n",
    "\n",
    "print(f\"\\nValue counts for the new '{esma_hit_col}' column:\")\n",
    "print(df_main[esma_hit_col].value_counts())\n",
    "num_esma_true = df_main[esma_hit_col].sum()\n",
    "if not afesm_repids_with_structure:\n",
    "    print(\"(Note: ESMA hits could not be determined due to missing AFESM metadata.)\")\n",
    "elif num_esma_true == len(proteins_hitting_esma_target):\n",
    "    print(\n",
    "        \"Verification successful: Number of True values matches number of unique hitting proteins.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"Warning: Mismatch detected! Number of True values ({num_esma_true}) does not match unique hitting proteins ({len(proteins_hitting_esma_target)}). Check ProteinID matching.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Add 'Is_Structurally_Dark' Column ---\n",
    "print(f\"\\n--- Adding '{dark_col}' column ---\")\n",
    "# True if PDB hit is False AND AFDB hit is False AND ESMA hit is False\n",
    "df_main[dark_col] = (\n",
    "    (~df_main[pdb_hit_col]) & (~df_main[afdb_hit_col]) & (~df_main[esma_hit_col])\n",
    ")\n",
    "\n",
    "print(f\"\\nValue counts for the new '{dark_col}' column:\")\n",
    "print(df_main[dark_col].value_counts())\n",
    "print(\n",
    "    f\"Identified {df_main[dark_col].sum()} 'structurally dark' proteins (Triple Negative).\"\n",
    ")\n",
    "\n",
    "\n",
    "# --- Save Updated Database ---\n",
    "print(\"\\n--- Saving Updated Database ---\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    # Reorder columns slightly? Optional. Put new columns near other hit columns.\n",
    "    cols = df_main.columns.tolist()\n",
    "    # Find insertion point (e.g., after mgnify_hit_col)\n",
    "    try:\n",
    "        insert_idx = cols.index(mgnify_hit_col) + 1\n",
    "        # Ensure new columns actually exist before trying to reorder\n",
    "        if esma_hit_col in cols:\n",
    "            cols.insert(insert_idx, cols.pop(cols.index(esma_hit_col)))\n",
    "            insert_idx += 1  # Increment index for the next column\n",
    "        if dark_col in cols:\n",
    "            cols.insert(insert_idx, cols.pop(cols.index(dark_col)))\n",
    "        df_main = df_main[cols]\n",
    "    except ValueError:\n",
    "        print(\"Could not reorder columns, saving with new columns at the end.\")\n",
    "\n",
    "    df_main.to_csv(output_db_path, index=False)\n",
    "    print(f\"Successfully saved updated database to '{output_db_path}'.\")\n",
    "    print(f\"Final shape: {df_main.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving updated database: {e}\")\n",
    "    raise\n",
    "print(f\"Database saved in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "print(\"\\n--- Integration Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3572c-0201-4e3f-ba9d-f6897d765cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports statistics from the DIAMOND search against eukaryotic proteomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbcbcfb-2226-4e4c-beee-021819939046",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob  # For finding files in a folder\n",
    "import time\n",
    "import re  # Added for parsing organism names\n",
    "\n",
    "# --- Configuration ---\n",
    "# Main database file\n",
    "db_path = \"proteome_database_combined_v1.7_broad_cat.csv\"\n",
    "# Folder containing DIAMOND search results files (tab-separated)\n",
    "diamond_results_folder = \"euk_diamond_search_results\"  # User provided this folder name\n",
    "# ESP Orthogroup list file\n",
    "esp_og_list_path = \"output_summary_data/all_esp_orthogroup_list_v4.txt\"\n",
    "\n",
    "# Output file for the database with DIAMOND hits and ESP column integrated\n",
    "output_db_updated_path = (\n",
    "    \"proteome_database_combined_v1.8_euk_hits_esps.csv\"  # Example new version\n",
    ")\n",
    "\n",
    "# DIAMOND parsing parameters\n",
    "e_value_threshold = 1e-10  # Adjust as needed, 1e-5 is also common\n",
    "# DIAMOND output columns (standard 12 fields for `outfmt 6`):\n",
    "# qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore\n",
    "# User example also had qlen and slen at the end, so we'll use 14 columns\n",
    "diamond_col_names = [\n",
    "    \"qseqid\",\n",
    "    \"sseqid_full\",\n",
    "    \"pident\",\n",
    "    \"length\",\n",
    "    \"mismatch\",\n",
    "    \"gapopen\",\n",
    "    \"qstart\",\n",
    "    \"qend\",\n",
    "    \"sstart\",\n",
    "    \"send\",\n",
    "    \"evalue\",\n",
    "    \"bitscore\",\n",
    "    \"qlen\",\n",
    "    \"slen\",\n",
    "]\n",
    "\n",
    "# Columns to be added to the main DataFrame\n",
    "hit_flag_col = \"Has_Euk_DIAMOND_Hit\"\n",
    "best_hit_sseqid_col = \"Euk_Hit_SSEQID\"\n",
    "best_hit_organism_col = \"Euk_Hit_Organism\"\n",
    "best_hit_pident_col = \"Euk_Hit_PIDENT\"\n",
    "best_hit_evalue_col = \"Euk_Hit_EVALUE\"\n",
    "esp_col = \"Is_ESP\"  # Name of the new ESP column we will create\n",
    "\n",
    "# Define relevant column names from your main database\n",
    "protein_id_col = \"ProteinID\"\n",
    "group_col = \"Group\"  # 'Asgard' or 'Virus'\n",
    "orthogroup_col = \"Orthogroup\"  # Needed for ESP identification\n",
    "structurally_dark_col = \"Is_Structurally_Dark\"\n",
    "num_domains_col = \"Num_Domains\"\n",
    "\n",
    "\n",
    "print(\"--- Starting DIAMOND Hit Integration, ESP Definition, and Analysis ---\")\n",
    "\n",
    "# --- 1. Load Main Database ---\n",
    "print(f\"\\nLoading main database from: {db_path}\")\n",
    "try:\n",
    "    df_main = pd.read_csv(db_path, low_memory=False)\n",
    "    print(f\"Successfully loaded main database. Shape: {df_main.shape}\")\n",
    "    # Diagnostic: Print some ProteinIDs from Asgard group in df_main\n",
    "    if protein_id_col in df_main.columns and group_col in df_main.columns:\n",
    "        print(\"  Example Asgard ProteinIDs from main DB (first 5):\")\n",
    "        print(df_main[df_main[group_col] == \"Asgard\"][protein_id_col].head().tolist())\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Main database file not found at '{db_path}'. Please check the path.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the main database: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 2. Define ESPs based on Orthogroup List ---\n",
    "print(f\"\\nLoading ESP Orthogroup list from: {esp_og_list_path}\")\n",
    "esp_og_set = set()\n",
    "try:\n",
    "    esp_og_df = pd.read_csv(esp_og_list_path, header=None, names=[orthogroup_col])\n",
    "    esp_og_set = set(esp_og_df[orthogroup_col])\n",
    "    print(f\"Successfully loaded {len(esp_og_set)} ESP Orthogroups.\")\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"ERROR: ESP Orthogroup list file not found at '{esp_og_list_path}'. '{esp_col}' will be False for all.\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"An error occurred while loading the ESP Orthogroup list: {e}. '{esp_col}' will be False for all.\"\n",
    "    )\n",
    "\n",
    "# Add 'Is_ESP' column to the main DataFrame\n",
    "if orthogroup_col in df_main.columns and group_col in df_main.columns and esp_og_set:\n",
    "    df_main[esp_col] = df_main.apply(\n",
    "        lambda row: row[orthogroup_col] in esp_og_set\n",
    "        if row[group_col] == \"Asgard\"\n",
    "        else False,\n",
    "        axis=1,\n",
    "    )\n",
    "    print(f\"Added '{esp_col}' column. Found {df_main[esp_col].sum()} ESPs in Asgard.\")\n",
    "else:\n",
    "    df_main[esp_col] = False  # Default to False if OGs not loaded or columns missing\n",
    "    if not esp_og_set:\n",
    "        print(\n",
    "            f\"ESP Orthogroup list was not loaded or is empty. '{esp_col}' column set to False for all proteins.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Could not define ESPs due to missing '{orthogroup_col}' or '{group_col}' columns. '{esp_col}' column set to False.\"\n",
    "        )\n",
    "\n",
    "\n",
    "# --- 3. Parse DIAMOND Results from Folder ---\n",
    "print(f\"\\nParsing DIAMOND results from folder: {diamond_results_folder}\")\n",
    "best_hits_data = {}  # To store best hit info for each query protein\n",
    "raw_diamond_files = glob.glob(\n",
    "    os.path.join(diamond_results_folder, \"*_diamond_hits.tsv\")\n",
    ")\n",
    "\n",
    "if not raw_diamond_files:\n",
    "    print(\n",
    "        f\"WARNING: No '*_diamond_hits.tsv' files found in '{diamond_results_folder}'.\"\n",
    "    )\n",
    "    df_diamond_raw_list = []\n",
    "else:\n",
    "    print(f\"Found {len(raw_diamond_files)} DIAMOND result files to process.\")\n",
    "    df_diamond_raw_list = []\n",
    "    for f_path in raw_diamond_files:\n",
    "        try:\n",
    "            df_temp = pd.read_csv(\n",
    "                f_path, sep=\"\\t\", header=None, names=diamond_col_names\n",
    "            )\n",
    "            # CRITICAL CHANGE: Do NOT add .ASG suffix here.\n",
    "            # The qseqid from DIAMOND output should match ProteinID in df_main directly.\n",
    "            # df_temp['qseqid'] = df_temp['qseqid'].astype(str) + \".ASG\" # REMOVED THIS LINE\n",
    "            df_temp[\"qseqid\"] = df_temp[\"qseqid\"].astype(str)  # Ensure it's string\n",
    "            df_diamond_raw_list.append(df_temp)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading or processing file {f_path}: {e}\")\n",
    "            continue  # Skip problematic files\n",
    "\n",
    "if df_diamond_raw_list:\n",
    "    df_diamond_raw = pd.concat(df_diamond_raw_list, ignore_index=True)\n",
    "    print(f\"  Read {len(df_diamond_raw)} total DIAMOND alignments from all files.\")\n",
    "\n",
    "    # Filter by e-value\n",
    "    df_diamond_filtered = df_diamond_raw[\n",
    "        df_diamond_raw[\"evalue\"] <= e_value_threshold\n",
    "    ].copy()\n",
    "    print(\n",
    "        f\"  Found {len(df_diamond_filtered)} alignments passing e-value threshold <= {e_value_threshold}.\"\n",
    "    )\n",
    "\n",
    "    if not df_diamond_filtered.empty:\n",
    "        # Sort by qseqid and then by e-value (ascending), then bitscore (descending) to easily get the best hit\n",
    "        df_diamond_filtered.sort_values(\n",
    "            by=[\"qseqid\", \"evalue\", \"bitscore\"],\n",
    "            ascending=[True, True, False],\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # Keep only the best hit per qseqid\n",
    "        df_best_hits = df_diamond_filtered.drop_duplicates(\n",
    "            subset=[\"qseqid\"], keep=\"first\"\n",
    "        ).copy()\n",
    "        print(\n",
    "            f\"  Identified {len(df_best_hits)} unique query proteins with significant eukaryotic hits.\"\n",
    "        )\n",
    "\n",
    "        # Diagnostic: Print some qseqids from DIAMOND best hits\n",
    "        print(\"  Example qseqids from DIAMOND best hits (first 5):\")\n",
    "        print(df_best_hits[\"qseqid\"].head().tolist())\n",
    "\n",
    "        # Prepare data for merging\n",
    "        for _, row in df_best_hits.iterrows():\n",
    "            query_id_for_db = row[\n",
    "                \"qseqid\"\n",
    "            ]  # This is now the original ID from DIAMOND file\n",
    "            subject_full = str(row[\"sseqid_full\"])\n",
    "            sseqid = subject_full\n",
    "            organism = \"Unknown\"  # Default\n",
    "\n",
    "            if \"|\" in subject_full:  # Standard parsing for NCBI-like headers\n",
    "                parts = subject_full.split(\"|\", 1)\n",
    "                sseqid = parts[0]\n",
    "                if (\n",
    "                    len(parts) > 1 and parts[1]\n",
    "                ):  # Check if there's anything after the first pipe\n",
    "                    desc_text = parts[1]\n",
    "                    # Try to extract organism from [Organism Name]\n",
    "                    match = re.search(r\"\\[(.*?)\\]\", desc_text)\n",
    "                    if match:\n",
    "                        organism = match.group(1)\n",
    "                    else:\n",
    "                        # If no brackets, try to get it from common patterns like XP_id|Genus species\n",
    "                        # This part might need refinement based on the exact format of sseqid_full\n",
    "                        sub_parts = desc_text.split(\" \")\n",
    "                        if len(sub_parts) > 0:\n",
    "                            # Check if the first part after pipe is just an accession (e.g. XP_12345)\n",
    "                            if not (\n",
    "                                sub_parts[0].startswith(\"XP_\")\n",
    "                                or sub_parts[0].startswith(\"NP_\")\n",
    "                                or sub_parts[0].startswith(\"WP_\")\n",
    "                            ):\n",
    "                                organism = sub_parts[0]  # Often the first word is genus\n",
    "                                if len(sub_parts) > 1 and not (\n",
    "                                    sub_parts[1].startswith(\"XP_\")\n",
    "                                    or sub_parts[1].startswith(\"NP_\")\n",
    "                                    or sub_parts[1].startswith(\"WP_\")\n",
    "                                ):\n",
    "                                    organism = f\"{sub_parts[0]} {sub_parts[1]}\"  # Genus species\n",
    "                            elif (\n",
    "                                len(sub_parts) > 1\n",
    "                            ):  # If first part is an ID, try the next\n",
    "                                organism = sub_parts[1]\n",
    "\n",
    "            best_hits_data[query_id_for_db] = {\n",
    "                hit_flag_col: True,\n",
    "                best_hit_sseqid_col: sseqid,\n",
    "                best_hit_organism_col: organism,\n",
    "                best_hit_pident_col: row[\"pident\"],\n",
    "                best_hit_evalue_col: row[\"evalue\"],\n",
    "            }\n",
    "    else:\n",
    "        print(\n",
    "            \"  No DIAMOND hits passed the e-value threshold after processing all files.\"\n",
    "        )\n",
    "else:\n",
    "    print(\"  No valid DIAMOND alignments loaded. Skipping hit processing.\")\n",
    "\n",
    "\n",
    "# --- 4. Merge DIAMOND Hit Information with Main Database ---\n",
    "print(\"\\nMerging DIAMOND hit information into the main database...\")\n",
    "df_main_updated = df_main.copy()  # Start with the df_main that now includes Is_ESP\n",
    "\n",
    "if best_hits_data:\n",
    "    df_hits_to_merge = pd.DataFrame.from_dict(best_hits_data, orient=\"index\")\n",
    "    df_hits_to_merge.index.name = protein_id_col\n",
    "\n",
    "    # Diagnostic: Check dtypes before merge\n",
    "    print(\n",
    "        f\"  dtype of df_main_updated['{protein_id_col}']: {df_main_updated[protein_id_col].dtype}\"\n",
    "    )\n",
    "    print(f\"  dtype of df_hits_to_merge.index: {df_hits_to_merge.index.dtype}\")\n",
    "\n",
    "    # Ensure ProteinID is string in both for robust merging\n",
    "    df_main_updated[protein_id_col] = df_main_updated[protein_id_col].astype(str)\n",
    "    df_hits_to_merge.index = df_hits_to_merge.index.astype(str)\n",
    "\n",
    "    df_main_updated = df_main_updated.merge(\n",
    "        df_hits_to_merge, on=protein_id_col, how=\"left\"\n",
    "    )\n",
    "\n",
    "    df_main_updated[hit_flag_col] = df_main_updated[hit_flag_col].fillna(False)\n",
    "else:\n",
    "    print(\"  No best hits data to merge. Adding empty hit flag column.\")\n",
    "    df_main_updated[hit_flag_col] = False\n",
    "    for col in [\n",
    "        best_hit_sseqid_col,\n",
    "        best_hit_organism_col,\n",
    "        best_hit_pident_col,\n",
    "        best_hit_evalue_col,\n",
    "    ]:\n",
    "        if col not in df_main_updated.columns:\n",
    "            df_main_updated[col] = np.nan\n",
    "\n",
    "\n",
    "print(f\"  Merge complete. Updated database shape: {df_main_updated.shape}\")\n",
    "print(\n",
    "    f\"  Total proteins flagged with eukaryotic hits: {df_main_updated[hit_flag_col].sum()}\"\n",
    ")\n",
    "\n",
    "# --- 5. Save Updated Database ---\n",
    "print(\n",
    "    f\"\\nSaving updated database (with ESPs and DIAMOND hits) to: {output_db_updated_path}\"\n",
    ")\n",
    "try:\n",
    "    df_main_updated.to_csv(output_db_updated_path, index=False)\n",
    "    print(\"Successfully saved updated database.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not save the updated database. Error: {e}\")\n",
    "\n",
    "# --- 6. Initial Summary Analyses ---\n",
    "print(\"\\n\\n--- Initial Summary of Eukaryotic DIAMOND Hits ---\")\n",
    "\n",
    "if group_col not in df_main_updated.columns:\n",
    "    print(\n",
    "        f\"ERROR: Group column '{group_col}' not found. Cannot perform group-specific summary.\"\n",
    "    )\n",
    "else:\n",
    "    # Overall counts\n",
    "    total_proteins = len(df_main_updated)\n",
    "    total_hits = df_main_updated[hit_flag_col].sum()\n",
    "    print(\"\\nOverall:\")\n",
    "    print(f\"  Total proteins in database: {total_proteins}\")\n",
    "    print(\n",
    "        f\"  Total proteins with eukaryotic DIAMOND hits: {total_hits} ({(total_hits / total_proteins * 100 if total_proteins > 0 else 0.0):.1f}%)\"\n",
    "    )\n",
    "\n",
    "    # Asgard proteins\n",
    "    df_asgard = df_main_updated[df_main_updated[group_col] == \"Asgard\"]\n",
    "    if not df_asgard.empty:\n",
    "        total_asgard = len(df_asgard)\n",
    "        asgard_hits = df_asgard[hit_flag_col].sum()\n",
    "        print(\"\\nAsgard Archaea:\")\n",
    "        print(f\"  Total Asgard proteins: {total_asgard}\")\n",
    "        print(\n",
    "            f\"  Asgard proteins with eukaryotic hits: {asgard_hits} ({(asgard_hits / total_asgard * 100 if total_asgard > 0 else 0.0):.1f}%)\"\n",
    "        )\n",
    "\n",
    "        # Asgard ESPs\n",
    "        if esp_col in df_asgard.columns:\n",
    "            df_asgard_esps = df_asgard[df_asgard[esp_col] == True]\n",
    "            if not df_asgard_esps.empty:\n",
    "                total_asgard_esps = len(df_asgard_esps)\n",
    "                asgard_esps_hits = df_asgard_esps[hit_flag_col].sum()\n",
    "                print(\n",
    "                    f\"  Asgard ESPs with eukaryotic hits: {asgard_esps_hits} ({(asgard_esps_hits / total_asgard_esps * 100 if total_asgard_esps > 0 else 0.0):.1f}%)\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"  No Asgard ESPs identified in the Asgard subset (column '{esp_col}' has no True values).\"\n",
    "                )\n",
    "        else:\n",
    "            print(\n",
    "                f\"  ESP column ('{esp_col}') was not successfully added. Skipping ESP hit analysis.\"\n",
    "            )\n",
    "\n",
    "        # Structurally Dark Asgard proteins\n",
    "        if structurally_dark_col in df_asgard.columns:\n",
    "            df_asgard_dark = df_asgard[df_asgard[structurally_dark_col] == True]\n",
    "            if not df_asgard_dark.empty:\n",
    "                total_asgard_dark = len(df_asgard_dark)\n",
    "                asgard_dark_hits = df_asgard_dark[hit_flag_col].sum()\n",
    "                print(\n",
    "                    f\"  Structurally Dark Asgard proteins with eukaryotic hits: {asgard_dark_hits} ({(asgard_dark_hits / total_asgard_dark * 100 if total_asgard_dark > 0 else 0.0):.1f}%)\"\n",
    "                )\n",
    "            else:\n",
    "                print(\"  No Structurally Dark Asgard proteins identified.\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"  Structurally Dark column ('{structurally_dark_col}') not found. Skipping dark protein hit analysis.\"\n",
    "            )\n",
    "\n",
    "        # Domain-less Asgard proteins\n",
    "        if num_domains_col in df_asgard.columns:\n",
    "            df_asgard_domainless = df_asgard[df_asgard[num_domains_col].isna()]\n",
    "            if not df_asgard_domainless.empty:\n",
    "                total_asgard_domainless = len(df_asgard_domainless)\n",
    "                asgard_domainless_hits = df_asgard_domainless[hit_flag_col].sum()\n",
    "                print(\n",
    "                    f\"  Domain-less Asgard proteins with eukaryotic hits: {asgard_domainless_hits} ({(asgard_domainless_hits / total_asgard_domainless * 100 if total_asgard_domainless > 0 else 0.0):.1f}%)\"\n",
    "                )\n",
    "            else:\n",
    "                print(\"  No Domain-less Asgard proteins identified.\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"  Num_Domains column ('{num_domains_col}') not found. Skipping domain-less protein hit analysis.\"\n",
    "            )\n",
    "    else:\n",
    "        print(\"\\nNo Asgard proteins found in the database.\")\n",
    "\n",
    "    # Giant Virus (GV) proteins\n",
    "    df_gv_subset = df_main_updated[df_main_updated[group_col] == \"Virus\"]\n",
    "    if not df_gv_subset.empty:\n",
    "        total_gv = len(df_gv_subset)\n",
    "        gv_hits = df_gv_subset[hit_flag_col].sum()\n",
    "        print(\"\\nGiant Viruses (GV):\")\n",
    "        print(f\"  Total GV proteins: {total_gv}\")\n",
    "        print(\n",
    "            f\"  GV proteins with eukaryotic hits: {gv_hits} ({(gv_hits / total_gv * 100 if total_gv > 0 else 0.0):.1f}%)\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nNo Giant Virus proteins found in the database.\")\n",
    "\n",
    "print(\n",
    "    \"\\n\\n--- DIAMOND Hit Integration, ESP Definition, and Initial Analysis Complete ---\"\n",
    ")\n",
    "print(\"Next steps could include:\")\n",
    "print(f\"  - Detailed analysis of the '{best_hit_organism_col}' distribution for hits.\")\n",
    "print(\n",
    "    \"  - Comparing domain architectures between Asgard ESPs and their eukaryotic hits (as per plans).\"\n",
    ")\n",
    "print(\n",
    "    \"  - Focusing on hits for specific proteins of interest (e.g., Ig-like domain proteins).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d740776c-2bfc-4a77-a3bf-ca3736981bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell specifically integrated the eukaryotic diamond hit information, for the giant virus orthogroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98c97a2-5373-43ea-974e-b8cf31bc056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Integrate Giant Virus (GV) Eukaryotic DIAMOND Hit Information (v2 - Standardized Organism Names)\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import sys\n",
    "import os\n",
    "import numpy as np  # For NaN\n",
    "import re  # For parsing\n",
    "import logging  # For detailed logging\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_formatter = logging.Formatter(\n",
    "    \"%(asctime)s [%(levelname)s] %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(\"GV_Euk_Hit_Integration_V2\")\n",
    "logger.handlers = []\n",
    "logger.setLevel(logging.INFO)\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setFormatter(log_formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# --- Configuration ---\n",
    "MAIN_DB_INPUT_PATH = \"proteome_database_v2.3.csv\"  # Input is v2.3\n",
    "GV_DIAMOND_RESULTS_PATH = \"giant_virus_diamond_results/gv_vs_euk_diamond_hits.tsv\"\n",
    "EUK_FASTA_PATH = \"euk63_proteomes_final.fasta\"\n",
    "MAIN_DB_OUTPUT_PATH = (\n",
    "    \"proteome_database_v2.5_gv_euk_hits_std_org.csv\"  # Output will be v2.5\n",
    ")\n",
    "E_VALUE_THRESHOLD = 1e-10\n",
    "\n",
    "DIAMOND_COLS = [\n",
    "    \"qseqid_full\",\n",
    "    \"sseqid_full\",\n",
    "    \"pident\",\n",
    "    \"length\",\n",
    "    \"mismatch\",\n",
    "    \"gapopen\",\n",
    "    \"qstart\",\n",
    "    \"qend\",\n",
    "    \"sstart\",\n",
    "    \"send\",\n",
    "    \"evalue\",\n",
    "    \"bitscore\",\n",
    "    \"qlen\",\n",
    "    \"slen\",\n",
    "]\n",
    "\n",
    "\n",
    "# --- Helper Function to Parse Eukaryotic FASTA Headers (Refined for Genus species) ---\n",
    "def parse_euk_fasta_headers_for_hits_std_org(fasta_file: Path) -> dict:\n",
    "    logger.info(\n",
    "        f\"Parsing Eukaryotic FASTA headers from: {fasta_file} for hit annotation (std org)...\"\n",
    "    )\n",
    "    header_info = {}\n",
    "    count = 0\n",
    "    skipped_count = 0\n",
    "    if not fasta_file.is_file():\n",
    "        logger.error(f\"Eukaryotic FASTA file not found: {fasta_file}\")\n",
    "        return header_info\n",
    "    try:\n",
    "        for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            count += 1\n",
    "            description = record.description\n",
    "\n",
    "            first_pipe_idx = description.find(\"|\")\n",
    "            first_space_idx = description.find(\" \")\n",
    "            end_of_id = -1\n",
    "            if first_pipe_idx != -1 and (\n",
    "                first_space_idx == -1 or first_pipe_idx < first_space_idx\n",
    "            ):\n",
    "                end_of_id = first_pipe_idx\n",
    "            elif first_space_idx != -1:\n",
    "                end_of_id = first_space_idx\n",
    "            fasta_id_key = (\n",
    "                description[:end_of_id].strip()\n",
    "                if end_of_id != -1\n",
    "                else description.strip()\n",
    "            )\n",
    "\n",
    "            if not fasta_id_key:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            genus_species_raw = \"Unknown Species\"\n",
    "            protein_name = \"Unknown Protein\"\n",
    "            remaining_description = (\n",
    "                description[len(fasta_id_key) :].lstrip().lstrip(\"|\").strip()\n",
    "            )\n",
    "\n",
    "            organism_match_in_brackets = re.search(r\"\\[(.*?)\\]$\", remaining_description)\n",
    "            if organism_match_in_brackets:\n",
    "                genus_species_raw = organism_match_in_brackets.group(1).strip()\n",
    "                protein_name = remaining_description[\n",
    "                    : organism_match_in_brackets.start()\n",
    "                ].strip()\n",
    "                if not protein_name:\n",
    "                    protein_name = \"Unknown Protein (Organism in brackets)\"\n",
    "            else:\n",
    "                parts_after_id = remaining_description.split(\"|\")\n",
    "                if len(parts_after_id) >= 2:  # Format: ID|Organism Info|Protein Name\n",
    "                    genus_species_raw = parts_after_id[0].strip()\n",
    "                    protein_name = \" | \".join(p.strip() for p in parts_after_id[1:])\n",
    "                elif (\n",
    "                    len(parts_after_id) == 1 and parts_after_id[0]\n",
    "                ):  # Format: ID|Protein Name (or ID|Organism if only one word)\n",
    "                    # Heuristic: if it contains spaces, assume it's an organism name attempt\n",
    "                    if \" \" in parts_after_id[0] or \"_\" in parts_after_id[0]:\n",
    "                        genus_species_raw = parts_after_id[0].strip()\n",
    "                        protein_name = \"Unknown Protein (Organism only after ID)\"\n",
    "                    else:  # Assume it's protein name\n",
    "                        protein_name = parts_after_id[0].strip()\n",
    "                elif not parts_after_id and remaining_description:\n",
    "                    protein_name = remaining_description\n",
    "\n",
    "            genus_species_cleaned = \"Unknown Species\"\n",
    "            if genus_species_raw and genus_species_raw != \"Unknown Species\":\n",
    "                temp_name_parts = genus_species_raw.replace(\"_\", \" \").split()\n",
    "                if len(temp_name_parts) >= 2:\n",
    "                    genus_species_cleaned = f\"{temp_name_parts[0]} {temp_name_parts[1]}\"\n",
    "                elif len(temp_name_parts) == 1:\n",
    "                    genus_species_cleaned = temp_name_parts[0]\n",
    "\n",
    "            header_info[fasta_id_key] = {\n",
    "                \"Genus_Species\": genus_species_cleaned,\n",
    "                \"Protein_Name\": protein_name if protein_name else \"Unknown Protein\",\n",
    "            }\n",
    "            if count % 200000 == 0:\n",
    "                logger.info(f\"  Processed {count:,} Eukaryotic FASTA records...\")\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Error parsing Eukaryotic FASTA file {fasta_file}: {e}\", exc_info=True\n",
    "        )\n",
    "    logger.info(\n",
    "        f\"Finished Eukaryotic FASTA parsing. Extracted info for {len(header_info):,} sequences (Skipped: {skipped_count}).\"\n",
    "    )\n",
    "    return header_info\n",
    "\n",
    "\n",
    "# --- Main Processing Logic (largely same as before, uses new helper) ---\n",
    "logger.info(\"--- Starting GV Eukaryotic Hit Integration (v2) ---\")\n",
    "# ... (rest of the GV hit integration logic from the previous cell, ensuring it calls\n",
    "#      parse_euk_fasta_headers_for_hits_std_org and saves to MAIN_DB_OUTPUT_PATH)\n",
    "# The following is a condensed version of that logic:\n",
    "\n",
    "logger.info(f\"Reading main database: {MAIN_DB_INPUT_PATH}\")\n",
    "try:\n",
    "    df_main = pd.read_csv(MAIN_DB_INPUT_PATH, low_memory=False)\n",
    "    df_main[\"ProteinID\"] = df_main[\"ProteinID\"].astype(str)\n",
    "    if \"Original_Seq_Length\" not in df_main.columns:\n",
    "        df_main[\"Original_Seq_Length\"] = df_main.get(\n",
    "            \"Length\", pd.Series(np.nan, index=df_main.index)\n",
    "        )\n",
    "    logger.info(f\"Main database loaded. Shape: {df_main.shape}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading main database: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "logger.info(f\"Reading GV DIAMOND results: {GV_DIAMOND_RESULTS_PATH}\")\n",
    "if not Path(GV_DIAMOND_RESULTS_PATH).is_file():\n",
    "    logger.error(f\"GV DIAMOND results file not found: {GV_DIAMOND_RESULTS_PATH}.\")\n",
    "    sys.exit(1)\n",
    "try:\n",
    "    df_gv_hits_raw = pd.read_csv(\n",
    "        GV_DIAMOND_RESULTS_PATH, sep=\"\\t\", header=None, names=DIAMOND_COLS\n",
    "    )\n",
    "    df_gv_hits_raw[\"qseqid\"] = (\n",
    "        df_gv_hits_raw[\"qseqid_full\"].astype(str).str.split(\"|\").str[0]\n",
    "    )\n",
    "    df_gv_hits_raw[\"sseqid_full\"] = df_gv_hits_raw[\"sseqid_full\"].astype(str)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading GV DIAMOND results: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "df_gv_hits_filtered = df_gv_hits_raw[\n",
    "    df_gv_hits_raw[\"evalue\"] <= E_VALUE_THRESHOLD\n",
    "].copy()\n",
    "logger.info(\n",
    "    f\"  Found {len(df_gv_hits_filtered)} GV DIAMOND alignments passing e-value.\"\n",
    ")\n",
    "\n",
    "if df_gv_hits_filtered.empty:\n",
    "    logger.warning(\"No GV DIAMOND hits passed e-value. Saving original DB.\")\n",
    "    df_main.to_csv(MAIN_DB_OUTPUT_PATH, index=False, na_rep=\"NA\")\n",
    "else:\n",
    "    df_gv_hits_filtered.sort_values(\n",
    "        by=[\"qseqid\", \"evalue\", \"bitscore\"], ascending=[True, True, False], inplace=True\n",
    "    )\n",
    "    df_gv_best_hits = df_gv_hits_filtered.drop_duplicates(\n",
    "        subset=[\"qseqid\"], keep=\"first\"\n",
    "    ).copy()\n",
    "    logger.info(\n",
    "        f\"  Identified {len(df_gv_best_hits)} unique GV proteins with Euk hits.\"\n",
    "    )\n",
    "\n",
    "    euk_header_map = parse_euk_fasta_headers_for_hits_std_org(\n",
    "        Path(EUK_FASTA_PATH)\n",
    "    )  # Call refined parser\n",
    "\n",
    "    gv_hit_data_list = []\n",
    "    for _, row in df_gv_best_hits.iterrows():\n",
    "        gv_protein_id = row[\"qseqid\"]\n",
    "        euk_sseqid_full = row[\"sseqid_full\"]\n",
    "        euk_sseqid_clean = euk_sseqid_full.split(\"|\")[0].split(\" \")[0]\n",
    "        header_data = euk_header_map.get(euk_sseqid_clean, {})\n",
    "        euk_organism = header_data.get(\n",
    "            \"Genus_Species\", \"Unknown Organism\"\n",
    "        )  # This will now be \"Genus species\"\n",
    "        euk_prot_name = header_data.get(\"Protein_Name\", \"Unknown Protein Name\")\n",
    "\n",
    "        q_len_from_db_series = df_main.loc[\n",
    "            df_main[\"ProteinID\"] == gv_protein_id, \"Original_Seq_Length\"\n",
    "        ]\n",
    "        q_len_from_db = (\n",
    "            q_len_from_db_series.iloc[0]\n",
    "            if not q_len_from_db_series.empty and pd.notna(q_len_from_db_series.iloc[0])\n",
    "            else row[\"qlen\"]\n",
    "        )\n",
    "        if pd.isna(q_len_from_db) or q_len_from_db == 0:\n",
    "            q_len_from_db = row[\"qlen\"]\n",
    "\n",
    "        query_cov = (\n",
    "            (row[\"qend\"] - row[\"qstart\"] + 1) / q_len_from_db\n",
    "            if q_len_from_db > 0\n",
    "            else 0\n",
    "        )\n",
    "        subject_cov = (\n",
    "            (row[\"send\"] - row[\"sstart\"] + 1) / row[\"slen\"] if row[\"slen\"] > 0 else 0\n",
    "        )\n",
    "\n",
    "        gv_hit_data_list.append(\n",
    "            {\n",
    "                \"ProteinID\": gv_protein_id,\n",
    "                \"Has_Euk_DIAMOND_Hit\": True,\n",
    "                \"Euk_Hit_SSEQID\": euk_sseqid_clean,\n",
    "                \"Euk_Hit_Organism\": euk_organism,\n",
    "                \"Euk_Hit_PIDENT\": row[\"pident\"],\n",
    "                \"Euk_Hit_EVALUE\": row[\"evalue\"],\n",
    "                \"Euk_Hit_Protein_Name\": euk_prot_name,\n",
    "                \"Euk_Hit_Qstart\": row[\"qstart\"],\n",
    "                \"Euk_Hit_Qend\": row[\"qend\"],\n",
    "                \"Euk_Hit_Sstart\": row[\"sstart\"],\n",
    "                \"Euk_Hit_Send\": row[\"send\"],\n",
    "                \"Euk_Hit_Slen_Diamond\": row[\"slen\"],\n",
    "                \"Query_Coverage\": query_cov,\n",
    "                \"Subject_Coverage\": subject_cov,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df_gv_hits_to_update = pd.DataFrame(gv_hit_data_list)\n",
    "\n",
    "    if not df_gv_hits_to_update.empty:\n",
    "        logger.info(\n",
    "            f\"Updating {len(df_gv_hits_to_update)} GV proteins with Euk hit details.\"\n",
    "        )\n",
    "        euk_hit_cols = [\n",
    "            \"Has_Euk_DIAMOND_Hit\",\n",
    "            \"Euk_Hit_SSEQID\",\n",
    "            \"Euk_Hit_Organism\",\n",
    "            \"Euk_Hit_PIDENT\",\n",
    "            \"Euk_Hit_EVALUE\",\n",
    "            \"Euk_Hit_Protein_Name\",\n",
    "            \"Euk_Hit_Qstart\",\n",
    "            \"Euk_Hit_Qend\",\n",
    "            \"Euk_Hit_Sstart\",\n",
    "            \"Euk_Hit_Send\",\n",
    "            \"Euk_Hit_Slen_Diamond\",\n",
    "            \"Query_Coverage\",\n",
    "            \"Subject_Coverage\",\n",
    "        ]\n",
    "        for col in euk_hit_cols:\n",
    "            if col not in df_main.columns:\n",
    "                df_main[col] = np.nan if col != \"Has_Euk_DIAMOND_Hit\" else False\n",
    "\n",
    "        df_main = df_main.set_index(\"ProteinID\")\n",
    "        df_gv_hits_to_update = df_gv_hits_to_update.set_index(\"ProteinID\")\n",
    "        gv_indices = df_main[df_main[\"Group\"] == \"GV\"].index\n",
    "        common_indices = gv_indices.intersection(df_gv_hits_to_update.index)\n",
    "\n",
    "        for col in df_gv_hits_to_update.columns:\n",
    "            df_main.loc[common_indices, col] = df_gv_hits_to_update.loc[\n",
    "                common_indices, col\n",
    "            ]\n",
    "\n",
    "        gv_without_hits_indices = gv_indices.difference(df_gv_hits_to_update.index)\n",
    "        if not gv_without_hits_indices.empty:\n",
    "            df_main.loc[gv_without_hits_indices, \"Has_Euk_DIAMOND_Hit\"] = False\n",
    "            for col in euk_hit_cols:\n",
    "                if col != \"Has_Euk_DIAMOND_Hit\":\n",
    "                    df_main.loc[gv_without_hits_indices, col] = np.nan\n",
    "        df_main.reset_index(inplace=True)\n",
    "        logger.info(\n",
    "            f\"Total GV proteins now flagged with Euk hits: {df_main[(df_main['Group'] == 'GV') & (df_main['Has_Euk_DIAMOND_Hit'] == True)].shape[0]}\"\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"No GV Euk hits to merge after processing.\")\n",
    "        df_main.loc[df_main[\"Group\"] == \"GV\", \"Has_Euk_DIAMOND_Hit\"] = df_main.loc[\n",
    "            df_main[\"Group\"] == \"GV\", \"Has_Euk_DIAMOND_Hit\"\n",
    "        ].fillna(False)\n",
    "\n",
    "    try:\n",
    "        df_main.to_csv(MAIN_DB_OUTPUT_PATH, index=False, na_rep=\"NA\")\n",
    "        logger.info(f\"Successfully saved updated database to: {MAIN_DB_OUTPUT_PATH}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing updated database CSV: {e}\")\n",
    "\n",
    "logger.info(\"--- GV Eukaryotic Hit Integration Cell (v2) Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac69326-5c9c-4e49-9e25-c5e78ace638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell merged the intra-orthogroup average pairwise sequence identity data into the proteome_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1049141d-fd13-4fb0-91cc-2cc696570a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Merge Intra-OG APSI Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Input: Path to the main database CSV created by a previous cell\n",
    "# Adjust this path to match the output of your main assembly cell\n",
    "MAIN_DB_INPUT_PATH = \"proteome_database_v2.5.csv\"\n",
    "\n",
    "# Input: Path to the APSI values CSV file\n",
    "APSI_CSV_PATH = \"output_summary_data_hit_validation_phase1/intra_og_apsi_values.csv\"  # Make sure this path is correct\n",
    "\n",
    "# Output: Path to save the database CSV after merging APSI data\n",
    "# You can overwrite the input file or save to a new file\n",
    "MAIN_DB_OUTPUT_PATH = \"proteome_database_v2.6.csv\"\n",
    "\n",
    "# Column name in the main database DataFrame that contains the Orthogroup ID\n",
    "MAIN_DB_OG_COLUMN = \"Orthogroup\"  # Verify this matches your main DataFrame\n",
    "\n",
    "# Column name in the APSI CSV file that contains the Orthogroup ID\n",
    "APSI_DB_OG_COLUMN = \"Orthogroup\"  # Verify this matches the APSI CSV file\n",
    "\n",
    "# --- Setup Logging (Optional, but good practice) ---\n",
    "logger = logging.getLogger(__name__)\n",
    "# Basic configuration if not already set up in the notebook\n",
    "if not logger.hasHandlers():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "\n",
    "# --- Main Logic ---\n",
    "logger.info(\"--- Starting APSI Merge Cell ---\")\n",
    "logger.info(f\"Reading main database from: {MAIN_DB_INPUT_PATH}\")\n",
    "\n",
    "try:\n",
    "    # Load the main database CSV\n",
    "    df_main = pd.read_csv(MAIN_DB_INPUT_PATH, low_memory=False)\n",
    "    logger.info(f\"Loaded {len(df_main):,} records from main database.\")\n",
    "    logger.info(f\"Main DB columns: {df_main.columns.tolist()}\")\n",
    "\n",
    "    # Check if the main OG column exists\n",
    "    if MAIN_DB_OG_COLUMN not in df_main.columns:\n",
    "        logger.error(\n",
    "            f\"Required column '{MAIN_DB_OG_COLUMN}' not found in {MAIN_DB_INPUT_PATH}. Cannot merge APSI.\"\n",
    "        )\n",
    "        raise KeyError(f\"Column '{MAIN_DB_OG_COLUMN}' not found.\")\n",
    "\n",
    "    logger.info(f\"Reading APSI data from: {APSI_CSV_PATH}\")\n",
    "    # Load the APSI data CSV\n",
    "    df_apsi = pd.read_csv(APSI_CSV_PATH)\n",
    "    logger.info(f\"Loaded {len(df_apsi):,} records from APSI file.\")\n",
    "    logger.info(f\"APSI DB columns: {df_apsi.columns.tolist()}\")\n",
    "\n",
    "    # Check if the APSI OG column exists\n",
    "    if APSI_DB_OG_COLUMN not in df_apsi.columns:\n",
    "        logger.error(\n",
    "            f\"Required column '{APSI_DB_OG_COLUMN}' not found in {APSI_CSV_PATH}. Cannot merge.\"\n",
    "        )\n",
    "        raise KeyError(f\"Column '{APSI_DB_OG_COLUMN}' not found.\")\n",
    "\n",
    "    # Prepare APSI DataFrame for merging\n",
    "    # Rename columns for clarity and to avoid potential conflicts\n",
    "    df_apsi_renamed = df_apsi.rename(\n",
    "        columns={\n",
    "            \"APSI\": \"Intra_OG_APSI\",\n",
    "            \"Num_Sequences\": \"Num_OG_Sequences\",\n",
    "            APSI_DB_OG_COLUMN: \"APSI_Orthogroup_Key\",  # Use a temporary unique key name\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Select only necessary columns from APSI data\n",
    "    apsi_cols_to_merge = [\"APSI_Orthogroup_Key\", \"Intra_OG_APSI\", \"Num_OG_Sequences\"]\n",
    "    df_apsi_to_merge = df_apsi_renamed[apsi_cols_to_merge]\n",
    "\n",
    "    # Perform the merge\n",
    "    logger.info(\n",
    "        f\"Merging APSI data into main database on '{MAIN_DB_OG_COLUMN}' <-> '{APSI_DB_OG_COLUMN}'\"\n",
    "    )\n",
    "    # Keep track of original columns to see if merge adds columns unexpectedly\n",
    "    original_cols = set(df_main.columns)\n",
    "\n",
    "    df_merged = df_main.merge(\n",
    "        df_apsi_to_merge,\n",
    "        left_on=MAIN_DB_OG_COLUMN,\n",
    "        right_on=\"APSI_Orthogroup_Key\",\n",
    "        how=\"left\",  # Keep all rows from the main database\n",
    "    )\n",
    "\n",
    "    # Check if merge was successful and drop the temporary key\n",
    "    if \"Intra_OG_APSI\" in df_merged.columns:\n",
    "        logger.info(\n",
    "            \"Merge successful. Added 'Intra_OG_APSI' and 'Num_OG_Sequences' columns.\"\n",
    "        )\n",
    "        # Drop the temporary key column used for merging\n",
    "        if \"APSI_Orthogroup_Key\" in df_merged.columns:\n",
    "            df_merged.drop(columns=[\"APSI_Orthogroup_Key\"], inplace=True)\n",
    "\n",
    "        # Report merge statistics\n",
    "        apsi_merged_count = df_merged[\"Intra_OG_APSI\"].notna().sum()\n",
    "        total_rows = len(df_merged)\n",
    "        logger.info(\n",
    "            f\"Number of proteins with merged APSI values: {apsi_merged_count} / {total_rows} ({apsi_merged_count / total_rows:.2%})\"\n",
    "        )\n",
    "\n",
    "        # Check if the number of rows changed unexpectedly\n",
    "        if len(df_merged) != len(df_main):\n",
    "            logger.warning(\n",
    "                f\"Row count changed after merge! Original: {len(df_main)}, Merged: {len(df_merged)}\"\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        logger.warning(\n",
    "            \"Merge completed, but 'Intra_OG_APSI' column was not added. Check column names and merge keys.\"\n",
    "        )\n",
    "        # Ensure placeholder columns exist if merge failed to add them\n",
    "        if \"Intra_OG_APSI\" not in df_merged.columns:\n",
    "            df_merged[\"Intra_OG_APSI\"] = pd.NA\n",
    "        if \"Num_OG_Sequences\" not in df_merged.columns:\n",
    "            df_merged[\"Num_OG_Sequences\"] = pd.NA\n",
    "\n",
    "    # Save the updated DataFrame\n",
    "    logger.info(f\"Saving updated database with APSI info to: {MAIN_DB_OUTPUT_PATH}\")\n",
    "    df_merged.to_csv(MAIN_DB_OUTPUT_PATH, index=False, na_rep=\"NA\")\n",
    "    logger.info(\"Successfully saved updated database.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"File not found error: {e}. Please check input paths.\")\n",
    "except KeyError as e:\n",
    "    logger.error(\n",
    "        f\"Column name error: {e}. Please check column names in configuration and CSV files.\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(\n",
    "        f\"An unexpected error occurred during APSI merge: {e}\", exc_info=True\n",
    "    )  # Log traceback\n",
    "\n",
    "logger.info(\"--- APSI Merge Cell Finished ---\")\n",
    "\n",
    "# Display the first few rows and info of the merged dataframe (optional)\n",
    "# print(\"\\nMerged DataFrame Info:\")\n",
    "# df_merged.info()\n",
    "# print(\"\\nMerged DataFrame Head:\")\n",
    "# print(df_merged.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (proteome_env)",
   "language": "python",
   "name": "proteome_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
