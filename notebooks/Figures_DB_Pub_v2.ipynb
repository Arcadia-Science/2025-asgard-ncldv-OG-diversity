{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82641214-bcf0-44fd-bdd7-bde412b2dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is designed to generate all the figures for the publication \"Assembling and annotating an Asgard archaea and giant virus dataset of over 840,000 proteins.\" It loads the final, fully annotated proteome database and uses Plotly to create a series of visualizations that characterize the dataset, explore protein features, and analyze sequence conservation. Each cell is organized to produce a specific figure or a panel within a figure, with outputs saved to a dedicated directory for publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20291be9-8ae4-45dd-9490-a69f71466ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Cell\n",
    "# This cell handles all the necessary imports, sets up logging, defines file paths, configures plotting defaults to match the Arcadia style guide, loads auxiliary data like InterPro entries, and defines helper functions. It concludes by loading the main proteome database and creating color maps for consistent data visualization across all figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f65b1-5650-4237-af59-5c442399e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup for Figures Notebook - Load Data and Define Helpers\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re  # For parsing organism names\n",
    "import sys  # For exit\n",
    "import logging  # For detailed logging\n",
    "from pathlib import Path  # For handling paths\n",
    "\n",
    "# Ensure Biopython is installed: pip install biopython\n",
    "try:\n",
    "    from Bio import SeqIO\n",
    "except ImportError:\n",
    "    print(\n",
    "        \"ERROR: Biopython is required for this script. Please install it: pip install biopython\"\n",
    "    )\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Plotly Imports ---\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "# --- Setup Logging ---\n",
    "# Get a named logger for this notebook\n",
    "logger = logging.getLogger(__name__)\n",
    "# Prevent adding handlers multiple times if script is re-run in interactive session\n",
    "if not logger.hasHandlers():\n",
    "    logger.setLevel(logging.INFO)\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    log_formatter = logging.Formatter(\n",
    "        \"%(asctime)s [%(levelname)s] %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "    console_handler.setFormatter(log_formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "else:\n",
    "    # Ensure level is set if handlers already exist (e.g., in notebook re-run)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "print(\"--- Figures Notebook Setup ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# INPUT file path: Use the latest database version that includes Euk annotations,\n",
    "# DIAMOND hit details, coverage columns, RBH flag, and potentially merged APSI/Motif data.\n",
    "# Make sure this path points to your most recent, complete database file!\n",
    "database_path = \"proteome_database_v3.5.csv\"  # <-- Update if your latest file has a different name/path\n",
    "\n",
    "# Directory where the previous notebook saved summary data (APSI, Motifs, etc.)\n",
    "# This path needs to match the output_summary_dir_phase1 from your analysis notebook\n",
    "# This is needed to load APSI/Motif data if it's not already in the main database file.\n",
    "output_summary_dir_phase1 = Path(\n",
    "    \"./output_summary_data_hit_validation_phase1\"\n",
    ")  # <-- Update this path if different\n",
    "\n",
    "# Directory to save plots and summary data generated in this notebook\n",
    "# Use distinct directory names for figure outputs vs. analysis outputs\n",
    "output_figure_dir = \"publication_figures\"\n",
    "output_figure_summary_dir = \"publication_figure_data\"  # For tables/data backing figures\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "# Use Path objects here for consistency\n",
    "Path(output_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Ensured plot output directory exists: {output_figure_dir}\")\n",
    "Path(output_figure_summary_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Ensured summary data directory exists: {output_figure_summary_dir}\")\n",
    "\n",
    "# Path to InterPro entry list file (ensure this file is in the correct location)\n",
    "# This is needed for translating IPR/domain names in helper functions\n",
    "interpro_entry_path = \"interpro_entry.txt\"  # <-- Update path if needed\n",
    "\n",
    "\n",
    "# --- Define Key Column Names (Based on your database structure) ---\n",
    "# Define all relevant column names here for easy access and consistency\n",
    "protein_id_col = \"ProteinID\"\n",
    "sequence_col = \"Sequence\"\n",
    "source_dataset_col = \"Source_Dataset\"\n",
    "source_genome_accession_col = \"Source_Genome_Accession\"\n",
    "source_protein_annotation_col = \"Source_Protein_Annotation\"\n",
    "ncbi_taxid_col = \"NCBI_TaxID\"\n",
    "asgard_phylum_col = \"Asgard_Phylum\"\n",
    "virus_family_col = \"Virus_Family\"\n",
    "virus_name_col = \"Virus_Name\"\n",
    "orthogroup_col = \"OG_ID\"\n",
    "ipr_col = \"IPR_Signatures\"\n",
    "ipr_go_terms_col = \"IPR_GO_Terms\"\n",
    "uniprotkb_ac_col = \"UniProtKB_AC\"\n",
    "num_domains_col = \"Num_Domains\"\n",
    "domain_arch_col = \"Domain_Architecture\"\n",
    "type_col = \"Type\"  # As in 'Annotated', 'Uncharacterized'\n",
    "is_hypothetical_col = \"Is_Hypothetical\"\n",
    "has_known_structure_col = \"Has_Known_Structure\"\n",
    "percent_disorder_col = \"Percent_Disorder\"\n",
    "specific_func_cat_col = \"Specific_Functional_Category\"\n",
    "broad_func_cat_col = \"Broad_Functional_Category\"\n",
    "category_trigger_col = \"Category_Trigger\"\n",
    "signal_peptide_col = \"Signal_Peptide_USPNet\"\n",
    "sp_cleavage_site_col = \"SP_Cleavage_Site_USPNet\"\n",
    "original_seq_length_col = \"Original_Seq_Length\"\n",
    "group_col = \"Group\"  # 'Asgard' or 'GV'\n",
    "seqsearch_pdb_hit_col = \"SeqSearch_PDB_Hit\"\n",
    "seqsearch_afdb_hit_col = \"SeqSearch_AFDB_Hit\"\n",
    "has_reference_structure_col = \"Has_Reference_Structure\"\n",
    "localization_col = \"Predicted_Subcellular_Localization\"\n",
    "mature_protein_sequence_col = \"Mature_Protein_Sequence\"\n",
    "mature_seq_length_col = \"Mature_Seq_Length\"\n",
    "seqsearch_mgnify_hit_col = \"SeqSearch_MGnify_Hit\"\n",
    "seqsearch_esma_hit_col = \"SeqSearch_ESMA_Hit\"\n",
    "structurally_dark_col = \"Is_Structurally_Dark\"  # Derived column\n",
    "esp_col = \"Is_ESP\"  # Derived column\n",
    "hit_flag_col = \"Has_Euk_DIAMOND_Hit\"  # Flag for any Euk hit passing initial e-value\n",
    "euk_hit_sseqid_col = \"Euk_Hit_SSEQID\"  # Best Euk hit SSEQID\n",
    "euk_hit_organism_col = \"Euk_Hit_Organism\"  # Best Euk hit organism\n",
    "euk_hit_pident_col = \"Euk_Hit_PIDENT\"  # Best Euk hit PIDENT\n",
    "euk_hit_evalue_col = \"Euk_Hit_EVALUE\"  # Best Euk hit EVALUE\n",
    "euk_hit_protein_name_col = \"Euk_Hit_Protein_Name\"  # Best Euk hit protein name\n",
    "euk_hit_qstart_col = \"Euk_Hit_Qstart\"  # Alignment start on query (Asgard)\n",
    "euk_hit_qend_col = \"Euk_Hit_Qend\"  # Alignment end on query (Asgard)\n",
    "euk_hit_sstart_col = \"Euk_Hit_Sstart\"  # Alignment start on subject (Euk)\n",
    "euk_hit_send_col = \"Euk_Hit_Send\"  # Alignment end on subject (Euk)\n",
    "euk_hit_slen_diamond_col = (\n",
    "    \"Euk_Hit_Slen_Diamond\"  # Length of Euk hit from DIAMOND output\n",
    ")\n",
    "query_coverage_col = \"Query_Coverage\"  # Calculated query coverage\n",
    "subject_coverage_col = \"Subject_Coverage\"  # Calculated subject coverage\n",
    "rbh_flag_col = \"Is_RBH\"  # Flag for Reciprocal Best Hit (if RBH analysis was included)\n",
    "\n",
    "# Add columns for APSI and Motifs if they were added to the main DB\n",
    "apsi_col = \"Intra_OG_APSI\"  # Assuming this column name if merged\n",
    "motif_col = (\n",
    "    \"Conserved_Motifs\"  # Assuming this column name if merged (might be list/string)\n",
    ")\n",
    "num_og_sequences_col = (\n",
    "    \"Num_OG_Sequences\"  # Number of sequences in the OG (used for MSA)\n",
    ")\n",
    "\n",
    "# --- Define Arcadia Color Palettes (from Style Guide PDF) ---\n",
    "print(\"\\n--- Defining Manual Arcadia Color Palettes (from Style Guide) ---\")\n",
    "# Define all colors from the style guide PDF for flexibility\n",
    "arcadia_colors_manual = {\n",
    "    \"aegean\": \"#5088C5\",\n",
    "    \"amber\": \"#F28360\",\n",
    "    \"seaweed\": \"#3B9886\",\n",
    "    \"canary\": \"#F7B846\",\n",
    "    \"aster\": \"#7A77AB\",\n",
    "    \"rose\": \"#F898AE\",\n",
    "    \"vital\": \"#73B5E3\",\n",
    "    \"tangerine\": \"#FFB984\",  # Corrected Tangerine from PDF: FFb883 -> FFB984 (common web palette) - using FFB984 as per previous code\n",
    "    \"oat\": \"#F5E4BE\",\n",
    "    \"wish\": \"#BABEE0\",\n",
    "    \"lime\": \"#97CD78\",\n",
    "    \"dragon\": \"#C85152\",\n",
    "    \"sky\": \"#C6E7F4\",\n",
    "    \"dress\": \"#F8C5C1\",\n",
    "    \"taupe\": \"#DBD1C3\",\n",
    "    \"denim\": \"#B6C8D4\",\n",
    "    \"sage\": \"#B5BEA4\",\n",
    "    \"mars\": \"#DA9085\",\n",
    "    \"marine\": \"#8A99AD\",\n",
    "    \"shell\": \"#EDE0D6\",\n",
    "    \"white\": \"#FFFFFF\",\n",
    "    \"gray\": \"#EBEDE8\",\n",
    "    \"chateau\": \"#B9AFA7\",  # Corrected Chateau from PDF: BAB0A8 -> B9AFA7\n",
    "    \"bark\": \"#8F8885\",\n",
    "    \"slate\": \"#43413F\",\n",
    "    \"charcoal\": \"#484B50\",\n",
    "    \"crow\": \"#292928\",\n",
    "    \"black\": \"#09090A\",\n",
    "    \"forest\": \"#596F74\",  # Forest is in Neutrals in PDF\n",
    "    \"parchment\": \"#FEF7F1\",\n",
    "    \"zephyr\": \"#F4FBFF\",  # Corrected Zephyr from PDF: F4FBFE -> F4FBFF\n",
    "    \"lichen\": \"#F7FBEF\",\n",
    "    \"dawn\": \"#F8F4F1\",\n",
    "}\n",
    "# Define the specific palettes as lists of hex codes for easy cycling/use\n",
    "arcadia_primary_palette = [\n",
    "    arcadia_colors_manual[c]\n",
    "    for c in [\n",
    "        \"aegean\",\n",
    "        \"amber\",\n",
    "        \"seaweed\",\n",
    "        \"canary\",\n",
    "        \"aster\",\n",
    "        \"rose\",\n",
    "        \"vital\",\n",
    "        \"tangerine\",\n",
    "        \"oat\",\n",
    "        \"wish\",\n",
    "        \"lime\",\n",
    "        \"dragon\",\n",
    "    ]\n",
    "]\n",
    "arcadia_secondary_palette = [\n",
    "    arcadia_colors_manual[c]\n",
    "    for c in [\"sky\", \"dress\", \"taupe\", \"denim\", \"sage\", \"mars\", \"marine\", \"shell\"]\n",
    "]\n",
    "arcadia_neutrals_palette = [\n",
    "    arcadia_colors_manual[c]\n",
    "    for c in [\"gray\", \"chateau\", \"bark\", \"slate\", \"charcoal\", \"forest\", \"crow\"]\n",
    "]\n",
    "arcadia_background_palette = [\n",
    "    arcadia_colors_manual[c] for c in [\"parchment\", \"zephyr\", \"lichen\", \"dawn\"]\n",
    "]  # Added background colors\n",
    "\n",
    "print(\"Manual Arcadia palettes created.\")\n",
    "\n",
    "# --- Configure Plotly Defaults (Adhering to Style Guide) ---\n",
    "print(\"\\n--- Configuring Plotly Defaults (Adhering to Style Guide) ---\")\n",
    "pio.templates.default = \"plotly_white\"  # Start with a clean white background\n",
    "\n",
    "# Define default layout for bold axis titles, NO gridlines, strong black axis lines, and NO plot titles\n",
    "# THIS VARIABLE NEEDS TO BE DEFINED *BEFORE* RUNNING FIGURE CELLS\n",
    "plotly_layout_defaults = go.Layout(\n",
    "    xaxis=dict(\n",
    "        title=dict(\n",
    "            font=dict(size=15, color=\"black\", family=\"Arial\", weight=\"bold\")\n",
    "        ),  # Bold axis title, 15pt\n",
    "        showgrid=False,  # Remove x-axis gridlines\n",
    "        zeroline=False,  # Optional: Remove zero line as well\n",
    "        showline=True,  # Show axis line\n",
    "        linecolor=\"black\",  # Axis line color\n",
    "        linewidth=1.5,  # Axis line width (adjust as needed for \"strong\")\n",
    "        mirror=False,  # Draw line on opposite side (False means only on the side where ticks/labels are)\n",
    "        ticks=\"outside\",  # Show ticks outside the axis line\n",
    "        ticklen=5,  # Tick length (5px as per style guide)\n",
    "        tickwidth=1.5,  # Tick width (match axis line width for prominence)\n",
    "        tickcolor=\"black\",  # Tick color\n",
    "        # Tick label font can be set here too if needed, e.g., tickfont=dict(size=15, family='Arial', color='black')\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=dict(\n",
    "            font=dict(size=15, color=\"black\", family=\"Arial\", weight=\"bold\")\n",
    "        ),  # Bold axis title, 15pt\n",
    "        showgrid=False,  # Remove y-axis gridlines\n",
    "        zeroline=False,  # Optional: Remove zero line as well\n",
    "        showline=True,  # Show axis line\n",
    "        linecolor=\"black\",  # Axis line color\n",
    "        linewidth=1.5,  # Axis line width\n",
    "        mirror=False,  # Draw line on opposite side\n",
    "        ticks=\"outside\",  # Show ticks outside the axis line\n",
    "        ticklen=5,  # Tick length\n",
    "        tickwidth=1.5,  # Tick width\n",
    "        tickcolor=\"black\",  # Tick color\n",
    "        # Tick label font can be set here too if needed\n",
    "    ),\n",
    "    title=None,  # Ensure no plot title by default\n",
    "    # Optional: Configure default font for tick labels if needed\n",
    "    font=dict(family=\"Arial\", size=15, color=\"black\"),  # Example for 15pt tick labels\n",
    "    # Optional: Configure default legend title font\n",
    "    # legend=dict(title=dict(font=dict(weight='bold')))\n",
    ")\n",
    "\n",
    "# Apply these default layout settings directly to the plotly_white template\n",
    "# This should ensure they are applied to all px plots by default\n",
    "pio.templates[\"plotly_white\"].layout.xaxis.title.font.update(size=15, weight=\"bold\")\n",
    "pio.templates[\"plotly_white\"].layout.yaxis.title.font.update(size=15, weight=\"bold\")\n",
    "pio.templates[\"plotly_white\"].layout.xaxis.showgrid = False\n",
    "pio.templates[\"plotly_white\"].layout.yaxis.showgrid = False\n",
    "pio.templates[\"plotly_white\"].layout.xaxis.showline = True  # Ensure line is shown\n",
    "pio.templates[\"plotly_white\"].layout.xaxis.linecolor = \"black\"\n",
    "pio.templates[\"plotly_white\"].layout.xaxis.linewidth = 1.5\n",
    "pio.templates[\"plotly_white\"].layout.xaxis.mirror = False  # Draw line on one side\n",
    "pio.templates[\"plotly_white\"].layout.xaxis.ticks = \"outside\"  # Show ticks outside\n",
    "pio.templates[\"plotly_white\"].layout.xaxis.ticklen = 5  # Tick length\n",
    "pio.templates[\"plotly_white\"].layout.xaxis.tickwidth = 1.5  # Tick width\n",
    "pio.templates[\"plotly_white\"].layout.xaxis.tickcolor = \"black\"  # Tick color\n",
    "\n",
    "pio.templates[\"plotly_white\"].layout.yaxis.showline = True  # Ensure line is shown\n",
    "pio.templates[\"plotly_white\"].layout.yaxis.linecolor = \"black\"\n",
    "pio.templates[\"plotly_white\"].layout.yaxis.linewidth = 1.5\n",
    "pio.templates[\"plotly_white\"].layout.yaxis.mirror = False  # Draw line on one side\n",
    "pio.templates[\"plotly_white\"].layout.yaxis.ticks = \"outside\"  # Show ticks outside\n",
    "pio.templates[\"plotly_white\"].layout.yaxis.ticklen = 5  # Tick length\n",
    "pio.templates[\"plotly_white\"].layout.yaxis.tickwidth = 1.5  # Tick width\n",
    "pio.templates[\"plotly_white\"].layout.yaxis.tickcolor = \"black\"  # Tick color\n",
    "\n",
    "\n",
    "pio.templates[\"plotly_white\"].layout.title = None  # Set no title in template\n",
    "# Note: Modifying default templates can have broad effects. Applying to individual figures or using update_layout might be safer.\n",
    "\n",
    "print(\"Plotly default template set to 'plotly_white'.\")\n",
    "print(\n",
    "    \"Default layout settings configured for bold axis titles (15pt), NO gridlines, strong black axis lines (1.5pt), tick marks (5px, 1.5pt, black), and NO plot titles.\"\n",
    ")\n",
    "\n",
    "\n",
    "# --- Load InterPro Entry Data ---\n",
    "# This is needed for translating IPR IDs to names in domain architectures/IPRs\n",
    "print(f\"\\n--- Loading InterPro Entry Data from '{interpro_entry_path}' ---\")\n",
    "ipr_lookup = {}\n",
    "start_time_ipr = time.time()\n",
    "try:\n",
    "    # Adjust usecols/names if your interpro_entry.txt file format is different\n",
    "    ipr_info_df = pd.read_csv(\n",
    "        interpro_entry_path,\n",
    "        sep=\"\\t\",\n",
    "        usecols=[0, 1, 2],\n",
    "        names=[\"IPR_ID\", \"Type\", \"Name\"],\n",
    "        header=0,\n",
    "        comment=\"#\",\n",
    "        on_bad_lines=\"warn\",\n",
    "    )\n",
    "    # Ensure column names match what's expected after loading\n",
    "    if \"ENTRY_AC\" in ipr_info_df.columns:\n",
    "        ipr_info_df.rename(columns={\"ENTRY_AC\": \"IPR_ID\"}, inplace=True)\n",
    "    if \"ENTRY_TYPE\" in ipr_info_df.columns:\n",
    "        ipr_info_df.rename(columns={\"ENTRY_TYPE\": \"Type\"}, inplace=True)\n",
    "    if \"ENTRY_NAME\" in ipr_info_df.columns:\n",
    "        ipr_info_df.rename(columns={\"ENTRY_NAME\": \"Name\"}, inplace=True)\n",
    "\n",
    "    if {\"IPR_ID\", \"Name\", \"Type\"}.issubset(ipr_info_df.columns):\n",
    "        ipr_info_df[\"IPR_ID\"] = ipr_info_df[\"IPR_ID\"].astype(str).str.strip()\n",
    "        ipr_lookup = ipr_info_df.set_index(\"IPR_ID\")[[\"Type\", \"Name\"]].to_dict(\"index\")\n",
    "        print(\n",
    "            f\"Loaded InterPro entry data for {len(ipr_lookup)} entries in {time.time() - start_time_ipr:.2f} seconds.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Warning: Expected columns ('IPR_ID', 'Name', 'Type') not all found in '{interpro_entry_path}' after loading.\"\n",
    "        )\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"Warning: InterPro entry file not found at '{interpro_entry_path}'. Domain name translations will not be available.\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"Warning: An error occurred loading or processing '{interpro_entry_path}': {e}\"\n",
    "    )\n",
    "\n",
    "if not ipr_lookup:\n",
    "    print(\n",
    "        \"Warning: ipr_lookup is empty. Domain name translations will not be available for plots/tables.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Define Helper Functions ---\n",
    "print(\"\\n--- Defining Helper Functions ---\")\n",
    "# Include necessary helper functions from your previous notebook here.\n",
    "# You might need: translate_architecture, truncate_string, clean_protein_name, etc.\n",
    "# Only include functions actually needed for figure generation and data manipulation *within* this notebook.\n",
    "\n",
    "\n",
    "# Example: translate_architecture (requires ipr_lookup)\n",
    "def translate_architecture(arch_string, lookup_dict=ipr_lookup):\n",
    "    \"\"\"Translates IPR IDs in a domain architecture string to names using a lookup dictionary.\"\"\"\n",
    "    if not isinstance(arch_string, str) or not arch_string or not lookup_dict:\n",
    "        return arch_string  # Return original if invalid input or no lookup\n",
    "\n",
    "    processed_arch_string = str(arch_string).replace(\"|\", \";\")\n",
    "    ipr_ids = processed_arch_string.split(\";\")\n",
    "    translated_parts = []\n",
    "    for ipr_id in ipr_ids:\n",
    "        ipr_id_clean = ipr_id.strip()\n",
    "        if ipr_id_clean in lookup_dict:\n",
    "            name = lookup_dict[ipr_id_clean].get(\"Name\", ipr_id_clean)\n",
    "            # Truncate long names for display\n",
    "            name = name[:40] + \"...\" if len(name) > 43 else name\n",
    "            translated_parts.append(f\"{name} ({ipr_id_clean})\")\n",
    "        elif ipr_id_clean:\n",
    "            translated_parts.append(ipr_id_clean)  # Keep unknown IDs\n",
    "\n",
    "    full_translation = \"; \".join(translated_parts)\n",
    "    max_len_display = 150  # Max length for display\n",
    "    if len(full_translation) > max_len_display:\n",
    "        full_translation = full_translation[: max_len_display - 3] + \"...\"\n",
    "\n",
    "    return full_translation\n",
    "\n",
    "\n",
    "# Example: truncate_string\n",
    "def truncate_string(text, max_len):\n",
    "    \"\"\"Truncates a string if it exceeds max_len, adding '...'.\"\"\"\n",
    "    if isinstance(text, str) and len(text) > max_len:\n",
    "        return text[: max_len - 3] + \"...\"\n",
    "    return text\n",
    "\n",
    "\n",
    "# Example: clean_protein_name\n",
    "def clean_protein_name(name):\n",
    "    \"\"\"Cleans common annotations from protein names.\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"Unknown/Not Found\"\n",
    "        name = str(name).strip()\n",
    "    name = re.sub(r\"\\s*\\|\\s*.*\", \"\", name)\n",
    "    name = re.sub(r\"\\bisoform\\s+[\\w-]+\\b\", \"\", name, flags=re.IGNORECASE).strip()\n",
    "    name = re.sub(r\"\\bpartial\\b\", \"\", name, flags=re.IGNORECASE).strip()\n",
    "    name = re.sub(r\"\\bputative\\b\", \"\", name, flags=re.IGNORECASE).strip()\n",
    "    name = re.sub(r\"\\bpredicted protein\\b\", \"\", name, flags=re.IGNORECASE).strip()\n",
    "    name = re.sub(r\"\\btype\\s+\\w+\\b\", \"\", name, flags=re.IGNORECASE).strip()\n",
    "    name = re.sub(r\"\\bprotein\\b\", \"\", name, flags=re.IGNORECASE).strip()\n",
    "    name = re.sub(\n",
    "        r\"\\buncharacterized\\b\", \"Uncharacterized\", name, flags=re.IGNORECASE\n",
    "    ).strip()\n",
    "    name = re.sub(r\"[;,]$\", \"\", name).strip()\n",
    "    name = re.sub(r\"\\s*\\(Fragment\\)$\", \"\", name, flags=re.IGNORECASE).strip()\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "    name = name[0].upper() + name[1:] if len(name) > 0 else name\n",
    "    return name if name else \"Unknown/Not Found\"\n",
    "\n",
    "\n",
    "# Add other necessary helper functions here...\n",
    "# e.g., get_ipr_counts if you plot IPR frequencies directly\n",
    "\n",
    "\n",
    "# --- Load Main Database ---\n",
    "print(f\"\\n--- Loading Data from '{database_path}' ---\")\n",
    "try:\n",
    "    # Load the database that already contains all annotations,\n",
    "    # including Euk hit details, coverage, and RBH flag (if added).\n",
    "    df_full = pd.read_csv(database_path, low_memory=False)\n",
    "    # Ensure ProteinID is string type right after loading\n",
    "    if protein_id_col in df_full.columns:\n",
    "        df_full[protein_id_col] = df_full[protein_id_col].astype(str)\n",
    "\n",
    "    # --- Merge APSI and Motif Data ---\n",
    "    # Assuming APSI and Motif results were saved to CSVs in the previous notebook\n",
    "    # and are NOT already merged into the main database file.\n",
    "    # If they ARE already in your database_path file, you can skip this merge step.\n",
    "\n",
    "    # Use the defined output_summary_dir_phase1 variable\n",
    "    apsi_file_path = (\n",
    "        output_summary_dir_phase1 / \"intra_og_apsi_values.csv\"\n",
    "    )  # Path from previous notebook\n",
    "    motifs_file_path = (\n",
    "        output_summary_dir_phase1 / \"intra_og_conserved_motifs.csv\"\n",
    "    )  # Path from previous notebook\n",
    "\n",
    "    print(\n",
    "        f\"\\nAttempting to merge APSI data from: {apsi_file_path}\"\n",
    "    )  # Added print statement\n",
    "    if apsi_file_path.is_file():\n",
    "        print(\"APSI file found.\")  # Added print statement\n",
    "        try:\n",
    "            df_apsi = pd.read_csv(apsi_file_path)\n",
    "            print(\n",
    "                f\"APSI file read successfully. Shape: {df_apsi.shape}. Columns: {df_apsi.columns.tolist()}\"\n",
    "            )  # Added print statement\n",
    "            # Rename 'Num_Sequences' from APSI file to avoid conflict if needed, or use it\n",
    "            df_apsi.rename(\n",
    "                columns={\"Num_Sequences\": num_og_sequences_col}, inplace=True\n",
    "            )\n",
    "            # Ensure Orthogroup column exists in df_apsi before merging\n",
    "            if orthogroup_col in df_apsi.columns:\n",
    "                # Rename the 'APSI' column in df_apsi to match the desired column name 'Intra_OG_APSI'\n",
    "                if \"APSI\" in df_apsi.columns:\n",
    "                    df_apsi.rename(columns={\"APSI\": apsi_col}, inplace=True)\n",
    "                    print(\n",
    "                        f\"Renamed 'APSI' column to '{apsi_col}' in APSI data.\"\n",
    "                    )  # Added print statement\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Warning: 'APSI' column not found in APSI data. Cannot rename to '{apsi_col}'.\"\n",
    "                    )\n",
    "\n",
    "                # Merge APSI data into df_full based on Orthogroup\n",
    "                # Note: APSI is per OG, so merge will add APSI to all proteins in that OG\n",
    "                # Use a left merge to keep all proteins from df_full\n",
    "                print(\n",
    "                    f\"Merging APSI data on column '{orthogroup_col}'. df_full shape before merge: {df_full.shape}\"\n",
    "                )  # Added print statement\n",
    "                # Select only the columns needed from df_apsi for the merge\n",
    "                cols_to_merge_from_apsi = [\n",
    "                    orthogroup_col,\n",
    "                    apsi_col,\n",
    "                    num_og_sequences_col,\n",
    "                ]\n",
    "                # Filter to include only columns that actually exist in df_apsi after potential renaming\n",
    "                cols_to_merge_from_apsi_existing = [\n",
    "                    col for col in cols_to_merge_from_apsi if col in df_apsi.columns\n",
    "                ]\n",
    "\n",
    "                if cols_to_merge_from_apsi_existing:\n",
    "                    df_full = df_full.merge(\n",
    "                        df_apsi[cols_to_merge_from_apsi_existing],\n",
    "                        on=orthogroup_col,\n",
    "                        how=\"left\",\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"Merged APSI data. df_full shape after merge: {df_full.shape}. Columns: {df_full.columns.tolist()}\"\n",
    "                    )  # Added print statement\n",
    "                else:\n",
    "                    print(\n",
    "                        \"Warning: No relevant columns found in APSI data for merging.\"\n",
    "                    )\n",
    "\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Warning: Orthogroup column '{orthogroup_col}' not found in APSI data. Skipping APSI merge.\"\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to merge APSI data: {e}\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Warning: APSI file not found at '{apsi_file_path}'. APSI data will not be available.\"\n",
    "        )\n",
    "        # Add empty APSI column if not present to prevent downstream errors\n",
    "        if apsi_col not in df_full.columns:\n",
    "            df_full[apsi_col] = np.nan\n",
    "        if num_og_sequences_col not in df_full.columns:\n",
    "            df_full[num_og_sequences_col] = np.nan\n",
    "\n",
    "    print(\n",
    "        f\"\\nAttempting to load and merge Motif data from: {motifs_file_path}\"\n",
    "    )  # Added print statement\n",
    "    if motifs_file_path.is_file():\n",
    "        print(\"Motif file found.\")  # Added print statement\n",
    "        try:\n",
    "            df_motifs = pd.read_csv(motifs_file_path)\n",
    "            print(\n",
    "                f\"Motif file read successfully. Shape: {df_motifs.shape}. Columns: {df_motifs.columns.tolist()}\"\n",
    "            )  # Added print statement\n",
    "            # You can now use df_motifs directly for motif analysis/plotting\n",
    "            print(\n",
    "                f\"Loaded {len(df_motifs)} motifs for {len(df_motifs['Orthogroup'].unique())} orthogroups.\"\n",
    "            )\n",
    "            # Example: Add a flag to df_full if an OG has any conserved motif\n",
    "            if orthogroup_col in df_motifs.columns:\n",
    "                ogs_with_motifs = df_motifs[orthogroup_col].unique()\n",
    "                df_full[\"Has_Conserved_Motif\"] = df_full[orthogroup_col].isin(\n",
    "                    ogs_with_motifs\n",
    "                )\n",
    "                print(\n",
    "                    f\"Added 'Has_Conserved_Motif' flag to df_full. Columns: {df_full.columns.tolist()}\"\n",
    "                )  # Added print statement\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Warning: Orthogroup column '{orthogroup_col}' not found in Motif data. Skipping 'Has_Conserved_Motif' flag.\"\n",
    "                )\n",
    "                if \"Has_Conserved_Motif\" not in df_full.columns:\n",
    "                    df_full[\"Has_Conserved_Motif\"] = False  # Ensure column exists\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load motif data: {e}\")\n",
    "            # Add empty flag column if not present\n",
    "            if \"Has_Conserved_Motif\" not in df_full.columns:\n",
    "                df_full[\"Has_Conserved_Motif\"] = False\n",
    "    else:\n",
    "        print(\n",
    "            f\"Warning: Motif file not found at '{motifs_file_path}'. Motif data will not be available.\"\n",
    "        )\n",
    "        if \"Has_Conserved_Motif\" not in df_full.columns:\n",
    "            df_full[\"Has_Conserved_Motif\"] = False\n",
    "\n",
    "    print(\n",
    "        f\"\\nSuccessfully loaded and prepared data. Final df_full shape: {df_full.shape}. Columns: {df_full.columns.tolist()}\"\n",
    "    )  # Added print statement\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"ERROR: Database file not found at '{database_path}'. Please check the path.\"\n",
    "    )\n",
    "    # Define df_full as empty to prevent downstream errors\n",
    "    df_full = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading or processing the database: {e}\")\n",
    "    # Define df_full as empty\n",
    "    df_full = pd.DataFrame()\n",
    "\n",
    "\n",
    "# --- Create Color Maps for Plotting (after df_full is loaded) ---\n",
    "# Re-create color maps based on the loaded data to ensure categories match\n",
    "print(\"\\n--- Creating Color Maps ---\")\n",
    "# Asgard Phylum Colors\n",
    "asgard_phylum_color_map = {}\n",
    "if asgard_phylum_col in df_full.columns and group_col in df_full.columns:\n",
    "    # Filter for Asgard group before getting unique phyla\n",
    "    asgard_phyla_cat = (\n",
    "        df_full[df_full[group_col] == \"Asgard\"][asgard_phylum_col].dropna().unique()\n",
    "    )\n",
    "    asgard_phyla_cat.sort()\n",
    "    if len(asgard_phyla_cat) > 0:\n",
    "        asgard_phylum_color_map = {\n",
    "            phylum: arcadia_primary_palette[i % len(arcadia_primary_palette)]\n",
    "            for i, phylum in enumerate(asgard_phyla_cat)\n",
    "        }\n",
    "    asgard_phylum_color_map[\"Unknown Phylum\"] = arcadia_colors_manual.get(\n",
    "        \"gray\", \"#bdbdbd\"\n",
    "    )\n",
    "print(f\"Asgard phylum color map created for {len(asgard_phylum_color_map)} phyla.\")\n",
    "\n",
    "# Localization Colors\n",
    "localization_color_map = {}\n",
    "if localization_col in df_full.columns:\n",
    "    # Get all unique localization values, including potential NaNs, from Asgard and GV groups\n",
    "    all_localizations = df_full[df_full[group_col].isin([\"Asgard\", \"GV\"])][\n",
    "        localization_col\n",
    "    ].unique()\n",
    "    # Convert to list, handling NaN explicitly if needed for mapping\n",
    "    all_localizations_list = [\n",
    "        str(loc) if pd.isna(loc) else loc for loc in all_localizations\n",
    "    ]\n",
    "    all_localizations_list.sort()  # Sort for consistent color assignment\n",
    "    localization_assignments = {\n",
    "        \"Archaea: Cytoplasmic/Membrane (non-SP)\": arcadia_colors_manual.get(\n",
    "            \"aegean\", \"#5088C5\"\n",
    "        ),\n",
    "        \"Archaea: Membrane-associated (Lipoprotein/Pilin)\": arcadia_colors_manual.get(\n",
    "            \"amber\", \"#F28360\"\n",
    "        ),\n",
    "        \"Archaea: Secreted/Membrane (Sec/Tat pathway)\": arcadia_colors_manual.get(\n",
    "            \"seaweed\", \"#3B9886\"\n",
    "        ),\n",
    "        \"Host: Cytoplasm/Nucleus/Virus Factory\": arcadia_colors_manual.get(\n",
    "            \"vital\", \"#73B5E3\"\n",
    "        ),\n",
    "        \"Host: Membrane-associated (Lipoprotein/Pilin-like)\": arcadia_colors_manual.get(\n",
    "            \"tangerine\", \"#FFB984\"\n",
    "        ),\n",
    "        \"Host: Secretory Pathway (Secreted/Membrane/Organelle)\": arcadia_colors_manual.get(\n",
    "            \"lime\", \"#97CD78\"\n",
    "        ),\n",
    "        \"CYTOPLASMIC\": arcadia_colors_manual.get(\n",
    "            \"vital\", \"#73B5E3\"\n",
    "        ),  # Include simplified terms if present\n",
    "        \"MEMBRANE\": arcadia_colors_manual.get(\"tangerine\", \"#FFB984\"),\n",
    "        \"EXTRACELLULAR\": arcadia_colors_manual.get(\"lime\", \"#97CD78\"),\n",
    "        \"Unknown\": arcadia_colors_manual.get(\"gray\", \"#EBEDE8\"),\n",
    "        \"nan\": arcadia_colors_manual.get(\"gray\", \"#EBEDE8\"),  # Handle NaN explicitly\n",
    "    }\n",
    "    fallback_palette_loc = (\n",
    "        arcadia_neutrals_palette + arcadia_secondary_palette\n",
    "    )  # Use defined palettes\n",
    "    fallback_idx_loc = 0\n",
    "    for loc in all_localizations_list:\n",
    "        if loc not in localization_color_map:\n",
    "            localization_color_map[loc] = localization_assignments.get(\n",
    "                loc, fallback_palette_loc[fallback_idx_loc % len(fallback_palette_loc)]\n",
    "            )\n",
    "            if loc not in localization_assignments:\n",
    "                fallback_idx_loc += 1\n",
    "print(f\"Localization color map created for {len(localization_color_map)} categories.\")\n",
    "\n",
    "# Broad Functional Category Colors\n",
    "broad_category_color_map = {}\n",
    "if broad_func_cat_col in df_full.columns:\n",
    "    # Get all unique broad categories, including potential NaNs, from Asgard and GV groups\n",
    "    all_broad_categories = df_full[df_full[group_col].isin([\"Asgard\", \"GV\"])][\n",
    "        broad_func_cat_col\n",
    "    ].unique()\n",
    "    # Convert to list, handling NaN explicitly\n",
    "    all_broad_categories_list = [\n",
    "        str(cat) if pd.isna(cat) else cat for cat in all_broad_categories\n",
    "    ]\n",
    "    all_broad_categories_list.sort()  # Sort for consistent color assignment\n",
    "\n",
    "    category_assignments = {\n",
    "        \"Cytoskeleton\": arcadia_colors_manual.get(\"aegean\", \"#5088C5\"),\n",
    "        \"Membrane Trafficking/Vesicles\": arcadia_colors_manual.get(\"amber\", \"#F28360\"),\n",
    "        \"ESCRT/Endosomal Sorting\": arcadia_colors_manual.get(\"seaweed\", \"#3B9886\"),\n",
    "        \"Ubiquitin System\": arcadia_colors_manual.get(\"aster\", \"#7A77AB\"),\n",
    "        \"N-glycosylation\": arcadia_colors_manual.get(\"rose\", \"#F898AE\"),\n",
    "        \"Nuclear Transport/Pore\": arcadia_colors_manual.get(\"vital\", \"#73B5E3\"),\n",
    "        \"DNA Info Processing\": arcadia_colors_manual.get(\"canary\", \"#F7B846\"),\n",
    "        \"RNA Info Processing\": arcadia_colors_manual.get(\"lime\", \"#97CD78\"),\n",
    "        \"Translation\": arcadia_colors_manual.get(\"tangerine\", \"#FFB984\"),\n",
    "        \"Signal Transduction\": arcadia_colors_manual.get(\"aster\", \"#7A77AB\"),\n",
    "        \"Metabolism\": arcadia_colors_manual.get(\n",
    "            \"sky\", \"#C6E7F4\"\n",
    "        ),  # Using sky as per previous code\n",
    "        \"Other Specific Annotation\": arcadia_colors_manual.get(\"denim\", \"#B6C8D4\"),\n",
    "        \"Unknown/Unclassified\": arcadia_colors_manual.get(\"gray\", \"#EBEDE8\"),\n",
    "        \"nan\": arcadia_colors_manual.get(\"gray\", \"#EBEDE8\"),  # Handle NaN explicitly\n",
    "    }\n",
    "    fallback_palette_cat = (\n",
    "        arcadia_primary_palette + arcadia_secondary_palette + arcadia_neutrals_palette\n",
    "    )  # Use defined palettes\n",
    "    fallback_idx_cat = 0\n",
    "    for category in all_broad_categories_list:\n",
    "        if category not in broad_category_color_map:\n",
    "            broad_category_color_map[category] = category_assignments.get(\n",
    "                category,\n",
    "                fallback_palette_cat[fallback_idx_cat % len(fallback_palette_cat)],\n",
    "            )\n",
    "            if category not in category_assignments:\n",
    "                fallback_idx_cat += 1\n",
    "print(\n",
    "    f\"Broad functional category color map created for {len(broad_category_color_map)} categories.\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n--- Figures Notebook Setup Complete ---\")\n",
    "print(\"DataFrame 'df_full' is loaded and prepared for figure generation.\")\n",
    "print(\"Color maps and helper functions are defined.\")\n",
    "print(\"You can now proceed with generating figures in subsequent cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c7873-c3a6-48d2-9d6d-a896f6daf183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is designed to generate plots for database-overview_82.png, the figure in the pub that characterizes the composition of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e22cdf-fd65-431a-932f-4b2d6c6f7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Figure 4 - Genome and Protein Counts by Taxonomy\n",
    "\n",
    "# --- Imports ---\n",
    "# Assumes pandas, numpy, plotly.express, plotly.graph_objects, plotly.io,\n",
    "# and necessary variables/helper functions (like df_full, group_col,\n",
    "# protein_id_col, source_genome_accession_col, asgard_phylum_col,\n",
    "# virus_family_col, arcadia_colors_manual, arcadia_primary_palette,\n",
    "# arcadia_secondary_palette, arcadia_neutrals_palette, output_figure_dir,\n",
    "# plotly_layout_defaults)\n",
    "# are defined in the setup cell (Cell 1).\n",
    "# Assumes df_filtered_groups is available from Cell 2 (Figure 2 generation) and\n",
    "# correctly filters for 'Asgard' and 'GV' groups.\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 4: Genome and Protein Counts by Taxonomy (Cell 6) ---\")\n",
    "\n",
    "# Ensure df_filtered_groups is available and not empty\n",
    "if \"df_filtered_groups\" not in locals() or df_filtered_groups.empty:\n",
    "    print(\n",
    "        \"ERROR: df_filtered_groups not available or is empty. Please run Cell 2 first.\"\n",
    "    )\n",
    "else:\n",
    "    # Define colors for Asgard and GV groups (re-defined for context):\n",
    "    group_colors = {\n",
    "        \"Asgard\": arcadia_colors_manual.get(\"aegean\", \"#5088C5\"),\n",
    "        \"GV\": arcadia_colors_manual.get(\"amber\", \"#F28360\"),\n",
    "    }\n",
    "    print(\n",
    "        f\"\\nUsing Group Colors (for context): Asgard={group_colors.get('Asgard')}, GV={group_colors.get('Amber')}\"\n",
    "    )\n",
    "\n",
    "    # --- Calculate Genome and Protein Counts by Taxonomy ---\n",
    "    print(\"\\n--- Calculating Genome and Protein Counts by Taxonomy ---\")\n",
    "\n",
    "    genome_counts_by_group_taxonomy = []\n",
    "    protein_counts_by_group_taxonomy = []\n",
    "    all_taxonomic_units_present = set()  # Collect all unique units from both datasets\n",
    "\n",
    "    if (\n",
    "        source_genome_accession_col in df_filtered_groups.columns\n",
    "        and group_col in df_filtered_groups.columns\n",
    "        and asgard_phylum_col in df_filtered_groups.columns\n",
    "        and virus_family_col in df_filtered_groups.columns\n",
    "        and protein_id_col in df_filtered_groups.columns\n",
    "    ):\n",
    "        # Process Asgard counts by Phylum\n",
    "        df_asgard = df_filtered_groups[df_filtered_groups[group_col] == \"Asgard\"].copy()\n",
    "        if not df_asgard.empty:\n",
    "            if asgard_phylum_col in df_asgard.columns:\n",
    "                # Genome counts\n",
    "                asgard_genome_counts = (\n",
    "                    df_asgard.groupby([group_col, asgard_phylum_col])[\n",
    "                        source_genome_accession_col\n",
    "                    ]\n",
    "                    .nunique()\n",
    "                    .reset_index(name=\"Count\")\n",
    "                )\n",
    "                asgard_genome_counts.rename(\n",
    "                    columns={asgard_phylum_col: \"Taxonomic_Unit\"}, inplace=True\n",
    "                )\n",
    "                genome_counts_by_group_taxonomy.append(asgard_genome_counts)\n",
    "                all_taxonomic_units_present.update(\n",
    "                    asgard_genome_counts[\"Taxonomic_Unit\"].dropna().unique()\n",
    "                )\n",
    "\n",
    "                # Protein counts\n",
    "                asgard_protein_counts = (\n",
    "                    df_asgard.groupby([group_col, asgard_phylum_col])[protein_id_col]\n",
    "                    .count()\n",
    "                    .reset_index(name=\"Count\")\n",
    "                )\n",
    "                asgard_protein_counts.rename(\n",
    "                    columns={asgard_phylum_col: \"Taxonomic_Unit\"}, inplace=True\n",
    "                )\n",
    "                protein_counts_by_group_taxonomy.append(asgard_protein_counts)\n",
    "                all_taxonomic_units_present.update(\n",
    "                    asgard_protein_counts[\"Taxonomic_Unit\"].dropna().unique()\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Warning: Skipping Asgard counts. Column '{asgard_phylum_col}' missing.\"\n",
    "                )\n",
    "        else:\n",
    "            print(\"Warning: Skipping Asgard counts. Data is empty.\")\n",
    "\n",
    "        # Process GV counts by Family\n",
    "        df_gv = df_filtered_groups[df_filtered_groups[group_col] == \"GV\"].copy()\n",
    "        if not df_gv.empty:\n",
    "            if virus_family_col in df_gv.columns:\n",
    "                # Genome counts\n",
    "                gv_genome_counts = (\n",
    "                    df_gv.groupby([group_col, virus_family_col])[\n",
    "                        source_genome_accession_col\n",
    "                    ]\n",
    "                    .nunique()\n",
    "                    .reset_index(name=\"Count\")\n",
    "                )\n",
    "                gv_genome_counts.rename(\n",
    "                    columns={virus_family_col: \"Taxonomic_Unit\"}, inplace=True\n",
    "                )\n",
    "                genome_counts_by_group_taxonomy.append(gv_genome_counts)\n",
    "                all_taxonomic_units_present.update(\n",
    "                    gv_genome_counts[\"Taxonomic_Unit\"].dropna().unique()\n",
    "                )\n",
    "\n",
    "                # Protein counts\n",
    "                gv_protein_counts = (\n",
    "                    df_gv.groupby([group_col, virus_family_col])[protein_id_col]\n",
    "                    .count()\n",
    "                    .reset_index(name=\"Count\")\n",
    "                )\n",
    "                gv_protein_counts.rename(\n",
    "                    columns={virus_family_col: \"Taxonomic_Unit\"}, inplace=True\n",
    "                )\n",
    "                protein_counts_by_group_taxonomy.append(gv_protein_counts)\n",
    "                all_taxonomic_units_present.update(\n",
    "                    gv_protein_counts[\"Taxonomic_Unit\"].dropna().unique()\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Warning: Skipping GV counts. Column '{virus_family_col}' missing.\"\n",
    "                )\n",
    "        else:\n",
    "            print(\"Warning: Skipping GV counts. Data is empty.\")\n",
    "\n",
    "        # Combine results into DataFrames\n",
    "        df_genome_counts_taxonomy = (\n",
    "            pd.concat(genome_counts_by_group_taxonomy, ignore_index=True)\n",
    "            if genome_counts_by_group_taxonomy\n",
    "            else pd.DataFrame()\n",
    "        )\n",
    "        df_protein_counts_taxonomy = (\n",
    "            pd.concat(protein_counts_by_group_taxonomy, ignore_index=True)\n",
    "            if protein_counts_by_group_taxonomy\n",
    "            else pd.DataFrame()\n",
    "        )\n",
    "\n",
    "        print(\"\\nGenome Counts by Group and Taxonomy (Partial):\")\n",
    "        print(df_genome_counts_taxonomy.to_markdown(index=False))\n",
    "        print(\"\\nProtein Counts by Group and Taxonomy (Partial):\")\n",
    "        print(df_protein_counts_taxonomy.to_markdown(index=False))\n",
    "\n",
    "        # --- Create Comprehensive Taxonomic Unit Color Map ---\n",
    "        # Use a combination of Arcadia palettes for the color map\n",
    "        # Prioritize primary, then secondary, then neutrals/monochromatic shades\n",
    "        # Use a larger pool of colors to minimize repetition\n",
    "        full_arcadia_palette = (\n",
    "            arcadia_primary_palette\n",
    "            + arcadia_secondary_palette\n",
    "            + arcadia_neutrals_palette\n",
    "            + [\n",
    "                arcadia_colors_manual.get(\"lapis\", \"#2B66A2\"),\n",
    "                arcadia_colors_manual.get(\"dusk\", \"#094468\"),  # Blue shades\n",
    "                arcadia_colors_manual.get(\"melon\", \"#FFCFAF\"),\n",
    "                arcadia_colors_manual.get(\"cinnabar\", \"#9E3F41\"),  # Orange/Red shades\n",
    "                arcadia_colors_manual.get(\"sun\", \"#FFD364\"),\n",
    "                arcadia_colors_manual.get(\"mustard\", \"#D68D22\"),  # Yellow shades\n",
    "                arcadia_colors_manual.get(\"iris\", \"#DCDFEF\"),\n",
    "                arcadia_colors_manual.get(\"tanzanite\", \"#54448C\"),  # Purple shades\n",
    "                arcadia_colors_manual.get(\"glass\", \"#C3E2DB\"),\n",
    "                arcadia_colors_manual.get(\"asparagus\", \"#2A6B5E\"),  # Teal shades\n",
    "                arcadia_colors_manual.get(\"putty\", \"#FFE3D4\"),\n",
    "                arcadia_colors_manual.get(\"candy\", \"#E2718F\"),  # Pink shades\n",
    "                arcadia_colors_manual.get(\"stone\", \"#EDE6DA\"),\n",
    "                arcadia_colors_manual.get(\"dove\", \"#CAD4DB\"),  # Warm/Cool gray shades\n",
    "                arcadia_colors_manual.get(\"steel\", \"#687787\"),\n",
    "                arcadia_colors_manual.get(\"mud\", \"#635C5A\"),  # More neutrals\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Assign colors to ALL unique taxonomic units found in either dataset\n",
    "        combined_taxonomy_color_map = {}\n",
    "        sorted_all_taxonomic_units = sorted(\n",
    "            list(all_taxonomic_units_present)\n",
    "        )  # Sort for consistent assignment\n",
    "        for i, unit in enumerate(sorted_all_taxonomic_units):\n",
    "            combined_taxonomy_color_map[unit] = full_arcadia_palette[\n",
    "                i % len(full_arcadia_palette)\n",
    "            ]\n",
    "\n",
    "        # Add 'Unknown Phylum'/'Unknown Family' if they exist and weren't in the unique list\n",
    "        if \"Unknown Phylum\" not in combined_taxonomy_color_map:\n",
    "            combined_taxonomy_color_map[\"Unknown Phylum\"] = arcadia_colors_manual.get(\n",
    "                \"gray\", \"#bdbdbd\"\n",
    "            )\n",
    "        if \"Unknown Family\" not in combined_taxonomy_color_map:\n",
    "            combined_taxonomy_color_map[\"Unknown Family\"] = arcadia_colors_manual.get(\n",
    "                \"gray\", \"#bdbdbd\"\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            f\"\\nCombined taxonomic unit color map created for {len(combined_taxonomy_color_map)} units.\"\n",
    "        )\n",
    "\n",
    "        # --- Figure 4A: Number of Genomes by Taxonomy (Stacked Bar and Pie Charts) ---\n",
    "        print(\"\\n--- Figure 4A: Number of Genomes by Taxonomy ---\")\n",
    "\n",
    "        if not df_genome_counts_taxonomy.empty:\n",
    "            # --- Figure 4A Part 1: Stacked Bar Chart ---\n",
    "            # Calculate total count for each Taxonomic_Unit across both groups for sorting\n",
    "            taxonomic_unit_total_counts_genomes = (\n",
    "                df_genome_counts_taxonomy.groupby(\"Taxonomic_Unit\")[\"Count\"]\n",
    "                .sum()\n",
    "                .sort_values(ascending=False)\n",
    "            )\n",
    "            # Get the order of taxonomic units based on total counts\n",
    "            taxonomic_unit_order_genomes = (\n",
    "                taxonomic_unit_total_counts_genomes.index.tolist()\n",
    "            )\n",
    "\n",
    "            fig_4a_bar = px.bar(\n",
    "                df_genome_counts_taxonomy,\n",
    "                x=group_col,  # X-axis is Group\n",
    "                y=\"Count\",\n",
    "                color=\"Taxonomic_Unit\",  # Color is Phylum or Family\n",
    "                barmode=\"stack\",  # Stack bars by Taxonomic_Unit\n",
    "                # title='Number of Genomes by Group and Taxonomy (Stacked Bar)', # No title\n",
    "                labels={\n",
    "                    group_col: \"Group\",\n",
    "                    \"Count\": \"Number of Genomes\",\n",
    "                    \"Taxonomic_Unit\": \"Phylum or Family\",\n",
    "                },\n",
    "                color_discrete_map=combined_taxonomy_color_map,  # Use combined color map\n",
    "                category_orders={\n",
    "                    group_col: [\"Asgard\", \"GV\"],\n",
    "                    \"Taxonomic_Unit\": taxonomic_unit_order_genomes,\n",
    "                },  # Ensure Group order and Taxonomic Unit order\n",
    "                template=\"plotly_white\",\n",
    "            )\n",
    "            # Apply default layout and remove gridlines\n",
    "            fig_4a_bar.update_layout(plotly_layout_defaults)\n",
    "            fig_4a_bar.update_xaxes(title_text=\"Group\", showgrid=False)\n",
    "            fig_4a_bar.update_yaxes(title_text=\"Number of Genomes\", showgrid=False)\n",
    "            fig_4a_bar.update_layout(\n",
    "                legend_title_text=\"Taxonomic Unit\"\n",
    "            )  # Add legend title\n",
    "\n",
    "            # Export Figure 4A Part 1 (HTML, PDF, SVG)\n",
    "            fig_4a_bar_path_html = (\n",
    "                Path(output_figure_dir) / \"figure4a_genomes_by_taxonomy_stackedbar.html\"\n",
    "            )\n",
    "            fig_4a_bar.write_html(str(fig_4a_bar_path_html))\n",
    "            print(f\"Figure 4A (Stacked Bar) HTML saved to: {fig_4a_bar_path_html}\")\n",
    "\n",
    "            # Export to PDF/SVG (requires kaleido)\n",
    "            try:\n",
    "                fig_4a_bar_path_pdf = (\n",
    "                    Path(output_figure_dir)\n",
    "                    / \"figure4a_genomes_by_taxonomy_stackedbar.pdf\"\n",
    "                )\n",
    "                fig_4a_bar.write_image(str(fig_4a_bar_path_pdf))\n",
    "                print(f\"Figure 4A (Stacked Bar) PDF saved to: {fig_4a_bar_path_pdf}\")\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Warning: Could not export Figure 4A (Stacked Bar) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\"\n",
    "                )\n",
    "            try:\n",
    "                fig_4a_bar_path_svg = (\n",
    "                    Path(output_figure_dir)\n",
    "                    / \"figure4a_genomes_by_taxonomy_stackedbar.svg\"\n",
    "                )\n",
    "                fig_4a_bar.write_image(str(fig_4a_bar_path_svg))\n",
    "                print(f\"Figure 4A (Stacked Bar) SVG saved to: {fig_4a_bar_path_svg}\")\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Warning: Could not export Figure 4A (Stacked Bar) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\"\n",
    "                )\n",
    "\n",
    "            # --- Figure 4A Part 2: Pie Charts (Asgard and GV) ---\n",
    "            print(\n",
    "                \"\\n--- Figure 4A Part 2: Genome Counts Pie Charts (Asgard and GV) ---\"\n",
    "            )\n",
    "\n",
    "            # Asgard Genome Pie Chart\n",
    "            df_asgard_genome_pie = df_genome_counts_taxonomy[\n",
    "                df_genome_counts_taxonomy[group_col] == \"Asgard\"\n",
    "            ].copy()\n",
    "            if not df_asgard_genome_pie.empty:\n",
    "                # Sort Asgard phyla by count for pie chart order\n",
    "                df_asgard_genome_pie = df_asgard_genome_pie.sort_values(\n",
    "                    \"Count\", ascending=False\n",
    "                ).copy()\n",
    "                fig_4a_pie_asgard = go.Figure(\n",
    "                    data=[\n",
    "                        go.Pie(\n",
    "                            labels=df_asgard_genome_pie[\"Taxonomic_Unit\"],\n",
    "                            values=df_asgard_genome_pie[\"Count\"],\n",
    "                            hole=0.3,  # Creates a donut chart\n",
    "                            marker=dict(\n",
    "                                colors=[\n",
    "                                    combined_taxonomy_color_map.get(unit, \"gray\")\n",
    "                                    for unit in df_asgard_genome_pie[\"Taxonomic_Unit\"]\n",
    "                                ]\n",
    "                            ),  # Apply colors\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "                # fig_4a_pie_asgard.update_layout(title_text='Asgard Genome Counts by Phylum') # No title\n",
    "                fig_4a_pie_asgard.update_layout(\n",
    "                    plotly_layout_defaults\n",
    "                )  # Apply defaults (no title, etc.)\n",
    "                fig_4a_pie_asgard.update_layout(\n",
    "                    margin=dict(t=0, b=0, l=0, r=0)\n",
    "                )  # Adjust margins\n",
    "                fig_4a_pie_asgard.update_traces(\n",
    "                    textinfo=\"percent+label\", insidetextorientation=\"radial\"\n",
    "                )\n",
    "\n",
    "                # Export Figure 4A Pie Asgard\n",
    "                fig_4a_pie_asgard_path_html = (\n",
    "                    Path(output_figure_dir)\n",
    "                    / \"figure4a_genomes_by_phylum_pie_asgard.html\"\n",
    "                )\n",
    "                fig_4a_pie_asgard.write_html(str(fig_4a_pie_asgard_path_html))\n",
    "                print(\n",
    "                    f\"Figure 4A (Asgard Pie) HTML saved to: {fig_4a_pie_asgard_path_html}\"\n",
    "                )\n",
    "                try:\n",
    "                    fig_4a_pie_asgard_path_pdf = (\n",
    "                        Path(output_figure_dir)\n",
    "                        / \"figure4a_genomes_by_phylum_pie_asgard.pdf\"\n",
    "                    )\n",
    "                    fig_4a_pie_asgard.write_image(str(fig_4a_pie_asgard_path_pdf))\n",
    "                    print(\n",
    "                        f\"Figure 4A (Asgard Pie) PDF saved to: {fig_4a_pie_asgard_path_pdf}\"\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Warning: Could not export Figure 4A (Asgard Pie) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\"\n",
    "                    )\n",
    "                try:\n",
    "                    fig_4a_pie_asgard_path_svg = (\n",
    "                        Path(output_figure_dir)\n",
    "                        / \"figure4a_genomes_by_phylum_pie_asgard.svg\"\n",
    "                    )\n",
    "                    fig_4a_pie_asgard.write_image(str(fig_4a_pie_asgard_path_svg))\n",
    "                    print(\n",
    "                        f\"Figure 4A (Asgard Pie) SVG saved to: {fig_4a_pie_asgard_path_svg}\"\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Warning: Could not export Figure 4A (Asgard Pie) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\"\n",
    "                    )\n",
    "\n",
    "            else:\n",
    "                print(\"No Asgard genome data for pie chart.\")\n",
    "\n",
    "            # GV Genome Pie Chart\n",
    "            df_gv_genome_pie = df_genome_counts_taxonomy[\n",
    "                df_genome_counts_taxonomy[group_col] == \"GV\"\n",
    "            ].copy()\n",
    "            if not df_gv_genome_pie.empty:\n",
    "                # Sort GV families by count for pie chart order\n",
    "                df_gv_genome_pie = df_gv_genome_pie.sort_values(\n",
    "                    \"Count\", ascending=False\n",
    "                ).copy()\n",
    "                fig_4a_pie_gv = go.Figure(\n",
    "                    data=[\n",
    "                        go.Pie(\n",
    "                            labels=df_gv_genome_pie[\"Taxonomic_Unit\"],\n",
    "                            values=df_gv_genome_pie[\"Count\"],\n",
    "                            hole=0.3,  # Creates a donut chart\n",
    "                            marker=dict(\n",
    "                                colors=[\n",
    "                                    combined_taxonomy_color_map.get(unit, \"gray\")\n",
    "                                    for unit in df_gv_genome_pie[\"Taxonomic_Unit\"]\n",
    "                                ]\n",
    "                            ),  # Apply colors\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "                # fig_4a_pie_gv.update_layout(title_text='GV Genome Counts by Family') # No title\n",
    "                fig_4a_pie_gv.update_layout(\n",
    "                    plotly_layout_defaults\n",
    "                )  # Apply defaults (no title, etc.)\n",
    "                fig_4a_pie_gv.update_layout(\n",
    "                    margin=dict(t=0, b=0, l=0, r=0)\n",
    "                )  # Adjust margins\n",
    "                fig_4a_pie_gv.update_traces(\n",
    "                    textinfo=\"percent+label\", insidetextorientation=\"radial\"\n",
    "                )\n",
    "\n",
    "                # Export Figure 4A Pie GV\n",
    "                fig_4a_pie_gv_path_html = (\n",
    "                    Path(output_figure_dir) / \"figure4a_genomes_by_family_pie_gv.html\"\n",
    "                )\n",
    "                fig_4a_pie_gv.write_html(str(fig_4a_pie_gv_path_html))\n",
    "                print(f\"Figure 4A (GV Pie) HTML saved to: {fig_4a_pie_gv_path_html}\")\n",
    "                try:\n",
    "                    fig_4a_pie_gv_path_pdf = (\n",
    "                        Path(output_figure_dir)\n",
    "                        / \"figure4a_genomes_by_family_pie_gv.pdf\"\n",
    "                    )\n",
    "                    fig_4a_pie_gv.write_image(str(fig_4a_pie_gv_path_pdf))\n",
    "                    print(f\"Figure 4A (GV Pie) PDF saved to: {fig_4a_pie_gv_path_pdf}\")\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Warning: Could not export Figure 4A (GV Pie) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\"\n",
    "                    )\n",
    "                try:\n",
    "                    fig_4a_pie_gv_path_svg = (\n",
    "                        Path(output_figure_dir)\n",
    "                        / \"figure4a_genomes_by_family_pie_gv.svg\"\n",
    "                    )\n",
    "                    fig_4a_pie_gv.write_image(str(fig_4a_pie_gv_path_svg))\n",
    "                    print(f\"Figure 4A (GV Pie) SVG saved to: {fig_4a_pie_gv_path_svg}\")\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Warning: Could not export Figure 4A (GV Pie) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\"\n",
    "                    )\n",
    "\n",
    "            else:\n",
    "                print(\"No GV genome data for pie chart.\")\n",
    "\n",
    "        else:\n",
    "            print(\"No genome counts by taxonomy available for plotting Figure 4A.\")\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"Skipping Figure 4A due to missing columns in filtered data: '{source_genome_accession_col}', '{group_col}', '{asgard_phylum_col}', or '{virus_family_col}'.\"\n",
    "        )\n",
    "\n",
    "    # --- Figure 4B: Number of Proteins by Taxonomy (Stacked Bar and Pie Charts) ---\n",
    "    print(\"\\n--- Figure 4B: Number of Proteins by Taxonomy ---\")\n",
    "\n",
    "    if (\n",
    "        protein_id_col in df_filtered_groups.columns\n",
    "        and group_col in df_filtered_groups.columns\n",
    "        and asgard_phylum_col in df_filtered_groups.columns\n",
    "        and virus_family_col in df_filtered_groups.columns\n",
    "    ):\n",
    "        # Calculate protein counts by Group and then by Phylum/Family\n",
    "        protein_counts_by_group_taxonomy = []\n",
    "\n",
    "        # Process Asgard proteins by Phylum\n",
    "        df_asgard_proteins = df_filtered_groups[\n",
    "            df_filtered_groups[group_col] == \"Asgard\"\n",
    "        ].copy()\n",
    "        if (\n",
    "            not df_asgard_proteins.empty\n",
    "            and asgard_phylum_col in df_asgard_proteins.columns\n",
    "        ):\n",
    "            asgard_protein_counts = (\n",
    "                df_asgard_proteins.groupby([group_col, asgard_phylum_col])[\n",
    "                    protein_id_col\n",
    "                ]\n",
    "                .count()\n",
    "                .reset_index(name=\"Count\")\n",
    "            )\n",
    "            asgard_protein_counts.rename(\n",
    "                columns={asgard_phylum_col: \"Taxonomic_Unit\"}, inplace=True\n",
    "            )\n",
    "            protein_counts_by_group_taxonomy.append(asgard_protein_counts)\n",
    "        else:\n",
    "            print(\n",
    "                f\"Warning: Skipping Asgard protein counts by Phylum. Data empty or column '{asgard_phylum_col}' missing.\"\n",
    "            )\n",
    "\n",
    "        # Process GV proteins by Family\n",
    "        df_gv_proteins = df_filtered_groups[\n",
    "            df_filtered_groups[group_col] == \"GV\"\n",
    "        ].copy()\n",
    "        if not df_gv_proteins.empty and virus_family_col in df_gv_proteins.columns:\n",
    "            gv_protein_counts = (\n",
    "                df_gv_proteins.groupby([group_col, virus_family_col])[protein_id_col]\n",
    "                .count()\n",
    "                .reset_index(name=\"Count\")\n",
    "            )\n",
    "            gv_protein_counts.rename(\n",
    "                columns={virus_family_col: \"Taxonomic_Unit\"}, inplace=True\n",
    "            )\n",
    "            protein_counts_by_group_taxonomy.append(gv_protein_counts)\n",
    "        else:\n",
    "            print(\n",
    "                f\"Warning: Skipping GV protein counts by Family. Data empty or column '{virus_family_col}' missing.\"\n",
    "            )\n",
    "\n",
    "        if protein_counts_by_group_taxonomy:\n",
    "            df_protein_counts_taxonomy = pd.concat(\n",
    "                protein_counts_by_group_taxonomy, ignore_index=True\n",
    "            )\n",
    "            print(\"\\nProtein Counts by Group and Taxonomy:\")\n",
    "            print(df_protein_counts_taxonomy.to_markdown(index=False))\n",
    "\n",
    "            # Combine color maps for plotting (same as for genomes)\n",
    "            # This is already done above based on ALL unique units from both datasets\n",
    "\n",
    "            # --- Figure 4B Part 1: Stacked Bar Chart ---\n",
    "            # Calculate total count for each Taxonomic_Unit across both groups for sorting\n",
    "            taxonomic_unit_total_counts = (\n",
    "                df_protein_counts_taxonomy.groupby(\"Taxonomic_Unit\")[\"Count\"]\n",
    "                .sum()\n",
    "                .sort_values(ascending=False)\n",
    "            )\n",
    "            # Get the order of taxonomic units based on total counts\n",
    "            taxonomic_unit_order = taxonomic_unit_total_counts.index.tolist()\n",
    "\n",
    "            fig_4b_bar = px.bar(\n",
    "                df_protein_counts_taxonomy,\n",
    "                x=group_col,  # X-axis is Group\n",
    "                y=\"Count\",\n",
    "                color=\"Taxonomic_Unit\",  # Color is Phylum or Family\n",
    "                barmode=\"stack\",  # Stack bars by Taxonomic_Unit\n",
    "                # title='Number of Proteins by Group and Taxonomy (Stacked Bar)', # No title\n",
    "                labels={\n",
    "                    group_col: \"Group\",\n",
    "                    \"Count\": \"Number of Proteins\",\n",
    "                    \"Taxonomic_Unit\": \"Phylum or Family\",\n",
    "                },\n",
    "                color_discrete_map=combined_taxonomy_color_map,  # Use combined color map\n",
    "                category_orders={\n",
    "                    group_col: [\"Asgard\", \"GV\"],\n",
    "                    \"Taxonomic_Unit\": taxonomic_unit_order,\n",
    "                },  # Ensure Group order and Taxonomic Unit order\n",
    "                template=\"plotly_white\",\n",
    "            )\n",
    "            # Apply default layout and remove gridlines\n",
    "            fig_4b_bar.update_layout(plotly_layout_defaults)\n",
    "            fig_4b_bar.update_xaxes(title_text=\"Group\", showgrid=False)\n",
    "            fig_4b_bar.update_yaxes(title_text=\"Number of Proteins\", showgrid=False)\n",
    "            fig_4b_bar.update_layout(\n",
    "                legend_title_text=\"Taxonomic Unit\"\n",
    "            )  # Add legend title\n",
    "\n",
    "            # Export Figure 4B Part 1 (HTML, PDF, SVG)\n",
    "            fig_4b_bar_path_html = (\n",
    "                Path(output_figure_dir)\n",
    "                / \"figure4b_proteins_by_taxonomy_stackedbar.html\"\n",
    "            )\n",
    "            fig_4b_bar.write_html(str(fig_4b_bar_path_html))\n",
    "            print(f\"Figure 4B (Stacked Bar) HTML saved to: {fig_4b_bar_path_html}\")\n",
    "\n",
    "            # Export to PDF/SVG (requires kaleido)\n",
    "            try:\n",
    "                fig_4b_bar_path_pdf = (\n",
    "                    Path(output_figure_dir)\n",
    "                    / \"figure4b_proteins_by_taxonomy_stackedbar.pdf\"\n",
    "                )\n",
    "                fig_4b_bar.write_image(str(fig_4b_bar_path_pdf))\n",
    "                print(f\"Figure 4B (Stacked Bar) PDF saved to: {fig_4b_bar_path_pdf}\")\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Warning: Could not export Figure 4B (Stacked Bar) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\"\n",
    "                )\n",
    "            try:\n",
    "                fig_4b_bar_path_svg = (\n",
    "                    Path(output_figure_dir)\n",
    "                    / \"figure4b_proteins_by_taxonomy_stackedbar.svg\"\n",
    "                )\n",
    "                fig_4b_bar.write_image(str(fig_4b_bar_path_svg))\n",
    "                print(f\"Figure 4B (Stacked Bar) SVG saved to: {fig_4b_bar_path_svg}\")\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Warning: Could not export Figure 4B (Stacked Bar) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\"\n",
    "                )\n",
    "\n",
    "            # --- Figure 4B Part 2: Pie Charts (Asgard and GV) ---\n",
    "            print(\n",
    "                \"\\n--- Figure 4B Part 2: Protein Counts Pie Charts (Asgard and GV) ---\"\n",
    "            )\n",
    "\n",
    "            # Asgard Protein Pie Chart\n",
    "            df_asgard_protein_pie = df_protein_counts_taxonomy[\n",
    "                df_protein_counts_taxonomy[group_col] == \"Asgard\"\n",
    "            ].copy()\n",
    "            if not df_asgard_protein_pie.empty:\n",
    "                # Sort Asgard phyla by count for pie chart order\n",
    "                df_asgard_protein_pie = df_asgard_protein_pie.sort_values(\n",
    "                    \"Count\", ascending=False\n",
    "                ).copy()\n",
    "                fig_4b_pie_asgard = go.Figure(\n",
    "                    data=[\n",
    "                        go.Pie(\n",
    "                            labels=df_asgard_protein_pie[\"Taxonomic_Unit\"],\n",
    "                            values=df_asgard_protein_pie[\"Count\"],\n",
    "                            hole=0.3,  # Creates a donut chart\n",
    "                            marker=dict(\n",
    "                                colors=[\n",
    "                                    combined_taxonomy_color_map.get(unit, \"gray\")\n",
    "                                    for unit in df_asgard_protein_pie[\"Taxonomic_Unit\"]\n",
    "                                ]\n",
    "                            ),  # Apply colors\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "                # fig_4b_pie_asgard.update_layout(title_text='Asgard Protein Counts by Phylum') # No title\n",
    "                fig_4b_pie_asgard.update_layout(\n",
    "                    plotly_layout_defaults\n",
    "                )  # Apply defaults (no title, etc.)\n",
    "                fig_4b_pie_asgard.update_layout(\n",
    "                    margin=dict(t=0, b=0, l=0, r=0)\n",
    "                )  # Adjust margins\n",
    "                fig_4b_pie_asgard.update_traces(\n",
    "                    textinfo=\"percent+label\", insidetextorientation=\"radial\"\n",
    "                )\n",
    "\n",
    "                # Export Figure 4B Pie Asgard\n",
    "                fig_4b_pie_asgard_path_html = (\n",
    "                    Path(output_figure_dir)\n",
    "                    / \"figure4b_proteins_by_phylum_pie_asgard.html\"\n",
    "                )\n",
    "                fig_4b_pie_asgard.write_html(str(fig_4b_pie_asgard_path_html))\n",
    "                print(\n",
    "                    f\"Figure 4B (Asgard Pie) HTML saved to: {fig_4b_pie_asgard_path_html}\"\n",
    "                )\n",
    "                try:\n",
    "                    fig_4b_pie_asgard_path_pdf = (\n",
    "                        Path(output_figure_dir)\n",
    "                        / \"figure4b_proteins_by_phylum_pie_asgard.pdf\"\n",
    "                    )\n",
    "                    fig_4b_pie_asgard.write_image(str(fig_4b_pie_asgard_path_pdf))\n",
    "                    print(\n",
    "                        f\"Figure 4B (Asgard Pie) PDF saved to: {fig_4b_pie_asgard_path_pdf}\"\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Warning: Could not export Figure 4B (Asgard Pie) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\"\n",
    "                    )\n",
    "                try:\n",
    "                    fig_4b_pie_asgard_path_svg = (\n",
    "                        Path(output_figure_dir)\n",
    "                        / \"figure4b_proteins_by_phylum_pie_asgard.svg\"\n",
    "                    )\n",
    "                    fig_4b_pie_asgard.write_image(str(fig_4b_pie_asgard_path_svg))\n",
    "                    print(\n",
    "                        f\"Figure 4B (Asgard Pie) SVG saved to: {fig_4b_pie_asgard_path_svg}\"\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Warning: Could not export Figure 4B (Asgard Pie) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\"\n",
    "                    )\n",
    "\n",
    "            else:\n",
    "                print(\"No Asgard protein data for pie chart.\")\n",
    "\n",
    "            # GV Protein Pie Chart\n",
    "            df_gv_protein_pie = df_protein_counts_taxonomy[\n",
    "                df_protein_counts_taxonomy[group_col] == \"GV\"\n",
    "            ].copy()\n",
    "            if not df_gv_protein_pie.empty:\n",
    "                # Sort GV families by count for pie chart order\n",
    "                df_gv_protein_pie = df_gv_protein_pie.sort_values(\n",
    "                    \"Count\", ascending=False\n",
    "                ).copy()\n",
    "                fig_4b_pie_gv = go.Figure(\n",
    "                    data=[\n",
    "                        go.Pie(\n",
    "                            labels=df_gv_protein_pie[\"Taxonomic_Unit\"],\n",
    "                            values=df_gv_protein_pie[\"Count\"],\n",
    "                            hole=0.3,  # Creates a donut chart\n",
    "                            marker=dict(\n",
    "                                colors=[\n",
    "                                    combined_taxonomy_color_map.get(unit, \"gray\")\n",
    "                                    for unit in df_gv_protein_pie[\"Taxonomic_Unit\"]\n",
    "                                ]\n",
    "                            ),  # Apply colors\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "                # fig_4b_pie_gv.update_layout(title_text='GV Protein Counts by Family') # No title\n",
    "                fig_4b_pie_gv.update_layout(\n",
    "                    plotly_layout_defaults\n",
    "                )  # Apply defaults (no title, etc.)\n",
    "                fig_4b_pie_gv.update_layout(\n",
    "                    margin=dict(t=0, b=0, l=0, r=0)\n",
    "                )  # Adjust margins\n",
    "                fig_4b_pie_gv.update_traces(\n",
    "                    textinfo=\"percent+label\", insidetextorientation=\"radial\"\n",
    "                )\n",
    "\n",
    "                # Export Figure 4B Pie GV\n",
    "                fig_4b_pie_gv_path_html = (\n",
    "                    Path(output_figure_dir) / \"figure4b_proteins_by_family_pie_gv.html\"\n",
    "                )\n",
    "                fig_4b_pie_gv.write_html(str(fig_4b_pie_gv_path_html))\n",
    "                print(f\"Figure 4B (GV Pie) HTML saved to: {fig_4b_pie_gv_path_html}\")\n",
    "                try:\n",
    "                    fig_4b_pie_gv_path_pdf = (\n",
    "                        Path(output_figure_dir)\n",
    "                        / \"figure4b_proteins_by_family_pie_gv.pdf\"\n",
    "                    )\n",
    "                    fig_4b_pie_gv.write_image(str(fig_4b_pie_gv_path_pdf))\n",
    "                    print(f\"Figure 4B (GV Pie) PDF saved to: {fig_4b_pie_gv_path_pdf}\")\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Warning: Could not export Figure 4B (GV Pie) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\"\n",
    "                    )\n",
    "                try:\n",
    "                    fig_4b_pie_gv_path_svg = (\n",
    "                        Path(output_figure_dir)\n",
    "                        / \"figure4b_proteins_by_family_pie_gv.svg\"\n",
    "                    )\n",
    "                    fig_4b_pie_gv.write_image(str(fig_4b_pie_gv_path_svg))\n",
    "                    print(f\"Figure 4B (GV Pie) SVG saved to: {fig_4b_pie_gv_path_svg}\")\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Warning: Could not export Figure 4B (GV Pie) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\"\n",
    "                    )\n",
    "\n",
    "            else:\n",
    "                print(\"No GV protein data for pie chart.\")\n",
    "\n",
    "        else:\n",
    "            print(\"No protein counts by taxonomy available for plotting Figure 4B.\")\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"Skipping Figure 4B due to missing columns in filtered data: '{protein_id_col}', '{group_col}', '{asgard_phylum_col}', or '{virus_family_col}'.\"\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Figure 4 Generation Complete ---\")\n",
    "print(f\"Figures saved to the '{output_figure_dir}' directory (HTML, PDF, SVG).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e68698-6e2f-4bdc-b8e4-d0dc1116ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell generates the plots for localization-function_100.png, which describes the subcellular localization and functional categorization of the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707423e5-28f2-40ac-80ae-da2662119861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Plots for localization-function_100.png - Localization and Functional Category Distribution\n",
    "\n",
    "# --- Imports ---\n",
    "# Assumes pandas, numpy, plotly.express, plotly.graph_objects, plotly.io,\n",
    "# and necessary variables/helper functions (like df_full, group_col,\n",
    "# protein_id_col, localization_col, broad_func_cat_col,\n",
    "# arcadia_colors_manual, arcadia_primary_palette, arcadia_secondary_palette,\n",
    "# arcadia_neutrals_palette, output_figure_dir, output_figure_summary_dir,\n",
    "# plotly_layout_defaults, localization_color_map, broad_category_color_map)\n",
    "# are defined in the setup cell (Cell 1).\n",
    "# Assumes df_filtered_groups is available from Cell 2 (Figure 2 generation) and\n",
    "# correctly filters for 'Asgard' and 'GV' groups.\n",
    "\n",
    "print(\n",
    "    \"\\n\\n--- Generating Figure 3: Localization and Functional Category Distribution (Cell 5) ---\"\n",
    ")\n",
    "\n",
    "# Ensure df_filtered_groups is available and not empty\n",
    "if \"df_filtered_groups\" not in locals() or df_filtered_groups.empty:\n",
    "    print(\n",
    "        \"ERROR: df_filtered_groups not available or is empty. Please run Cell 2 first.\"\n",
    "    )\n",
    "else:\n",
    "    # Define colors for Asgard and GV using the specified hex codes (re-defined for clarity in this cell)\n",
    "    group_colors = {\n",
    "        \"Asgard\": arcadia_colors_manual.get(\"aegean\", \"#5088C5\"),\n",
    "        \"GV\": arcadia_colors_manual.get(\"amber\", \"#F28360\"),\n",
    "    }\n",
    "    print(\n",
    "        f\"\\nUsing Group Colors: Asgard={group_colors.get('Asgard')}, GV={group_colors.get('GV')}\"\n",
    "    )\n",
    "\n",
    "    # --- Figure 3A: Predicted Subcellular Localization Distribution ---\n",
    "    print(\"\\n--- Figure 3A: Predicted Subcellular Localization Distribution ---\")\n",
    "\n",
    "    if (\n",
    "        localization_col in df_filtered_groups.columns\n",
    "        and group_col in df_filtered_groups.columns\n",
    "    ):\n",
    "        # Calculate counts for each localization category by group\n",
    "        localization_counts = (\n",
    "            df_filtered_groups.groupby(group_col)[localization_col]\n",
    "            .value_counts()\n",
    "            .reset_index(name=\"Count\")\n",
    "        )\n",
    "\n",
    "        # Calculate percentages for plotting\n",
    "        localization_percentages = (\n",
    "            df_filtered_groups.groupby(group_col)[localization_col]\n",
    "            .value_counts(normalize=True)\n",
    "            .mul(100)\n",
    "            .reset_index(name=\"Percentage\")\n",
    "        )\n",
    "\n",
    "        # Merge counts and percentages\n",
    "        df_localization_summary = pd.merge(\n",
    "            localization_counts,\n",
    "            localization_percentages,\n",
    "            on=[group_col, localization_col],\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"\\nPredicted Subcellular Localization Distribution (Counts and Percentages):\"\n",
    "        )\n",
    "        print(\n",
    "            df_localization_summary.sort_values(\n",
    "                [\"Group\", \"Count\"], ascending=[True, False]\n",
    "            ).to_markdown(index=False)\n",
    "        )\n",
    "\n",
    "        # Save full localization summary to a summary file\n",
    "        full_localization_summary_path = (\n",
    "            Path(output_figure_summary_dir) / \"figure3a_localization_full_summary.csv\"\n",
    "        )\n",
    "        try:\n",
    "            df_localization_summary.to_csv(\n",
    "                str(full_localization_summary_path), index=False\n",
    "            )\n",
    "            print(\n",
    "                f\"Full localization summary saved to: {full_localization_summary_path}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving full localization summary: {e}\")\n",
    "\n",
    "        # Define category order based on overall frequency for consistent plotting\n",
    "        overall_localization_order = (\n",
    "            df_filtered_groups[localization_col].value_counts().index.tolist()\n",
    "        )\n",
    "\n",
    "        if not df_localization_summary.empty:\n",
    "            fig_3a = px.bar(\n",
    "                df_localization_summary,\n",
    "                x=group_col,  # X-axis is Group\n",
    "                y=\"Percentage\",\n",
    "                color=localization_col,  # Color is Localization Category\n",
    "                barmode=\"group\",  # Group bars by X-axis value (Group)\n",
    "                # title='Predicted Subcellular Localization Distribution by Group', # No title\n",
    "                labels={\n",
    "                    localization_col: \"Predicted Subcellular Localization\",\n",
    "                    \"Percentage\": \"Percentage of Proteins (%)\",\n",
    "                    group_col: \"Group\",\n",
    "                },\n",
    "                color_discrete_map=localization_color_map,  # Use localization color map\n",
    "                category_orders={\n",
    "                    group_col: [\"Asgard\", \"GV\"],\n",
    "                    localization_col: overall_localization_order,\n",
    "                },  # Apply consistent order\n",
    "                template=\"plotly_white\",\n",
    "            )\n",
    "            # Apply default layout and remove gridlines\n",
    "            fig_3a.update_layout(plotly_layout_defaults)\n",
    "            fig_3a.update_xaxes(\n",
    "                title_text=\"Group\", showgrid=False\n",
    "            )  # X-axis title is Group\n",
    "            fig_3a.update_yaxes(title_text=\"Percentage of Proteins (%)\", showgrid=False)\n",
    "            fig_3a.update_layout(\n",
    "                legend_title_text=\"Predicted Subcellular Localization\"\n",
    "            )  # Add legend title\n",
    "            fig_3a.update_xaxes(tickangle=0)  # No angle needed for Group labels\n",
    "\n",
    "            # Export Figure 3A (HTML, PDF, SVG)\n",
    "            fig_3a_path_html = (\n",
    "                Path(output_figure_dir) / \"figure3a_localization_distribution.html\"\n",
    "            )\n",
    "            fig_3a.write_html(str(fig_3a_path_html))\n",
    "            print(f\"Figure 3A HTML saved to: {fig_3a_path_html}\")\n",
    "\n",
    "            # Export to PDF/SVG (requires kaleido)\n",
    "            try:\n",
    "                fig_3a_path_pdf = (\n",
    "                    Path(output_figure_dir) / \"figure3a_localization_distribution.pdf\"\n",
    "                )\n",
    "                fig_3a.write_image(str(fig_3a_path_pdf))\n",
    "                print(f\"Figure 3A PDF saved to: {fig_3a_path_pdf}\")\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Warning: Could not export Figure 3A to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\"\n",
    "                )\n",
    "            try:\n",
    "                fig_3a_path_svg = (\n",
    "                    Path(output_figure_dir) / \"figure3a_localization_distribution.svg\"\n",
    "                )\n",
    "                fig_3a.write_image(str(fig_3a_path_svg))\n",
    "                print(f\"Figure 3A SVG saved to: {fig_3a_path_svg}\")\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Warning: Could not export Figure 3A to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\"\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            print(\"No localization data available for plotting Figure 3A.\")\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"Skipping Figure 3A due to missing columns in filtered data: '{localization_col}' or '{group_col}'.\"\n",
    "        )\n",
    "\n",
    "    # --- Figure 3B: Broad Functional Category Distribution ---\n",
    "    print(\"\\n--- Figure 3B: Broad Functional Category Distribution ---\")\n",
    "\n",
    "    if (\n",
    "        broad_func_cat_col in df_filtered_groups.columns\n",
    "        and group_col in df_filtered_groups.columns\n",
    "    ):\n",
    "        # Filter out unwanted categories for plotting (same as Figure 2D)\n",
    "        categories_to_exclude = [\"Unknown/Unclassified\", \"Other Specific Annotation\"]\n",
    "        df_category_plot_data = df_filtered_groups[\n",
    "            (~df_filtered_groups[broad_func_cat_col].isin(categories_to_exclude))\n",
    "        ].copy()\n",
    "\n",
    "        # Check if df_category_plot_data is empty after filtering categories\n",
    "        if df_category_plot_data.empty:\n",
    "            print(\n",
    "                \"Skipping Figure 3B plotting as no functional categories remain after excluding 'Unknown/Unclassified' and 'Other Specific Annotation'.\"\n",
    "            )\n",
    "        else:\n",
    "            # Calculate counts for each category by group in the FILTERED data for plotting\n",
    "            # Use value_counts to get counts for all categories (including 0 if they exist after filtering)\n",
    "            category_counts_plot = (\n",
    "                df_category_plot_data.groupby([group_col, broad_func_cat_col])\n",
    "                .size()\n",
    "                .reset_index(name=\"Count\")\n",
    "            )\n",
    "\n",
    "            # Calculate percentages for plotting (based on the FILTERED data for plotting)\n",
    "            # Need to calculate total count in the filtered data (for plotting) for normalization\n",
    "            total_filtered_proteins_plot_by_group = (\n",
    "                df_category_plot_data.groupby(group_col)\n",
    "                .size()\n",
    "                .reset_index(name=\"Total_Filtered_Plot\")\n",
    "            )\n",
    "\n",
    "            # Merge counts with total counts to calculate percentages\n",
    "            df_category_plot_summary = pd.merge(\n",
    "                category_counts_plot,\n",
    "                total_filtered_proteins_plot_by_group,\n",
    "                on=group_col,\n",
    "                how=\"left\",\n",
    "            )\n",
    "            df_category_plot_summary[\"Percentage\"] = (\n",
    "                df_category_plot_summary[\"Count\"]\n",
    "                / df_category_plot_summary[\"Total_Filtered_Plot\"]\n",
    "            ) * 100\n",
    "            df_category_plot_summary = df_category_plot_summary.drop(\n",
    "                columns=[\"Total_Filtered_Plot\"]\n",
    "            )  # Drop the temporary total column\n",
    "\n",
    "            # --- Filter by Count > 0 and Plot ---\n",
    "            # Check if df_category_plot_summary is not empty and has 'Count' column before filtering\n",
    "            if (\n",
    "                not df_category_plot_summary.empty\n",
    "                and \"Count\" in df_category_plot_summary.columns\n",
    "            ):\n",
    "                # Only filter by Count > 0 if there are any counts > 0 to filter\n",
    "                if (df_category_plot_summary[\"Count\"] > 0).any():\n",
    "                    df_category_plot_summary_filtered_by_count = (\n",
    "                        df_category_plot_summary[\n",
    "                            df_category_plot_summary[\"Count\"] > 0\n",
    "                        ].copy()\n",
    "                    )\n",
    "                else:\n",
    "                    # If all counts are 0, the filtered dataframe will be empty\n",
    "                    df_category_plot_summary_filtered_by_count = pd.DataFrame(\n",
    "                        columns=df_category_plot_summary.columns\n",
    "                    )\n",
    "\n",
    "                # Define category order for plotting based on frequency in the PLOTTED data\n",
    "                if not df_category_plot_summary_filtered_by_count.empty:\n",
    "                    plot_category_order = (\n",
    "                        df_category_plot_summary_filtered_by_count.groupby(\n",
    "                            broad_func_cat_col\n",
    "                        )[\"Count\"]\n",
    "                        .sum()\n",
    "                        .sort_values(ascending=False)\n",
    "                        .index.tolist()\n",
    "                    )\n",
    "\n",
    "                    fig_3b = px.bar(\n",
    "                        df_category_plot_summary_filtered_by_count,  # Use the DataFrame filtered by count\n",
    "                        x=group_col,  # X-axis is Group\n",
    "                        y=\"Percentage\",\n",
    "                        color=broad_func_cat_col,  # Color is Functional Category\n",
    "                        barmode=\"group\",  # Group bars by X-axis value (Group)\n",
    "                        # title='Broad Functional Category Distribution by Group', # No title\n",
    "                        labels={\n",
    "                            broad_func_cat_col: \"Broad Functional Category\",\n",
    "                            \"Percentage\": \"Percentage of Proteins (%)\",\n",
    "                            group_col: \"Group\",\n",
    "                        },\n",
    "                        color_discrete_map=broad_category_color_map,  # Use functional category color map\n",
    "                        category_orders={\n",
    "                            group_col: [\"Asgard\", \"GV\"],\n",
    "                            broad_func_cat_col: plot_category_order,\n",
    "                        },  # Apply consistent order\n",
    "                        template=\"plotly_white\",\n",
    "                    )\n",
    "                    # Apply default layout and remove gridlines\n",
    "                    fig_3b.update_layout(plotly_layout_defaults)\n",
    "                    fig_3b.update_xaxes(\n",
    "                        title_text=\"Group\", showgrid=False\n",
    "                    )  # X-axis title is Group\n",
    "                    fig_3b.update_yaxes(\n",
    "                        title_text=\"Percentage of Proteins (%)\", showgrid=False\n",
    "                    )\n",
    "                    fig_3b.update_layout(\n",
    "                        legend_title_text=\"Broad Functional Category\"\n",
    "                    )  # Add legend title\n",
    "                    fig_3b.update_xaxes(tickangle=0)  # No angle needed for Group labels\n",
    "\n",
    "                    # Export Figure 3B (HTML, PDF, SVG)\n",
    "                    fig_3b_path_html = (\n",
    "                        Path(output_figure_dir)\n",
    "                        / \"figure3b_broad_functional_category_distribution_filtered.html\"\n",
    "                    )\n",
    "                    fig_3b.write_html(str(fig_3b_path_html))\n",
    "                    print(f\"Figure 3B HTML saved to: {fig_3b_path_html}\")\n",
    "\n",
    "                    # Export to PDF/SVG (requires kaleido)\n",
    "                    try:\n",
    "                        fig_3b_path_pdf = (\n",
    "                            Path(output_figure_dir)\n",
    "                            / \"figure3b_broad_functional_category_distribution_filtered.pdf\"\n",
    "                        )\n",
    "                        fig_3b.write_image(str(fig_3b_path_pdf))\n",
    "                        print(f\"Figure 3B PDF saved to: {fig_3b_path_pdf}\")\n",
    "                    except Exception as e:\n",
    "                        print(\n",
    "                            f\"Warning: Could not export Figure 3B to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\"\n",
    "                        )\n",
    "                    try:\n",
    "                        fig_3b_path_svg = (\n",
    "                            Path(output_figure_dir)\n",
    "                            / \"figure3b_broad_functional_category_distribution_filtered.svg\"\n",
    "                        )\n",
    "                        fig_3b.write_image(str(fig_3b_path_svg))\n",
    "                        print(f\"Figure 3B SVG saved to: {fig_3b_path_svg}\")\n",
    "                    except Exception as e:\n",
    "                        print(\n",
    "                            f\"Warning: Could not export Figure 3B to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\"\n",
    "                        )\n",
    "\n",
    "print(\"\\n\\n--- Figure 3 Generation Complete ---\")\n",
    "print(f\"Figures saved to the '{output_figure_dir}' directory (HTML, PDF, SVG).\")\n",
    "# Note: Full functional category summary is saved in Cell 2 (Figure 2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcb7519-518c-4fb8-9981-29b0ffab8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intraorthogroup-diversity_100.png Figure\n",
    "\n",
    "# This cell loads the orthogroup diversity metrics and generates the plots for intraorthogroup-diversity_100.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d68f7-906b-4c65-9f97-d0b2b12a4d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Figure 9 - Overview of Orthogroup Diversity Metrics\n",
    "\n",
    "# --- Imports & Setup Assumptions ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "\n",
    "# This cell assumes that pandas (pd), numpy (np), plotly.express (px),\n",
    "# plotly.graph_objects (go), Path from pathlib,\n",
    "# and all necessary variables/helper functions from Cell 1 (Setup) are available.\n",
    "# Key variables from Cell 1 & subsequent cells:\n",
    "# - df_og_summary: DataFrame created in Cell 9, containing OG-level summaries (Orthogroup, Intra_OG_APSI, Group).\n",
    "# - output_figure_dir, output_summary_dir_phase1: Directories to save plots/data.\n",
    "# - plotly_layout_defaults: Default layout for Plotly figures.\n",
    "# - arcadia_primary_palette, group_colors: Color palettes/maps.\n",
    "# - orthogroup_col, apsi_col ('Intra_OG_APSI'), group_col\n",
    "\n",
    "print(\n",
    "    \"\\n\\n--- Generating Figure 9: Overview of Orthogroup Diversity Metrics (Cell 13) ---\"\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "diversity_metrics_path = (\n",
    "    \"orthogroup_diversity_metrics.csv\"  # Path to the uploaded diversity file\n",
    ")\n",
    "\n",
    "# --- Check if necessary DataFrames are available ---\n",
    "error_found = False\n",
    "if \"df_og_summary\" not in locals() or df_og_summary.empty:\n",
    "    print(\n",
    "        \"ERROR: DataFrame 'df_og_summary' (with OG-level APSI and Group) not found or is empty. Please run Cell 9 first.\"\n",
    "    )\n",
    "    error_found = True\n",
    "elif not all(\n",
    "    col in df_og_summary.columns for col in [orthogroup_col, apsi_col, group_col]\n",
    "):\n",
    "    missing_cols = [\n",
    "        col\n",
    "        for col in [orthogroup_col, apsi_col, group_col]\n",
    "        if col not in df_og_summary.columns\n",
    "    ]\n",
    "    print(f\"ERROR: 'df_og_summary' is missing required columns: {missing_cols}.\")\n",
    "    error_found = True\n",
    "\n",
    "if error_found:\n",
    "    print(\"Cannot proceed with diversity overview generation due to missing base data.\")\n",
    "    # Create an empty df_merged_diversity to prevent further errors in this cell if it's used later\n",
    "    df_merged_diversity = pd.DataFrame()\n",
    "else:\n",
    "    # --- 1. Load Diversity Metrics ---\n",
    "    print(\n",
    "        f\"\\n--- Loading Orthogroup Diversity Metrics from: {diversity_metrics_path} ---\"\n",
    "    )\n",
    "    try:\n",
    "        df_diversity = pd.read_csv(diversity_metrics_path)\n",
    "        if (\n",
    "            \"OG_ID\" in df_diversity.columns\n",
    "            and orthogroup_col not in df_diversity.columns\n",
    "        ):\n",
    "            df_diversity = df_diversity.rename(columns={\"OG_ID\": orthogroup_col})\n",
    "\n",
    "        # Select and rename relevant diversity columns\n",
    "        diversity_cols_to_use = {\n",
    "            orthogroup_col: orthogroup_col,\n",
    "            \"Tree_n_tips\": \"Observed_Richness\",\n",
    "            \"Tree_entropy\": \"Shannon_Entropy\",\n",
    "        }\n",
    "        actual_diversity_cols = {\n",
    "            k: v for k, v in diversity_cols_to_use.items() if k in df_diversity.columns\n",
    "        }\n",
    "        df_diversity_subset = (\n",
    "            df_diversity[list(actual_diversity_cols.keys())]\n",
    "            .rename(columns=actual_diversity_cols)\n",
    "            .copy()\n",
    "        )\n",
    "\n",
    "        # Ensure orthogroup_col is string for merging\n",
    "        if orthogroup_col in df_diversity_subset.columns:\n",
    "            df_diversity_subset[orthogroup_col] = df_diversity_subset[\n",
    "                orthogroup_col\n",
    "            ].astype(str)\n",
    "\n",
    "        print(\n",
    "            f\"Loaded and processed diversity metrics for {len(df_diversity_subset)} orthogroups.\"\n",
    "        )\n",
    "        if df_diversity_subset.empty:\n",
    "            print(\"WARNING: Diversity metrics DataFrame is empty.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\n",
    "            f\"ERROR: Diversity metrics file '{diversity_metrics_path}' not found. Diversity metrics will be missing.\"\n",
    "        )\n",
    "        df_diversity_subset = pd.DataFrame(\n",
    "            columns=[orthogroup_col, \"Observed_Richness\", \"Shannon_Entropy\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing diversity metrics file: {e}\")\n",
    "        df_diversity_subset = pd.DataFrame(\n",
    "            columns=[orthogroup_col, \"Observed_Richness\", \"Shannon_Entropy\"]\n",
    "        )\n",
    "\n",
    "    # --- 2. Merge Diversity Metrics with df_og_summary (contains APSI and Group) ---\n",
    "    print(\"\\n--- Merging Diversity Metrics with OG Summary Data (APSI, Group) ---\")\n",
    "    if not df_diversity_subset.empty and orthogroup_col in df_diversity_subset.columns:\n",
    "        # Ensure orthogroup_col in df_og_summary is also string\n",
    "        df_og_summary[orthogroup_col] = df_og_summary[orthogroup_col].astype(str)\n",
    "\n",
    "        df_merged_diversity = pd.merge(\n",
    "            df_og_summary, df_diversity_subset, on=orthogroup_col, how=\"inner\"\n",
    "        )\n",
    "        # Using 'inner' merge to only keep OGs present in both (i.e., those with diversity metrics and APSI/Group)\n",
    "        print(\n",
    "            f\"Merged diversity data. Resulting DataFrame shape: {df_merged_diversity.shape}\"\n",
    "        )\n",
    "        if df_merged_diversity.empty:\n",
    "            print(\n",
    "                \"WARNING: Merged diversity DataFrame is empty. No common OGs found or one of the DFs was empty.\"\n",
    "            )\n",
    "    else:\n",
    "        print(\"Skipping merge as diversity data is empty or missing orthogroup column.\")\n",
    "        df_merged_diversity = pd.DataFrame()  # Ensure it's defined as empty\n",
    "\n",
    "    # --- 3. Generate Plots ---\n",
    "    if not df_merged_diversity.empty:\n",
    "        # A. Distribution of Shannon Entropy\n",
    "        if \"Shannon_Entropy\" in df_merged_diversity.columns:\n",
    "            print(\"\\n--- Plotting Distribution of Shannon Entropy ---\")\n",
    "            fig_shannon = px.histogram(\n",
    "                df_merged_diversity.dropna(subset=[\"Shannon_Entropy\"]),\n",
    "                x=\"Shannon_Entropy\",\n",
    "                color=group_col,\n",
    "                marginal=\"box\",  # or \"rug\", \"violin\"\n",
    "                barmode=\"overlay\",\n",
    "                # title=\"Distribution of Shannon Entropy per Orthogroup\", # No title\n",
    "                labels={\"Shannon_Entropy\": \"Shannon Entropy (Tree-based)\"},\n",
    "                color_discrete_map=group_colors,  # from Cell 1\n",
    "            )\n",
    "            fig_shannon.update_layout(plotly_layout_defaults)\n",
    "            fig_shannon.update_xaxes(\n",
    "                title_text=\"Shannon Entropy (Tree-based)\", showgrid=False\n",
    "            )\n",
    "            fig_shannon.update_yaxes(title_text=\"Number of Orthogroups\", showgrid=False)\n",
    "            fig_shannon.update_traces(opacity=0.75)\n",
    "            fig_shannon.show()\n",
    "            # Save plot\n",
    "            fig_shannon_path_html = (\n",
    "                Path(output_figure_dir) / \"figure9a_shannon_entropy_distribution.html\"\n",
    "            )\n",
    "            fig_shannon.write_html(str(fig_shannon_path_html))\n",
    "            print(f\"Figure 9A (Shannon Entropy) HTML saved to: {fig_shannon_path_html}\")\n",
    "            try:\n",
    "                fig_shannon_path_pdf = (\n",
    "                    Path(output_figure_dir)\n",
    "                    / \"figure9a_shannon_entropy_distribution.pdf\"\n",
    "                )\n",
    "                fig_shannon.write_image(str(fig_shannon_path_pdf))\n",
    "            except Exception as e:\n",
    "                print(f\"PDF export error for Shannon Entropy: {e}\")\n",
    "            try:\n",
    "                fig_shannon_path_svg = (\n",
    "                    Path(output_figure_dir)\n",
    "                    / \"figure9a_shannon_entropy_distribution.svg\"\n",
    "                )\n",
    "                fig_shannon.write_image(str(fig_shannon_path_svg))\n",
    "            except Exception as e:\n",
    "                print(f\"SVG export error for Shannon Entropy: {e}\")\n",
    "        else:\n",
    "            print(\"Skipping Shannon Entropy distribution: column not found.\")\n",
    "\n",
    "        # B. Distribution of Observed Richness\n",
    "        if \"Observed_Richness\" in df_merged_diversity.columns:\n",
    "            print(\"\\n--- Plotting Distribution of Observed Richness (Tree Tips) ---\")\n",
    "            fig_richness = px.histogram(\n",
    "                df_merged_diversity.dropna(subset=[\"Observed_Richness\"]),\n",
    "                x=\"Observed_Richness\",\n",
    "                color=group_col,\n",
    "                marginal=\"box\",\n",
    "                barmode=\"overlay\",\n",
    "                # title=\"Distribution of Observed Richness (Tree Tips) per Orthogroup\", # No title\n",
    "                labels={\"Observed_Richness\": \"Observed Richness (Number of Tree Tips)\"},\n",
    "                color_discrete_map=group_colors,\n",
    "            )\n",
    "            fig_richness.update_layout(plotly_layout_defaults)\n",
    "            fig_richness.update_xaxes(\n",
    "                title_text=\"Observed Richness (Number of Tree Tips)\", showgrid=False\n",
    "            )  # Consider log scale if highly skewed: type=\"log\"\n",
    "            fig_richness.update_yaxes(\n",
    "                title_text=\"Number of Orthogroups\", showgrid=False\n",
    "            )\n",
    "            fig_richness.update_traces(opacity=0.75)\n",
    "            fig_richness.show()\n",
    "            # Save plot\n",
    "            fig_richness_path_html = (\n",
    "                Path(output_figure_dir) / \"figure9b_observed_richness_distribution.html\"\n",
    "            )\n",
    "            fig_richness.write_html(str(fig_richness_path_html))\n",
    "            print(\n",
    "                f\"Figure 9B (Observed Richness) HTML saved to: {fig_richness_path_html}\"\n",
    "            )\n",
    "            try:\n",
    "                fig_richness_path_pdf = (\n",
    "                    Path(output_figure_dir)\n",
    "                    / \"figure9b_observed_richness_distribution.pdf\"\n",
    "                )\n",
    "                fig_richness.write_image(str(fig_richness_path_pdf))\n",
    "            except Exception as e:\n",
    "                print(f\"PDF export error for Richness: {e}\")\n",
    "            try:\n",
    "                fig_richness_path_svg = (\n",
    "                    Path(output_figure_dir)\n",
    "                    / \"figure9b_observed_richness_distribution.svg\"\n",
    "                )\n",
    "                fig_richness.write_image(str(fig_richness_path_svg))\n",
    "            except Exception as e:\n",
    "                print(f\"SVG export error for Richness: {e}\")\n",
    "        else:\n",
    "            print(\"Skipping Observed Richness distribution: column not found.\")\n",
    "\n",
    "        # C. Shannon Entropy vs. Observed Richness\n",
    "        if (\n",
    "            \"Shannon_Entropy\" in df_merged_diversity.columns\n",
    "            and \"Observed_Richness\" in df_merged_diversity.columns\n",
    "        ):\n",
    "            print(\"\\n--- Plotting Shannon Entropy vs. Observed Richness ---\")\n",
    "            fig_ent_vs_rich = px.scatter(\n",
    "                df_merged_diversity.dropna(\n",
    "                    subset=[\"Shannon_Entropy\", \"Observed_Richness\"]\n",
    "                ),\n",
    "                x=\"Observed_Richness\",\n",
    "                y=\"Shannon_Entropy\",\n",
    "                color=group_col,\n",
    "                # title='Shannon Entropy vs. Observed Richness per Orthogroup', # No title\n",
    "                labels={\n",
    "                    \"Observed_Richness\": \"Observed Richness (Tree Tips)\",\n",
    "                    \"Shannon_Entropy\": \"Shannon Entropy (Tree-based)\",\n",
    "                },\n",
    "                color_discrete_map=group_colors,\n",
    "                opacity=0.7,\n",
    "                trendline=\"ols\",  # Ordinary Least Squares trendline\n",
    "                trendline_scope=\"overall\",  # or \"trace\" for per-group trendlines\n",
    "            )\n",
    "            fig_ent_vs_rich.update_layout(plotly_layout_defaults)\n",
    "            fig_ent_vs_rich.update_xaxes(\n",
    "                title_text=\"Observed Richness (Tree Tips)\", showgrid=False\n",
    "            )  # Consider log scale: type=\"log\"\n",
    "            fig_ent_vs_rich.update_yaxes(\n",
    "                title_text=\"Shannon Entropy (Tree-based)\", showgrid=False\n",
    "            )\n",
    "            fig_ent_vs_rich.show()\n",
    "            # Save plot\n",
    "            fig_ent_rich_path_html = (\n",
    "                Path(output_figure_dir) / \"figure9c_entropy_vs_richness.html\"\n",
    "            )\n",
    "            fig_ent_vs_rich.write_html(str(fig_ent_rich_path_html))\n",
    "            print(\n",
    "                f\"Figure 9C (Entropy vs Richness) HTML saved to: {fig_ent_rich_path_html}\"\n",
    "            )\n",
    "            try:\n",
    "                fig_ent_rich_path_pdf = (\n",
    "                    Path(output_figure_dir) / \"figure9c_entropy_vs_richness.pdf\"\n",
    "                )\n",
    "                fig_ent_vs_rich.write_image(str(fig_ent_rich_path_pdf))\n",
    "            except Exception as e:\n",
    "                print(f\"PDF export error for Entropy vs Richness: {e}\")\n",
    "            try:\n",
    "                fig_ent_rich_path_svg = (\n",
    "                    Path(output_figure_dir) / \"figure9c_entropy_vs_richness.svg\"\n",
    "                )\n",
    "                fig_ent_vs_rich.write_image(str(fig_ent_rich_path_svg))\n",
    "            except Exception as e:\n",
    "                print(f\"SVG export error for Entropy vs Richness: {e}\")\n",
    "        else:\n",
    "            print(\n",
    "                \"Skipping Shannon Entropy vs. Observed Richness plot: columns not found.\"\n",
    "            )\n",
    "\n",
    "        # D. Shannon Entropy vs. APSI\n",
    "        if (\n",
    "            \"Shannon_Entropy\" in df_merged_diversity.columns\n",
    "            and apsi_col in df_merged_diversity.columns\n",
    "        ):\n",
    "            print(\"\\n--- Plotting Shannon Entropy vs. APSI ---\")\n",
    "            # Convert APSI to percentage for plotting if it's not already\n",
    "            df_merged_diversity[\"APSI_Percent\"] = df_merged_diversity[apsi_col] * 100\n",
    "\n",
    "            fig_ent_vs_apsi = px.scatter(\n",
    "                df_merged_diversity.dropna(subset=[\"Shannon_Entropy\", \"APSI_Percent\"]),\n",
    "                x=\"APSI_Percent\",\n",
    "                y=\"Shannon_Entropy\",\n",
    "                color=group_col,\n",
    "                # title='Shannon Entropy vs. Intra-OG APSI', # No title\n",
    "                labels={\n",
    "                    \"APSI_Percent\": \"Intra-OG APSI (%)\",\n",
    "                    \"Shannon_Entropy\": \"Shannon Entropy (Tree-based)\",\n",
    "                },\n",
    "                color_discrete_map=group_colors,\n",
    "                opacity=0.7,\n",
    "                trendline=\"ols\",\n",
    "                trendline_scope=\"overall\",\n",
    "            )\n",
    "            fig_ent_vs_apsi.update_layout(plotly_layout_defaults)\n",
    "            fig_ent_vs_apsi.update_xaxes(title_text=\"Intra-OG APSI (%)\", showgrid=False)\n",
    "            fig_ent_vs_apsi.update_yaxes(\n",
    "                title_text=\"Shannon Entropy (Tree-based)\", showgrid=False\n",
    "            )\n",
    "            fig_ent_vs_apsi.show()\n",
    "            # Save plot\n",
    "            fig_ent_apsi_path_html = (\n",
    "                Path(output_figure_dir) / \"figure9d_entropy_vs_apsi.html\"\n",
    "            )\n",
    "            fig_ent_vs_apsi.write_html(str(fig_ent_apsi_path_html))\n",
    "            print(\n",
    "                f\"Figure 9D (Entropy vs APSI) HTML saved to: {fig_ent_apsi_path_html}\"\n",
    "            )\n",
    "            try:\n",
    "                fig_ent_apsi_path_pdf = (\n",
    "                    Path(output_figure_dir) / \"figure9d_entropy_vs_apsi.pdf\"\n",
    "                )\n",
    "                fig_ent_vs_apsi.write_image(str(fig_ent_apsi_path_pdf))\n",
    "            except Exception as e:\n",
    "                print(f\"PDF export error for Entropy vs APSI: {e}\")\n",
    "            try:\n",
    "                fig_ent_apsi_path_svg = (\n",
    "                    Path(output_figure_dir) / \"figure9d_entropy_vs_apsi.svg\"\n",
    "                )\n",
    "                fig_ent_vs_apsi.write_image(str(fig_ent_apsi_path_svg))\n",
    "            except Exception as e:\n",
    "                print(f\"SVG export error for Entropy vs APSI: {e}\")\n",
    "        else:\n",
    "            print(\"Skipping Shannon Entropy vs. APSI plot: columns not found.\")\n",
    "\n",
    "        # E. Observed Richness vs. APSI\n",
    "        if (\n",
    "            \"Observed_Richness\" in df_merged_diversity.columns\n",
    "            and apsi_col in df_merged_diversity.columns\n",
    "        ):\n",
    "            print(\"\\n--- Plotting Observed Richness vs. APSI ---\")\n",
    "            # APSI_Percent column should exist from previous plot\n",
    "\n",
    "            fig_rich_vs_apsi = px.scatter(\n",
    "                df_merged_diversity.dropna(\n",
    "                    subset=[\"Observed_Richness\", \"APSI_Percent\"]\n",
    "                ),\n",
    "                x=\"APSI_Percent\",\n",
    "                y=\"Observed_Richness\",\n",
    "                color=group_col,\n",
    "                # title='Observed Richness vs. Intra-OG APSI', # No title\n",
    "                labels={\n",
    "                    \"APSI_Percent\": \"Intra-OG APSI (%)\",\n",
    "                    \"Observed_Richness\": \"Observed Richness (Tree Tips)\",\n",
    "                },\n",
    "                color_discrete_map=group_colors,\n",
    "                opacity=0.7,\n",
    "                trendline=\"ols\",\n",
    "                trendline_scope=\"overall\",\n",
    "            )\n",
    "            fig_rich_vs_apsi.update_layout(plotly_layout_defaults)\n",
    "            fig_rich_vs_apsi.update_xaxes(\n",
    "                title_text=\"Intra-OG APSI (%)\", showgrid=False\n",
    "            )\n",
    "            fig_rich_vs_apsi.update_yaxes(\n",
    "                title_text=\"Observed Richness (Tree Tips)\", showgrid=False\n",
    "            )  # Consider log scale: type=\"log\"\n",
    "            fig_rich_vs_apsi.show()\n",
    "            # Save plot\n",
    "            fig_rich_apsi_path_html = (\n",
    "                Path(output_figure_dir) / \"figure9e_richness_vs_apsi.html\"\n",
    "            )\n",
    "            fig_rich_vs_apsi.write_html(str(fig_rich_apsi_path_html))\n",
    "            print(\n",
    "                f\"Figure 9E (Richness vs APSI) HTML saved to: {fig_rich_apsi_path_html}\"\n",
    "            )\n",
    "            try:\n",
    "                fig_rich_apsi_path_pdf = (\n",
    "                    Path(output_figure_dir) / \"figure9e_richness_vs_apsi.pdf\"\n",
    "                )\n",
    "                fig_rich_vs_apsi.write_image(str(fig_rich_apsi_path_pdf))\n",
    "            except Exception as e:\n",
    "                print(f\"PDF export error for Richness vs APSI: {e}\")\n",
    "            try:\n",
    "                fig_rich_apsi_path_svg = (\n",
    "                    Path(output_figure_dir) / \"figure9e_richness_vs_apsi.svg\"\n",
    "                )\n",
    "                fig_rich_vs_apsi.write_image(str(fig_rich_apsi_path_svg))\n",
    "            except Exception as e:\n",
    "                print(f\"SVG export error for Richness vs APSI: {e}\")\n",
    "        else:\n",
    "            print(\"Skipping Observed Richness vs. APSI plot: columns not found.\")\n",
    "\n",
    "        # Save the merged diversity data\n",
    "        merged_diversity_summary_path = (\n",
    "            Path(output_summary_dir_phase1)\n",
    "            / \"figure9_merged_diversity_and_apsi_data.csv\"\n",
    "        )\n",
    "        try:\n",
    "            df_merged_diversity.to_csv(merged_diversity_summary_path, index=False)\n",
    "            print(\n",
    "                f\"\\nMerged diversity and APSI data saved to: {merged_diversity_summary_path}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving merged diversity data: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Skipping Figure 9 plots as 'df_merged_diversity' is empty.\")\n",
    "\n",
    "print(\n",
    "    \"\\n\\n--- Cell 13 (Figure 9 - Overview of Orthogroup Diversity Metrics) Complete ---\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f9147b-7698-45b6-9e32-638f1422860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell generates plots to show which eukaryotic organisms frequently appear as top hits in DIAMOND searches. These plots comprise Figure Eukaryote-homologs_85.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa22d5-eae7-4ddc-99cc-b5f17e2e0155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Figure Eukaryote-homologs_85.png - Characterizing Eukaryotic Hits for Asgard and Giant Virus Proteins (Consistent Colors)\n",
    "\n",
    "# --- Imports & Setup Assumptions ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "\n",
    "# This cell assumes that pandas (pd), numpy (np), plotly.express (px),\n",
    "# plotly.graph_objects (go), Path from pathlib, make_subplots,\n",
    "# and all necessary variables/helper functions from Cell 1 (Setup) are available.\n",
    "# Key variables from Cell 1:\n",
    "# - output_figure_dir, output_summary_dir_phase1: Directories to save plots/data.\n",
    "# - plotly_layout_defaults: Default layout for Plotly figures.\n",
    "# - arcadia_primary_palette, arcadia_secondary_palette: Color palettes.\n",
    "# - group_col, protein_id_col,\n",
    "# - 'Has_Euk_DIAMOND_Hit', 'Euk_Hit_Organism', 'Euk_Hit_Protein_Name' (column names)\n",
    "# - clean_protein_name (helper function from Cell 1)\n",
    "\n",
    "print(\n",
    "    \"\\n\\n--- Generating Figure Eukaryote-homologs_85.png: Characterizing Eukaryotic Hits (Consistent Colors) (Cell 21) ---\"\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "DB_PATH_V2_4 = \"proteome_database_v2.5.csv\"  # INPUT: Database with GV Euk Hits\n",
    "TOP_N_ORGANISMS = 25\n",
    "TOP_N_PROTEIN_NAMES = 30\n",
    "\n",
    "# --- Load Data ---\n",
    "error_found_loading = False\n",
    "df_db_v2_4 = pd.DataFrame()\n",
    "\n",
    "if not Path(DB_PATH_V2_4).is_file():\n",
    "    print(\n",
    "        f\"ERROR: Database file '{DB_PATH_V2_4}' not found. Please ensure it was created correctly.\"\n",
    "    )\n",
    "    error_found_loading = True\n",
    "else:\n",
    "    try:\n",
    "        df_db_v2_4 = pd.read_csv(DB_PATH_V2_4, low_memory=False)\n",
    "        required_cols = [\n",
    "            group_col,\n",
    "            \"Has_Euk_DIAMOND_Hit\",\n",
    "            \"Euk_Hit_Organism\",\n",
    "            \"Euk_Hit_Protein_Name\",\n",
    "            protein_id_col,\n",
    "        ]\n",
    "        if not all(col in df_db_v2_4.columns for col in required_cols):\n",
    "            missing = [col for col in required_cols if col not in df_db_v2_4.columns]\n",
    "            print(\n",
    "                f\"ERROR: Loaded database '{DB_PATH_V2_4}' is missing critical columns: {missing}\"\n",
    "            )\n",
    "            error_found_loading = True\n",
    "        else:\n",
    "            print(\n",
    "                f\"Successfully loaded database '{DB_PATH_V2_4}'. Shape: {df_db_v2_4.shape}\"\n",
    "            )\n",
    "            df_db_v2_4[\"Has_Euk_DIAMOND_Hit\"] = (\n",
    "                df_db_v2_4[\"Has_Euk_DIAMOND_Hit\"].fillna(False).astype(bool)\n",
    "            )\n",
    "            df_db_v2_4[\"Euk_Hit_Organism\"] = (\n",
    "                df_db_v2_4[\"Euk_Hit_Organism\"].fillna(\"Unknown\").astype(str).str.strip()\n",
    "            )\n",
    "            df_db_v2_4[\"Euk_Hit_Protein_Name\"] = (\n",
    "                df_db_v2_4[\"Euk_Hit_Protein_Name\"].fillna(\"Unknown\").astype(str)\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading database '{DB_PATH_V2_4}': {e}\")\n",
    "        error_found_loading = True\n",
    "\n",
    "if error_found_loading:\n",
    "    print(\"Cannot proceed with Figure 13 generation due to missing input data.\")\n",
    "else:\n",
    "    # --- Prepare Data for Panels A and B ---\n",
    "    df_asgard_hits_panel_a = df_db_v2_4[\n",
    "        (df_db_v2_4[group_col] == \"Asgard\")\n",
    "        & (df_db_v2_4[\"Has_Euk_DIAMOND_Hit\"] == True)\n",
    "    ].copy()\n",
    "\n",
    "    df_gv_hits_panel_b = df_db_v2_4[\n",
    "        (\n",
    "            df_db_v2_4.get(group_col) == \"GV\"\n",
    "        )  # Use .get for safety if group_col might be missing\n",
    "        & (df_db_v2_4[\"Has_Euk_DIAMOND_Hit\"] == True)\n",
    "    ].copy()\n",
    "\n",
    "    # --- Create a Consistent Color Map for Eukaryotic Organisms ---\n",
    "    print(\"\\n--- Creating Consistent Color Map for Eukaryotic Organisms ---\")\n",
    "    top_asgard_orgs_list = []\n",
    "    if not df_asgard_hits_panel_a.empty:\n",
    "        top_asgard_orgs_list = (\n",
    "            df_asgard_hits_panel_a[\"Euk_Hit_Organism\"]\n",
    "            .value_counts()\n",
    "            .nlargest(TOP_N_ORGANISMS)\n",
    "            .index.tolist()\n",
    "        )\n",
    "\n",
    "    top_gv_orgs_list = []\n",
    "    if not df_gv_hits_panel_b.empty:\n",
    "        top_gv_orgs_list = (\n",
    "            df_gv_hits_panel_b[\"Euk_Hit_Organism\"]\n",
    "            .value_counts()\n",
    "            .nlargest(TOP_N_ORGANISMS)\n",
    "            .index.tolist()\n",
    "        )\n",
    "\n",
    "    # Combine and get unique organisms from the top lists of both groups\n",
    "    all_top_organisms = sorted(\n",
    "        list(set(top_asgard_orgs_list + top_gv_orgs_list) - {\"Unknown\"})\n",
    "    )\n",
    "\n",
    "    euk_organism_color_map = {}\n",
    "    # Use a combined palette for more variety if many unique organisms\n",
    "    combined_palette = (\n",
    "        arcadia_primary_palette + arcadia_secondary_palette + arcadia_neutrals_palette\n",
    "    )\n",
    "    # Ensure 'Unknown' gets a specific color if it appears\n",
    "    euk_organism_color_map[\"Unknown\"] = \"#cccccc\"  # A neutral gray for Unknown\n",
    "\n",
    "    for i, org_name in enumerate(all_top_organisms):\n",
    "        euk_organism_color_map[org_name] = combined_palette[i % len(combined_palette)]\n",
    "\n",
    "    print(\n",
    "        f\"Created color map for {len(euk_organism_color_map)} unique eukaryotic organisms (including Unknown).\"\n",
    "    )\n",
    "\n",
    "    # --- Create Figure with Subplots ---\n",
    "    fig = make_subplots(\n",
    "        rows=3,\n",
    "        cols=1,\n",
    "        subplot_titles=(\n",
    "            \"<b>A.</b> Top Eukaryotic Organisms Hit by Asgard Proteins\",\n",
    "            \"<b>B.</b> Top Eukaryotic Organisms Hit by Giant Virus Proteins\",\n",
    "            \"<b>C.</b> Top Eukaryotic Protein Functions Hit by Giant Virus Proteins\",\n",
    "        ),\n",
    "        vertical_spacing=0.1,\n",
    "    )\n",
    "\n",
    "    # --- Panel A: Top Eukaryotic Organisms Hit by Asgard Proteins ---\n",
    "    print(\"\\n--- Generating Panel A: Asgard Eukaryotic Hit Organisms ---\")\n",
    "    if not df_asgard_hits_panel_a.empty:\n",
    "        asgard_org_counts = (\n",
    "            df_asgard_hits_panel_a[\"Euk_Hit_Organism\"]\n",
    "            .value_counts()\n",
    "            .nlargest(TOP_N_ORGANISMS)\n",
    "            .reset_index()\n",
    "        )\n",
    "        asgard_org_counts.columns = [\"Euk_Hit_Organism\", \"Count\"]\n",
    "\n",
    "        if not asgard_org_counts.empty:\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=asgard_org_counts[\"Euk_Hit_Organism\"],\n",
    "                    y=asgard_org_counts[\"Count\"],\n",
    "                    name=\"Asgard Hits\",  # Will not be shown in legend if showlegend=False for the trace\n",
    "                    marker_color=[\n",
    "                        euk_organism_color_map.get(org, \"#999999\")\n",
    "                        for org in asgard_org_counts[\"Euk_Hit_Organism\"]\n",
    "                    ],  # Apply consistent map\n",
    "                ),\n",
    "                row=1,\n",
    "                col=1,\n",
    "            )\n",
    "            fig.update_xaxes(\n",
    "                title_text=\"\",\n",
    "                tickangle=45,\n",
    "                categoryorder=\"total descending\",\n",
    "                row=1,\n",
    "                col=1,\n",
    "            )\n",
    "            fig.update_yaxes(title_text=\"Number of Asgard Proteins\", row=1, col=1)\n",
    "\n",
    "            panel_a_data_path = (\n",
    "                Path(output_summary_dir_phase1)\n",
    "                / \"figure13_panel_a_asgard_euk_org_hits.csv\"\n",
    "            )\n",
    "            try:\n",
    "                df_asgard_hits_panel_a[\n",
    "                    \"Euk_Hit_Organism\"\n",
    "                ].value_counts().reset_index().to_csv(panel_a_data_path, index=False)\n",
    "                print(f\"Data for Panel A saved to {panel_a_data_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving Panel A data: {e}\")\n",
    "        else:\n",
    "            print(\"No eukaryotic hits found for Asgard proteins to plot for Panel A.\")\n",
    "    else:\n",
    "        print(\"No Asgard proteins with eukaryotic hits found for Panel A.\")\n",
    "\n",
    "    # --- Panel B: Top Eukaryotic Organisms Hit by Giant Virus Proteins ---\n",
    "    print(\"\\n--- Generating Panel B: Giant Virus Eukaryotic Hit Organisms ---\")\n",
    "    if not df_gv_hits_panel_b.empty:\n",
    "        gv_org_counts = (\n",
    "            df_gv_hits_panel_b[\"Euk_Hit_Organism\"]\n",
    "            .value_counts()\n",
    "            .nlargest(TOP_N_ORGANISMS)\n",
    "            .reset_index()\n",
    "        )\n",
    "        gv_org_counts.columns = [\"Euk_Hit_Organism\", \"Count\"]\n",
    "\n",
    "        if not gv_org_counts.empty:\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=gv_org_counts[\"Euk_Hit_Organism\"],\n",
    "                    y=gv_org_counts[\"Count\"],\n",
    "                    name=\"GV Hits\",\n",
    "                    marker_color=[\n",
    "                        euk_organism_color_map.get(org, \"#999999\")\n",
    "                        for org in gv_org_counts[\"Euk_Hit_Organism\"]\n",
    "                    ],  # Apply consistent map\n",
    "                ),\n",
    "                row=2,\n",
    "                col=1,\n",
    "            )\n",
    "            fig.update_xaxes(\n",
    "                title_text=\"\",\n",
    "                tickangle=45,\n",
    "                categoryorder=\"total descending\",\n",
    "                row=2,\n",
    "                col=1,\n",
    "            )\n",
    "            fig.update_yaxes(title_text=\"Number of GV Proteins\", row=2, col=1)\n",
    "\n",
    "            panel_b_data_path = (\n",
    "                Path(output_summary_dir_phase1) / \"figure13_panel_b_gv_euk_org_hits.csv\"\n",
    "            )\n",
    "            try:\n",
    "                df_gv_hits_panel_b[\n",
    "                    \"Euk_Hit_Organism\"\n",
    "                ].value_counts().reset_index().to_csv(panel_b_data_path, index=False)\n",
    "                print(f\"Data for Panel B saved to {panel_b_data_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving Panel B data: {e}\")\n",
    "        else:\n",
    "            print(\"No eukaryotic hits found for GV proteins to plot for Panel B.\")\n",
    "    else:\n",
    "        print(\"No GV proteins with eukaryotic hits found for Panel B.\")\n",
    "\n",
    "    # --- Panel C: Top Eukaryotic Protein Functions Hit by Giant Virus Proteins ---\n",
    "    print(\"\\n--- Generating Panel C: Giant Virus Eukaryotic Hit Protein Functions ---\")\n",
    "    if not df_gv_hits_panel_b.empty:\n",
    "        if \"clean_protein_name\" in locals() and callable(clean_protein_name):\n",
    "            df_gv_hits_panel_b[\"Cleaned_Euk_Hit_Protein_Name\"] = df_gv_hits_panel_b[\n",
    "                \"Euk_Hit_Protein_Name\"\n",
    "            ].apply(clean_protein_name)\n",
    "        else:\n",
    "            print(\n",
    "                \"WARNING: 'clean_protein_name' function not found. Using raw Euk_Hit_Protein_Name for Panel C.\"\n",
    "            )\n",
    "            df_gv_hits_panel_b[\"Cleaned_Euk_Hit_Protein_Name\"] = df_gv_hits_panel_b[\n",
    "                \"Euk_Hit_Protein_Name\"\n",
    "            ]\n",
    "\n",
    "        gv_prot_name_counts_all = df_gv_hits_panel_b[\n",
    "            \"Cleaned_Euk_Hit_Protein_Name\"\n",
    "        ].value_counts()\n",
    "        # Filter out very generic names AFTER counting, before selecting top N for plot\n",
    "        gv_prot_name_counts_filtered = gv_prot_name_counts_all[\n",
    "            ~gv_prot_name_counts_all.index.astype(str).str.contains(\n",
    "                \"Hypothetical|Unknown|Uncharacterized|^nan$\",\n",
    "                case=False,\n",
    "                na=False,\n",
    "                regex=True,\n",
    "            )\n",
    "        ]\n",
    "        gv_prot_name_counts_plot = gv_prot_name_counts_filtered.nlargest(\n",
    "            TOP_N_PROTEIN_NAMES\n",
    "        ).reset_index()\n",
    "        gv_prot_name_counts_plot.columns = [\"Cleaned_Euk_Hit_Protein_Name\", \"Count\"]\n",
    "\n",
    "        if not gv_prot_name_counts_plot.empty:\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=gv_prot_name_counts_plot[\"Cleaned_Euk_Hit_Protein_Name\"],\n",
    "                    y=gv_prot_name_counts_plot[\"Count\"],\n",
    "                    name=\"GV Hit Protein Functions\",\n",
    "                    marker_color=arcadia_secondary_palette[\n",
    "                        0 % len(arcadia_secondary_palette)\n",
    "                    ],  # Different palette for this panel\n",
    "                ),\n",
    "                row=3,\n",
    "                col=1,\n",
    "            )\n",
    "            fig.update_xaxes(\n",
    "                title_text=\"Eukaryotic Protein Function (Cleaned Name)\",\n",
    "                tickangle=45,\n",
    "                categoryorder=\"total descending\",\n",
    "                row=3,\n",
    "                col=1,\n",
    "            )\n",
    "            fig.update_yaxes(title_text=\"Number of GV Protein Hits\", row=3, col=1)\n",
    "\n",
    "            panel_c_data_path = (\n",
    "                Path(output_summary_dir_phase1)\n",
    "                / \"figure13_panel_c_gv_euk_prot_func_hits.csv\"\n",
    "            )\n",
    "            try:\n",
    "                gv_prot_name_counts_all.reset_index().to_csv(\n",
    "                    panel_c_data_path, index=False\n",
    "                )  # Save all counts\n",
    "                print(f\"Data for Panel C saved to {panel_c_data_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving Panel C data: {e}\")\n",
    "        else:\n",
    "            print(\n",
    "                \"No eukaryotic protein names found for GV hits to plot for Panel C after filtering generic names.\"\n",
    "            )\n",
    "    else:\n",
    "        print(\"No GV proteins with eukaryotic hits found for Panel C analysis.\")\n",
    "\n",
    "    # --- Final Layout Updates for the Entire Figure ---\n",
    "    fig.update_layout(\n",
    "        height=1400,  # Increased height for better label spacing\n",
    "        plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        margin=dict(l=80, r=50, t=100, b=220),  # Increased bottom margin\n",
    "        showlegend=False,\n",
    "    )\n",
    "    if \"plotly_layout_defaults\" in locals():\n",
    "        fig.update_layout(font=plotly_layout_defaults.font)\n",
    "        for i in range(len(fig.layout.annotations)):\n",
    "            if fig.layout.annotations[i].text.startswith(\"<b>\"):\n",
    "                fig.layout.annotations[i].font.size = 14\n",
    "                if plotly_layout_defaults.font:\n",
    "                    fig.layout.annotations[\n",
    "                        i\n",
    "                    ].font.family = plotly_layout_defaults.font.family\n",
    "\n",
    "    if \"plotly_layout_defaults\" in locals():\n",
    "        for axis_name_template in [\"xaxis\", \"yaxis\"]:\n",
    "            for i in range(1, 4):\n",
    "                axis_ref_name = f\"{axis_name_template}{i if i > 1 else ''}\"\n",
    "                if hasattr(fig.layout, axis_ref_name) and hasattr(\n",
    "                    plotly_layout_defaults, axis_name_template\n",
    "                ):\n",
    "                    default_axis_style = getattr(\n",
    "                        plotly_layout_defaults, axis_name_template\n",
    "                    )\n",
    "                    current_axis = getattr(fig.layout, axis_ref_name)\n",
    "                    if (\n",
    "                        current_axis.title\n",
    "                        and current_axis.title.text\n",
    "                        and default_axis_style.title\n",
    "                        and default_axis_style.title.font\n",
    "                    ):\n",
    "                        current_axis.title.font.size = (\n",
    "                            default_axis_style.title.font.size\n",
    "                        )\n",
    "                        current_axis.title.font.family = (\n",
    "                            default_axis_style.title.font.family\n",
    "                        )\n",
    "                        current_axis.title.font.weight = (\n",
    "                            default_axis_style.title.font.weight\n",
    "                        )\n",
    "                    elif default_axis_style.title and default_axis_style.title.font:\n",
    "                        current_axis.title.font.size = (\n",
    "                            default_axis_style.title.font.size\n",
    "                        )\n",
    "                        current_axis.title.font.family = (\n",
    "                            default_axis_style.title.font.family\n",
    "                        )\n",
    "                        current_axis.title.font.weight = (\n",
    "                            default_axis_style.title.font.weight\n",
    "                        )\n",
    "                    if default_axis_style.tickfont:\n",
    "                        current_axis.tickfont.size = default_axis_style.tickfont.size\n",
    "                        current_axis.tickfont.family = (\n",
    "                            default_axis_style.tickfont.family\n",
    "                        )\n",
    "                    current_axis.showgrid = False\n",
    "                    current_axis.zeroline = False\n",
    "                    current_axis.linecolor = \"black\"\n",
    "                    current_axis.linewidth = 1.5\n",
    "                    current_axis.ticks = \"outside\"\n",
    "                    current_axis.ticklen = 5\n",
    "                    current_axis.tickwidth = 1.5\n",
    "                    current_axis.tickcolor = \"black\"\n",
    "    fig.show()\n",
    "\n",
    "    # --- Save Figure ---\n",
    "    fig_path_html = (\n",
    "        Path(output_figure_dir)\n",
    "        / \"figure_eukaryotic_hit_characterization_consistent_colors.html\"\n",
    "    )\n",
    "    fig.write_html(str(fig_path_html))\n",
    "    print(\n",
    "        f\"Figure Eukaryote-homologs_85.png (Consistent Colors) HTML saved to: {fig_path_html}\"\n",
    "    )\n",
    "    try:\n",
    "        fig_path_pdf = (\n",
    "            Path(output_figure_dir)\n",
    "            / \"figure_eukaryotic_hit_characterization_consistent_colors.pdf\"\n",
    "        )\n",
    "        fig.write_image(str(fig_path_pdf), width=900, height=1400)\n",
    "        print(f\"Figure Eukaryote-homologs_85.png PDF saved to: {fig_path_pdf}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not export Figure 13 to PDF. Error: {e}\")\n",
    "    try:\n",
    "        fig_path_svg = (\n",
    "            Path(output_figure_dir)\n",
    "            / \"figure13_eukaryotic_hit_characterization_consistent_colors.svg\"\n",
    "        )\n",
    "        fig.write_image(str(fig_path_svg), width=900, height=1400)\n",
    "        print(f\"Figure Eukaryote-homologs_85.png SVG saved to: {fig_path_svg}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not export Figure 13 to SVG. Error: {e}\")\n",
    "\n",
    "print(\n",
    "    \"\\n\\n--- Cell 21 (Figure Eukaryote-homologs_85.png - Eukaryotic Hit Characterization with Consistent Colors) Complete ---\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa738a7-a27a-4d8a-aab0-b1ddfc0bf0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell reproduces the orthogroup diversity/functional enrichment heatmap with the green-purple color scale used in the pub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff38b4f-f50a-45fc-95f2-d4ed0f75fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20B (New): Regenerate Functional Enrichment Heatmap with Green-Purple Colorscale\n",
    "\n",
    "# --- Imports & Setup ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "\n",
    "# Import the necessary components from arcadia-pycolor\n",
    "try:\n",
    "    from arcadia_pycolor import colors\n",
    "    from arcadia_pycolor.gradient import Gradient\n",
    "\n",
    "    print(\"Successfully imported arcadia_pycolor components.\")\n",
    "except ImportError:\n",
    "    print(\n",
    "        \"ERROR: arcadia-pycolor is not installed. Please install it: pip install arcadia-pycolor\"\n",
    "    )\n",
    "\n",
    "    # Define dummy objects to prevent crashes, though plot will look wrong\n",
    "    class colors:\n",
    "        fern = lime = lichen = lilac = aster = ghost = \"grey\"\n",
    "\n",
    "    class Gradient:\n",
    "        def __init__(self, name, color_list, pos_list):\n",
    "            pass\n",
    "\n",
    "        def to_plotly_colorscale(self):\n",
    "            return \"Viridis\"\n",
    "\n",
    "        def reverse(self):\n",
    "            return self\n",
    "\n",
    "        def __add__(self, other):\n",
    "            return self\n",
    "\n",
    "\n",
    "# --- Configuration (should match previous cells) ---\n",
    "# Define the standard internal column names the script will use.\n",
    "orthogroup_col = \"OG_ID\"\n",
    "group_col = \"Group\"\n",
    "broad_func_cat_col = \"Broad_Functional_Category\"\n",
    "apsi_col = \"Intra_OG_APSI\"\n",
    "\n",
    "# --- Paths to input files ---\n",
    "# Main database, which has the functional categories\n",
    "main_database_path = (\n",
    "    \"proteome_database_v3.5.csv\"  # <-- IMPORTANT: Point to your main (corrected) DB\n",
    ")\n",
    "# Summary file with extreme OG flags from Cell 15\n",
    "extreme_ogs_summary_path = (\n",
    "    Path(\"output_summary_data_hit_validation_phase1\")\n",
    "    / \"figure11_extreme_diversity_ogs_summary_expanded_revised.csv\"\n",
    ")\n",
    "# Summary file with diversity metrics from Cell 13\n",
    "merged_diversity_path = (\n",
    "    Path(\"output_summary_data_hit_validation_phase1\")\n",
    "    / \"figure9_merged_diversity_and_apsi_data.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "# Profiles to include in the heatmap\n",
    "profiles_for_heatmap = {\n",
    "    \"High Entropy & High APSI\": \"Hi Ent, Hi APSI\",\n",
    "    \"Low Entropy & Low APSI\": \"Lo Ent, Lo APSI\",\n",
    "}\n",
    "# Categories to exclude from the heatmap for clarity\n",
    "categories_to_exclude_from_heatmap = [\n",
    "    \"Other Specific Annotation\",\n",
    "    \"General Protein Features\",\n",
    "    \"Unknown/Unclassified\",\n",
    "]\n",
    "\n",
    "# --- Load Data ---\n",
    "print(\"\\n--- Loading necessary data for heatmap generation ---\")\n",
    "error_found = False\n",
    "try:\n",
    "    df_extreme_ogs_summary = pd.read_csv(extreme_ogs_summary_path)\n",
    "    # --- FIX: Standardize orthogroup column name ---\n",
    "    if (\n",
    "        \"Orthogroup\" in df_extreme_ogs_summary.columns\n",
    "        and orthogroup_col not in df_extreme_ogs_summary.columns\n",
    "    ):\n",
    "        df_extreme_ogs_summary.rename(\n",
    "            columns={\"Orthogroup\": orthogroup_col}, inplace=True\n",
    "        )\n",
    "    print(f\"Loaded extreme OGs summary from: {extreme_ogs_summary_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"ERROR: Extreme OGs summary file not found at '{extreme_ogs_summary_path}'. Please run Cell 15 first.\"\n",
    "    )\n",
    "    df_extreme_ogs_summary = pd.DataFrame()\n",
    "    error_found = True\n",
    "\n",
    "try:\n",
    "    df_merged_diversity = pd.read_csv(merged_diversity_path)\n",
    "    # --- FIX: Standardize orthogroup column name ---\n",
    "    if (\n",
    "        \"Orthogroup\" in df_merged_diversity.columns\n",
    "        and orthogroup_col not in df_merged_diversity.columns\n",
    "    ):\n",
    "        df_merged_diversity.rename(columns={\"Orthogroup\": orthogroup_col}, inplace=True)\n",
    "    print(f\"Loaded merged diversity data from: {merged_diversity_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"ERROR: Merged diversity data file not found at '{merged_diversity_path}'. Please run Cell 13 first.\"\n",
    "    )\n",
    "    df_merged_diversity = pd.DataFrame()\n",
    "    error_found = True\n",
    "\n",
    "try:\n",
    "    df_full = pd.read_csv(main_database_path, low_memory=False)\n",
    "    df_full.columns = df_full.columns.str.strip()\n",
    "    # --- FIX: Standardize orthogroup column name ---\n",
    "    if \"Orthogroup\" in df_full.columns and orthogroup_col not in df_full.columns:\n",
    "        df_full.rename(columns={\"Orthogroup\": orthogroup_col}, inplace=True)\n",
    "    print(\n",
    "        f\"Loaded main database from '{main_database_path}' to get functional categories.\"\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"ERROR: Main database file not found at '{main_database_path}'. Cannot get functional categories.\"\n",
    "    )\n",
    "    df_full = pd.DataFrame()\n",
    "    error_found = True\n",
    "\n",
    "\n",
    "# --- Data Preparation for Heatmap ---\n",
    "if not error_found:\n",
    "    print(\"\\n--- Preparing data for heatmap ---\")\n",
    "\n",
    "    # --- Merge Broad_Functional_Category into df_merged_diversity ---\n",
    "    if broad_func_cat_col not in df_merged_diversity.columns:\n",
    "        print(\n",
    "            f\"'{broad_func_cat_col}' not in diversity data, merging from main database...\"\n",
    "        )\n",
    "        if orthogroup_col in df_full.columns:\n",
    "            # Get one representative functional category per orthogroup\n",
    "            og_categories = df_full[\n",
    "                [orthogroup_col, broad_func_cat_col]\n",
    "            ].drop_duplicates(subset=[orthogroup_col])\n",
    "            # Merge it into the diversity dataframe\n",
    "            df_merged_diversity = pd.merge(\n",
    "                df_merged_diversity, og_categories, on=orthogroup_col, how=\"left\"\n",
    "            )\n",
    "            df_merged_diversity[broad_func_cat_col].fillna(\n",
    "                \"Unknown/Unclassified\", inplace=True\n",
    "            )\n",
    "            print(\"Merge complete.\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"ERROR: Cannot merge categories because '{orthogroup_col}' is missing from df_full.\"\n",
    "            )\n",
    "            error_found = True\n",
    "\n",
    "    # Check for group column and standardize its name too\n",
    "    if (\n",
    "        \"Source_Dataset\" in df_merged_diversity.columns\n",
    "        and group_col not in df_merged_diversity.columns\n",
    "    ):\n",
    "        df_merged_diversity.rename(columns={\"Source_Dataset\": group_col}, inplace=True)\n",
    "\n",
    "    if not error_found:\n",
    "        heatmap_data_list = []\n",
    "\n",
    "        # Calculate baseline functional distributions AFTER excluding unwanted categories\n",
    "        baseline_func_dist_asgard = (\n",
    "            df_merged_diversity[\n",
    "                (df_merged_diversity[group_col] == \"Asgard\")\n",
    "                & (\n",
    "                    ~df_merged_diversity[broad_func_cat_col].isin(\n",
    "                        categories_to_exclude_from_heatmap\n",
    "                    )\n",
    "                )\n",
    "            ][broad_func_cat_col]\n",
    "            .value_counts(normalize=True)\n",
    "            .mul(100)\n",
    "        )\n",
    "\n",
    "        baseline_func_dist_gv = (\n",
    "            df_merged_diversity[\n",
    "                (df_merged_diversity[group_col] == \"GV\")\n",
    "                & (\n",
    "                    ~df_merged_diversity[broad_func_cat_col].isin(\n",
    "                        categories_to_exclude_from_heatmap\n",
    "                    )\n",
    "                )\n",
    "            ][broad_func_cat_col]\n",
    "            .value_counts(normalize=True)\n",
    "            .mul(100)\n",
    "        )\n",
    "\n",
    "        for profile_key, short_profile_label in profiles_for_heatmap.items():\n",
    "            for group_val in [\"Asgard\", \"GV\"]:\n",
    "                df_profile_group_subset = df_extreme_ogs_summary[\n",
    "                    df_extreme_ogs_summary[\"Reason_for_Highlight\"].str.contains(\n",
    "                        profile_key, regex=False, na=False\n",
    "                    )\n",
    "                    & (df_extreme_ogs_summary[group_col] == group_val)\n",
    "                    & (\n",
    "                        ~df_extreme_ogs_summary[broad_func_cat_col].isin(\n",
    "                            categories_to_exclude_from_heatmap\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "                if df_profile_group_subset.empty:\n",
    "                    continue\n",
    "\n",
    "                observed_dist = (\n",
    "                    df_profile_group_subset[broad_func_cat_col]\n",
    "                    .value_counts(normalize=True)\n",
    "                    .mul(100)\n",
    "                )\n",
    "                baseline_dist = (\n",
    "                    baseline_func_dist_asgard\n",
    "                    if group_val == \"Asgard\"\n",
    "                    else baseline_func_dist_gv\n",
    "                )\n",
    "\n",
    "                for func_cat, obs_perc in observed_dist.items():\n",
    "                    if func_cat in categories_to_exclude_from_heatmap:\n",
    "                        continue\n",
    "                    exp_perc = baseline_dist.get(func_cat, 0)\n",
    "                    log2_fold_change = np.log2((obs_perc + 1e-9) / (exp_perc + 1e-9))\n",
    "                    heatmap_data_list.append(\n",
    "                        {\n",
    "                            \"Functional_Category\": func_cat,\n",
    "                            \"Profile_Group\": f\"{group_val} - {short_profile_label}\",\n",
    "                            \"Log2_Fold_Change\": log2_fold_change,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        if not heatmap_data_list:\n",
    "            print(\"No data available to generate the heatmap.\")\n",
    "        else:\n",
    "            df_heatmap = pd.DataFrame(heatmap_data_list)\n",
    "            df_heatmap_pivot = df_heatmap.pivot(\n",
    "                index=\"Functional_Category\",\n",
    "                columns=\"Profile_Group\",\n",
    "                values=\"Log2_Fold_Change\",\n",
    "            ).fillna(0)\n",
    "            df_heatmap_pivot = df_heatmap_pivot.sort_index()\n",
    "\n",
    "            if not df_heatmap_pivot.empty:\n",
    "                print(\"\\n--- Generating Heatmap ---\")\n",
    "\n",
    "                # --- Define the necessary gradients to build purple_green ---\n",
    "                greens = Gradient(\n",
    "                    \"greens\",\n",
    "                    [colors.fern, colors.lime, colors.lichen],\n",
    "                    [0, 0.622, 1],\n",
    "                )\n",
    "                purples = Gradient(\n",
    "                    \"purples\",\n",
    "                    [colors.lilac, colors.aster, colors.ghost],\n",
    "                    [0, 0.144, 1.0],\n",
    "                )\n",
    "\n",
    "                # --- CORRECTED: Build the purple_green gradient object ---\n",
    "                purple_green = purples + greens.reverse()\n",
    "                purple_green.name = \"purple_green\"\n",
    "\n",
    "                # --- Get the Green-Purple Colorscale for Plotly ---\n",
    "                green_purple_colorscale = purple_green.to_plotly_colorscale()\n",
    "                print(\n",
    "                    \"Using 'purple_green' gradient and converting to Plotly colorscale.\"\n",
    "                )\n",
    "\n",
    "                # Determine a symmetric color scale range\n",
    "                abs_max_val = df_heatmap_pivot.abs().max().max()\n",
    "                z_min = -abs_max_val if abs_max_val > 0 else -1\n",
    "                z_max = abs_max_val if abs_max_val > 0 else 1\n",
    "\n",
    "                # Create the heatmap figure\n",
    "                fig_heatmap = go.Figure(\n",
    "                    data=go.Heatmap(\n",
    "                        z=df_heatmap_pivot.values,\n",
    "                        x=df_heatmap_pivot.columns,\n",
    "                        y=df_heatmap_pivot.index,\n",
    "                        colorscale=green_purple_colorscale,  # <-- APPLYING THE NEW COLORSCALE\n",
    "                        zmid=0,\n",
    "                        zmin=z_min,\n",
    "                        zmax=z_max,\n",
    "                        colorbar_title=\"Log2 (Obs/Exp %)\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # Apply styling consistent with the rest of the notebook\n",
    "                fig_heatmap.update_layout(\n",
    "                    title_text=\"<b>Functional Enrichment in 'Surprising' Diversity Profiles</b>\",\n",
    "                    xaxis_title=\"\",\n",
    "                    yaxis_title=\"\",\n",
    "                    height=500,\n",
    "                    width=800,\n",
    "                    plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "                    paper_bgcolor=\"rgba(0,0,0,0)\",\n",
    "                    margin=dict(l=250, r=50, t=50, b=150),  # Adjust margins for labels\n",
    "                )\n",
    "                fig_heatmap.update_xaxes(tickangle=20, tickfont=dict(size=10))\n",
    "                fig_heatmap.update_yaxes(\n",
    "                    categoryorder=\"array\",\n",
    "                    categoryarray=df_heatmap_pivot.index.tolist(),\n",
    "                    tickfont=dict(size=10),\n",
    "                )\n",
    "\n",
    "                fig_heatmap.show()\n",
    "\n",
    "                # --- Save Figure ---\n",
    "                output_figure_dir = Path(\"publication_figures\")\n",
    "                output_figure_dir.mkdir(exist_ok=True)\n",
    "                fig_path_html = (\n",
    "                    output_figure_dir / \"figure12_panel_b_heatmap_green_purple.html\"\n",
    "                )\n",
    "                fig_heatmap.write_html(str(fig_path_html))\n",
    "                print(f\"\\nUpdated heatmap HTML saved to: {fig_path_html}\")\n",
    "                try:\n",
    "                    fig_path_svg = (\n",
    "                        output_figure_dir / \"figure12_panel_b_heatmap_green_purple.svg\"\n",
    "                    )\n",
    "                    fig_heatmap.write_image(str(fig_path_svg))\n",
    "                    print(f\"Updated heatmap SVG saved to: {fig_path_svg}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not export heatmap to SVG. Error: {e}\")\n",
    "else:\n",
    "    print(\"\\nCould not generate heatmap due to data loading errors.\")\n",
    "\n",
    "print(\"\\n--- Heatmap Regeneration Cell Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c1b60-d01c-43a3-815d-77c6c93f99ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (asgard_gv_env)",
   "language": "python",
   "name": "asgard_gv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
