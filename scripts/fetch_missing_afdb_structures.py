#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Fetches AlphaFold DB structures based on a list of proteins identified as
missing from the local structure log but having an Avg_pLDDT.

Input: afdb_missing_log_with_uniprot_ac_v6.csv
Output: Downloads PDB files to downloaded_structures/alphafold/
        Appends to or creates structure_download_log.csv
"""

import pandas as pd
import requests
import time
import logging
from pathlib import Path
from tqdm.auto import tqdm # Optional: for progress bar (pip install tqdm)

# --- Setup Logging ---
logger = logging.getLogger(__name__)
if not logger.handlers: # Check if handlers are already added
    logger.setLevel(logging.INFO)
    console_handler = logging.StreamHandler()
    log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(filename)s.%(funcName)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    console_handler.setFormatter(log_formatter)
    logger.addHandler(console_handler)
else:
    logger.setLevel(logging.INFO) # Ensure level is set if handlers exist

# --- Configuration ---
SCRIPT_DIR = Path(__file__).resolve().parent
BASE_DIR = SCRIPT_DIR.parent # Assumes script is in 'scripts' directory
ANALYSIS_OUTPUTS_DIR = BASE_DIR / "analysis_outputs"
DATA_DIR = BASE_DIR / "data"

# Input CSV generated by crosscheck_missing_afdb_script (v2)
INPUT_CSV_FILE = ANALYSIS_OUTPUTS_DIR / "afdb_missing_log_with_uniprot_ac_v6.csv"

BASE_OUTPUT_DIR = DATA_DIR / "downloaded_structures"
AFDB_OUTPUT_DIR = BASE_OUTPUT_DIR / "alphafold_structures"
# RCSB_PDB_OUTPUT_DIR = BASE_OUTPUT_DIR / "rcsb_pdb" # Not used in this focused version

# Main cumulative download log
MASTER_LOG_CSV_FILE = DATA_DIR / "structure_download_log_v2.csv"

PROTEIN_ID_COL = "ProteinID"
UNIPROT_AC_COL = "UniProtKB_AC"
LOCAL_FILE_FOUND_COL = "Local_AFDB_File_Found" # From the input CSV

AFDB_MODEL_VERSION = "v4" # Common AlphaFold model version
AFDB_URL_TEMPLATE = f"https://alphafold.ebi.ac.uk/files/AF-{{uniprot_ac}}-F1-model_{AFDB_MODEL_VERSION}.pdb"
# RCSB_PDB_URL_TEMPLATE = "https://files.rcsb.org/download/{pdb_id}.pdb" # Not used here

REQUEST_DELAY = 0.2 # Seconds to wait between requests
# --- End Configuration ---

def download_file(url: str, output_path: Path, protein_id_log: str) -> bool:
    """Downloads a file from a URL to a specified path."""
    try:
        logger.info(f"[{protein_id_log}] Attempting to download: {url}")
        response = requests.get(url, timeout=60, stream=True) # Increased timeout
        response.raise_for_status()
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        logger.info(f"[{protein_id_log}] Successfully downloaded and saved to: {output_path}")
        return True
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 404:
            logger.warning(f"[{protein_id_log}] File not found (404): {url}")
        else:
            logger.error(f"[{protein_id_log}] HTTP Error {e.response.status_code} for {url}: {e}")
    except requests.exceptions.RequestException as e:
        logger.error(f"[{protein_id_log}] Request failed for {url}: {e}")
    except Exception as e:
        logger.error(f"[{protein_id_log}] An unexpected error occurred downloading {url}: {e}", exc_info=True)
    return False

def main():
    logger.info("Starting missing AlphaFold structure retrieval process...")
    AFDB_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    logger.info(f"AlphaFold DB structures will be saved to: {AFDB_OUTPUT_DIR}")

    # Load existing master download log if it exists
    existing_log_entries = pd.DataFrame()
    if MASTER_LOG_CSV_FILE.exists():
        try:
            existing_log_entries = pd.read_csv(MASTER_LOG_CSV_FILE)
            logger.info(f"Loaded {len(existing_log_entries)} entries from existing master log: {MASTER_LOG_CSV_FILE}")
        except Exception as e:
            logger.error(f"Could not read existing master log {MASTER_LOG_CSV_FILE}: {e}. Will start a new log if it's empty or treat as empty.")
            existing_log_entries = pd.DataFrame() # Ensure it's an empty DF on error

    # Prepare a set of successfully downloaded UniProt ACs from the existing log for quick lookup
    # Assuming 'Retrieved_From' contains 'AlphaFoldDB' and 'Download_Status' starts with 'Success'
    # And that UniProt AC is part of the Saved_Filename or can be inferred.
    # For simplicity, we'll check ProteinID against the log.
    successfully_logged_protein_ids = set()
    if not existing_log_entries.empty and PROTEIN_ID_COL in existing_log_entries.columns and \
       'Download_Status' in existing_log_entries.columns and 'Retrieved_From' in existing_log_entries.columns:
        successful_afdb_logs = existing_log_entries[
            existing_log_entries['Retrieved_From'].astype(str).str.contains("AlphaFoldDB", case=False, na=False) &
            existing_log_entries['Download_Status'].astype(str).str.startswith("Success")
        ]
        successfully_logged_protein_ids = set(successful_afdb_logs[PROTEIN_ID_COL])
        logger.info(f"Found {len(successfully_logged_protein_ids)} ProteinIDs already logged as successfully downloaded from AlphaFoldDB.")


    new_log_records = []

    try:
        df_input = pd.read_csv(INPUT_CSV_FILE)
        logger.info(f"Read {len(df_input)} proteins from input file: '{INPUT_CSV_FILE}'.")
    except FileNotFoundError:
        logger.critical(f"ERROR: Input file '{INPUT_CSV_FILE}' not found. Exiting.")
        return
    except Exception as e:
        logger.critical(f"Error reading input CSV '{INPUT_CSV_FILE}': {e}")
        return

    # Ensure required columns are in the input dataframe
    required_input_cols = [PROTEIN_ID_COL, UNIPROT_AC_COL, LOCAL_FILE_FOUND_COL]
    if not all(col in df_input.columns for col in required_input_cols):
        missing_cols = [col for col in required_input_cols if col not in df_input.columns]
        logger.critical(f"ERROR: Input CSV is missing required columns: {missing_cols}. Exiting.")
        return

    proteins_to_fetch = df_input[~df_input[LOCAL_FILE_FOUND_COL]]
    logger.info(f"Found {len(proteins_to_fetch)} proteins marked with '{LOCAL_FILE_FOUND_COL}'=False to potentially fetch.")

    for index, row in tqdm(proteins_to_fetch.iterrows(), total=len(proteins_to_fetch), desc="Fetching Missing AFDB Structures"):
        protein_id = row[PROTEIN_ID_COL]
        uniprot_ac_val = row.get(UNIPROT_AC_COL)
        
        retrieved_from = None
        saved_filename = None
        status_message = "Not attempted"

        # Check if already successfully logged
        if protein_id in successfully_logged_protein_ids:
            logger.info(f"[{protein_id}] Already logged as successfully downloaded from AlphaFoldDB. Skipping.")
            # Optionally, add a log entry indicating it was skipped due to prior success
            # For now, we just skip to avoid duplicate processing or log entries.
            continue

        if pd.notna(uniprot_ac_val) and isinstance(uniprot_ac_val, str) and uniprot_ac_val.strip():
            uniprot_ac_clean = uniprot_ac_val.strip()
            afdb_url = AFDB_URL_TEMPLATE.format(uniprot_ac=uniprot_ac_clean)
            # Use the standard AFDB filename convention based on UniProt AC
            afdb_filename = f"AF-{uniprot_ac_clean}-F1-model_{AFDB_MODEL_VERSION}.pdb"
            afdb_output_path = AFDB_OUTPUT_DIR / afdb_filename

            if not afdb_output_path.exists(): # Check if file exists locally before download attempt
                if download_file(afdb_url, afdb_output_path, protein_id):
                    retrieved_from = "AlphaFoldDB"
                    saved_filename = afdb_filename
                    status_message = "Success (AlphaFoldDB)"
                else:
                    status_message = "Failed (AlphaFoldDB)" # download_file logs specifics
            else:
                logger.info(f"[{protein_id}] Structure file already exists locally: {afdb_output_path}. Logging as cached.")
                retrieved_from = "AlphaFoldDB (LocalFileExists)" # Indicate it was found locally, not re-downloaded by this run
                saved_filename = afdb_filename
                status_message = "Success (AlphaFoldDB - LocalFileExists)"
            
            time.sleep(REQUEST_DELAY)
        else:
            logger.warning(f"[{protein_id}] Skipped AFDB download due to missing or invalid UniProt AC: '{uniprot_ac_val}'.")
            status_message = "Skipped AFDB (Invalid/Missing UniProt AC)"
        
        # Always add a log record for proteins that were attempted or explicitly skipped after this point
        new_log_records.append({
            PROTEIN_ID_COL: protein_id,
            UNIPROT_AC_COL: uniprot_ac_val, # Log the original UniProt AC value
            # PDB_IDS_COL: row.get(PDB_IDS_COL), # If you had PDB IDs in your input
            "Retrieved_From": retrieved_from,
            "Saved_Filename": saved_filename,
            "Download_Status": status_message,
            "OG_ID": row.get("OG_ID", None) # Include OG_ID if present in input
        })

    if new_log_records:
        df_new_logs = pd.DataFrame(new_log_records)
        
        # Combine with existing log entries, avoid duplicates based on ProteinID and successful AFDB download
        if not existing_log_entries.empty:
            # A more robust way to avoid duplicates might be needed if ProteinIDs can have multiple attempts
            # For now, simple concat and drop_duplicates on key fields
            df_combined_log = pd.concat([existing_log_entries, df_new_logs], ignore_index=True)
            # Keep the latest attempt for a protein if multiple, or based on success
            # This simple drop_duplicates might not be perfect for all re-run scenarios
            df_combined_log.drop_duplicates(subset=[PROTEIN_ID_COL, 'Retrieved_From', 'Saved_Filename'], keep='last', inplace=True)
        else:
            df_combined_log = df_new_logs
        
        try:
            df_combined_log.sort_values(by=[PROTEIN_ID_COL], inplace=True)
            df_combined_log.to_csv(MASTER_LOG_CSV_FILE, index=False)
            logger.info(f"Master download log updated and saved to: {MASTER_LOG_CSV_FILE} with {len(df_combined_log)} total entries.")
        except Exception as e:
            logger.error(f"Error writing master log CSV file: {e}", exc_info=True)
    else:
        logger.info("No new download attempts were made or log entries generated.")
    
    logger.info("--- Missing AlphaFold Structure Retrieval Process Finished ---")

if __name__ == "__main__":
    main()
